{
    "Géographie": [
        {
            "pageid": 1224,
            "ns": 0,
            "title": "Géographie",
            "content": "La géographie (du grec ancien γεωγραφία / geōgraphía, composé de γῆ / gê, « la Terre » et γράφω / gráphô, « écrire, dessiner », puis du latin geographia, littéralement traduit par « dessin de la Terre ») est une science centrée sur le présent, ayant pour objet la description de la Terre et en particulier l'étude des phénomènes physiques, biologiques et humains qui se produisent sur le globe terrestre, à un certain niveau d'abstraction relative qui s'y prête, pluridisciplinarité comprise voire transdisciplinarité en un certain sens. En effet Yves Lacoste disait : « La géographie ne sert pas seulement à faire de la géopolitique. Cela sert aussi, pour tout un chacun, à admirer davantage de beaux paysages, en comprenant mieux comment ils sont construits. » Le portail de l'information géographique du gouvernement du Québec définit la géographie comme « une science de la connaissance de l'aspect actuel, naturel et humain de la surface terrestre. Elle permet de comprendre l'organisation spatiale de phénomènes (physiques ou humains) qui se manifestent dans notre environnement et façonnent notre monde ».\nElle se divise en trois branches principales :\n\ngéographie mathématique qui est la première à être apparue avec « l'étude des formes et des dimensions de la Terre » à travers la géodésie ;\ngéographie physique qui comprend des disciplines scientifiques intégrant les sciences naturelles ;\ngéographie humaine qui comprend des disciplines scientifiques intégrant les sciences humaines et sociales.\nGrâce aux progrès de l'astronomie et de l'astronautique, des formations sont maintenant connues ailleurs que sur cette planète, le terme est utilisé pour tous les objets célestes.\n\n\n== Évolution et étendue de la notion de géographie ==\n\nLa première personne à utiliser le mot « géographie » était Ératosthène (276-194 av. J.-C.) pour un ouvrage aujourd'hui perdu mais l'arrivée de la géographie est attribuée à Hérodote (484-420 av. J.-C.) ; aussi considéré comme étant le premier historien. Pour les Grecs, c'est la description rationnelle de la Terre en comprenant principalement la géographie physique. Il s'agit d'une science qui répond à une curiosité nouvelle, et qui va déterminer la géopolitique en définissant les territoires à conquérir et à tenir, ce qui implique la réalisation de cartes. Pour Strabon, la géographie est la base de la formation de celui qui voulait décider.\nQuatre traditions historiques dans la recherche géographique sont l'analyse spatiale des phénomènes naturels et humains (la géographie comme une étude de la répartition des êtres vivants), des études territoriales (lieux et régions), l'étude des relations entre l'Homme et son environnement, et la recherche en sciences de la terre.\nAvec l'évolution de la recherche scientifique, plusieurs domaines de la géographie ont évolué vers un statut de science à part entière. On peut citer la climatologie, l'océanographie, la cartographie, etc. ce qui a eu pour effet de principalement recentrer les activités du géographe sur les interactions humaines (aspect social) et de son rapport à son environnement (aspect spatial). Les géographie physique et mathématique sont les branches de la géographie qui ont le plus subit cette évolution des sciences alors que la géographie humaine a profité de ce changement pour passer de la géopolitique à une étude plus rationnelle et enrichie des rapports humains et des relations qu'ils entretiennent avec leurs environnements à travers des disciplines nouvelles.\nNéanmoins, la géographie moderne est une discipline englobante qui cherche avant tout à mieux comprendre notre planète et toutes ses complexités humaines et naturelles, non seulement où les objets sont, via l'élaboration de cartes, mais comment ils ont changé et viennent à l'être. Longtemps les géographes ont perçu leur discipline comme une discipline carrefour (Jacqueline Bonnamour), « pont entre les sciences humaines et physiques ». L'approche géographique d'un phénomène ne se limite pas uniquement à l'utilisation de la cartographie (l'étude des cartes). La grille de questionnement, associée à la cartographie, permet d'ajuster l'analyse de l'objet — l'espace — et d'expliquer pourquoi on trouve tel ou tel phénomène ici et pas ailleurs. La géographie s'applique donc à déterminer les causes, aussi bien naturelles qu'humaines ; et lorsqu'ils observent des différences, leurs conséquences.\nAujourd'hui, une division de la géographie en deux branches principales s'est imposée à l'usage, la géographie humaine et la géographie physique.\nCependant la géographie reste par excellence une discipline de synthèse qui interroge à la fois « les traces » laissées par les sociétés (mise en valeur des espaces ou impacts) ou la nature (orogenèse des montagnes, impact du climat…) et les dynamiques en œuvre aussi bien dans les sociétés (émergence socio-économique de la façade asiatique pacifique, désindustrialisation progressive des pays développés à économie de marché) qu'au sein de l'environnement physique (« Changement global », montée du niveau marin…). La géographie s'intéresse donc à la fois aux héritages (physiques ou humains) et aux dynamiques (démographiques, socioéconomiques, culturelles, climatiques, etc.) présents dans les espaces.\nPar ailleurs cette discipline tend à intégrer divers champs culturels tels que la peinture paysagiste, le roman ou encore le cinéma.\n\n\n== Histoire ==\n\n\n=== Géographie antique ===\n\nDans le bassin méditerranéen, la géographie est à l'origine composée de mesures expérimentales et de récits sur des voyages et des lieux pour répertorier l'univers connu. Les cartes et l'exploration sont surtout le fait des savants du monde grec. Ainsi, Claude Ptolémée répertorie tout l'univers connu dans son ouvrage Géographie. Anaximandre réalise l'une des premières cartes du monde connu.\nLes Grecs sont la première civilisation connue pour avoir étudié la géographie, à la fois comme science et comme philosophie[réf. nécessaire]. Thalès de Milet, Hérodote (auteur de la première chorographie), Ératosthène (première carte du monde connu – l'écoumène –, calcul de la circonférence terrestre), Hipparque, Aristote, Ptolémée ont apporté des contributions majeures à la discipline. Les Romains ont apporté de nouvelles techniques alors qu'ils cartographiaient de nouvelles régions.\nCes précurseurs développent quatre branches de la géographie qui vont perdurer jusqu'à la Renaissance :\n\ndécouvrir et explorer les continents ;\nmesurer l'espace terrestre (géodésie) ;\nsituer la Terre dans les systèmes astronomiques (cosmographie) ;\nreprésenter l'espace terrestre (cartographie).\n\n\n=== Époque moderne ===\n\nAprès la Renaissance et les grandes découvertes, la géographie s'impose comme une discipline à part entière dans le domaine scientifique.\nNicolas Copernic développe la théorie de l'héliocentrisme selon laquelle le Soleil est au centre de l'Univers et que la Terre tourne autour du Soleil. Gérard Mercator publie en 1569 une mappemonde en dix-huit feuillets appelée « projection Mercator » qui fournit aux navigateurs une réelle description des contours des terres.\n\n\n=== Époque contemporaine ===\n\nEntre le XIXe et le XXe siècle, plusieurs courants se développent tentant de démontrer l'interaction entre l'homme et la nature, avec plus ou moins de succès et de rigueur d'approche :\n\nle courant déterministe, emmené par le géographe allemand Carl Ritter. Le déterminisme considère qu'une cause naturelle produit une conséquence sociale ;\nle courant environnementaliste, développé par le géographe allemand Friedrich Ratzel. Tout être vivant est le produit du milieu dans lequel il vit ;\nle courant possibiliste de Vidal de La Blache qui cherche à nuancer les approches précédentes. Il n'y a pas de déterminants géographiques, mais des possibilités que l'homme choisit, ou non, d'utiliser. La nature propose, l'homme dispose.\nL'École française de géographie, créée par Paul Vidal de La Blache, développe aussi une spécificité : la géographie régionale. Il s'agit de traiter de l'unique, de la région (« idiographie » ou travail sur les spécificités), évitant ainsi les dérives nomothétiques, mais tombant dans une connaissance encyclopédique.\nÉlisée Reclus est l'auteur d'une encyclopédie (la Nouvelle Géographie universelle, en 19 tomes). Son regard géographique fut influencé par ses convictions anarchistes.\n\n\n=== Les Grandes Mutations du XXe siècle ===\nLa nouvelle géographie se développe à partir des années 1960 aux États-Unis et gagne la France, la Suisse et surtout l'Allemagne dans les années 1970. Elle est directement influencée par les géographies anglo-saxonnes et scandinaves. Inspirée par les mathématiques (statistiques) et les règles de l'économie, cette géographie tente d'établir des « lois » universelles (science nomothétique)[réf. nécessaire].\nEn créant des connaissances multidisciplinaires, la géographie donne des clés de lecture et d’analyse des grands enjeux contemporains liant espaces et sociétés. Elle s’adresse à divers publics : les politiques, les médias, les scientifiques, ainsi que la société dans son ensemble. Dans notre monde de plus en plus globalisé, cette discipline permet notamment d'appréhender de manière multiscalaire et critique les flux de biens, d'informations et de personnes afin de résoudre les défis posés par les changements climatiques, l'urbanisation, ou encore les migrations et les conflits armés. La géographie constitue ainsi un outil d'expertise et d’éducation de ces enjeux, permettant d'agir sur un plan local, national et global[réf. nécessaire].\n\n\n== Branches scientifiques ==\n\n\n=== Géographie mathématique ===\n\nLa géographie mathématique se concentre sur la surface de la Terre, l'étude de sa représentation mathématique et sa relation à la Lune et au Soleil. Elle est la première forme de science géographique apparue pendant l'antiquité grecque et comprend aujourd'hui les disciplines scientifiques et techniques suivantes :\n\nla géodésie, science ayant pour objet l'étude de la forme et la mesure des dimensions de la Terre ;\nla cartographie, la réalisation et l'étude des cartes. Le principe majeur de la cartographie est la représentation de données sur un support réduit représentant un espace réel ;\nla géographie astronomique ;\nla géomatique, développée récemment, une branche de la géographie qui se distingue des précédentes par le recours à l'outil l'informatique pour analyser le territoire. Elle complète les systèmes d'information développés par ailleurs dans d'autres disciplines par une référence spatiale : la localisation géographique, couramment définie par un système de coordonnées géographiques (X, Y, Z). On distingue ainsi les systèmes d'information géographique (SIG) et la télédétection satellite ;\nl'analyse spatiale, qui recouvre un ensemble d'outils mais aussi de concepts permettant de modéliser les structures spatiales et d'analyser les dimensions spatiales de la vie en société ;\nla photogrammétrie, permettant de déterminer les dimensions et les volumes des objets à partir de mesures effectuées sur des photographies montrant les perspectives de ces objets ;\nla topographie.\n\n\tLa géographie mathématique inclut les sciences suivantes :\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n=== Géographie physique ===\n\nLa géographie physique est une discipline qui a pour but de « décrire, comparer et expliquer les paysages ». Elle s'organise en plusieurs spécialités : la géomorphologie (structurale et dynamique), la climatologie, l'hydrologie, la biogéographie et la paléogéographie. Ces disciplines concourent à l'analyse du milieu naturel, on dit plus communément aujourd'hui, des paysages, qui est un géosystème : ensemble géographique doté d'une structure et d'un fonctionnement propres, qui s'inscrit dans l'espace et dans le temps (échelles spatio-temporelles). Le géosystème comporte des composants abiotiques, biotiques et anthropiques qui sont en interaction :\n\nles composants abiotiques (« sans vie », les facteurs du milieu autres que ceux du vivant) relèvent :\nde la lithosphère (les roches) ;\net de l'atmosphère, déterminant le climat. Le climat se manifeste dans le déplacement des masses d'air mais aussi au niveau des milieux rocheux via des agents météoriques qui participent au climat du sol, aux processus de météorisation (modifications intervenant dans les roches sous l'effet des phénomènes atmosphériques) ;\net enfin, de l'hydrosphère (les eaux) dont l'étude générale est le domaine de l'hydrologie qui se subdivise en hydrologie continentale et en hydrologie marine (ou océanographie). L'hydrographie concerne l'étude de la répartition des eaux (voir réseau hydrographique). Dans le milieu naturel, l'eau ne concerne pas seulement l'eau atmosphérique, les rivières, les lacs, les mers et océans et, les glaciers — l'eau doit être envisagée sous ses trois formes — mais aussi l'eau contenue dans la lithosphère.\nles composants biotiques (bios, la vie) représentés par la biosphère (végétaux à travers la phytogéographie et animaux y compris la faune du sol dans la pédofaune) ;\nles composants anthropiques (anthrôpos, l'homme). L'étude actuelle des géosystèmes est caractérisée par une prise en compte plus grande de l'anthropisation, de même que l'accent est mis sur l'évolution dans le temps.\nAinsi par exemple, la géomorphologie analyse l'une des composantes du milieu naturel, en relation étroite avec les autres disciplines de la géographie physique et des sciences de la Terre (géologie). On distingue une géomorphologie structurale qui correspond dans le relief à l’expression directe de la structure, d’une géomorphologie dynamique (voire climatique) dont les formes sont liées à l’action d’un climat particulier. Cette discipline s'associe également à l'analyse du milieu dans son ensemble dans le cadre de projets d'aménagements ou de conservation des milieux naturels\nLa géographie physique a initialement pour objet principal le milieu. C'est la branche de la géographie qui a dominé jusque dans les années 1950-1970 par le biais de la géomorphologie, en particulier structurale, et donc l'ensemble de la discipline. L'étude de géographie physique et du paysage était la base de l'étude de la géographie pour le père de la géographie française, Paul Vidal de La Blache. Pour comprendre l'organisation des sociétés humaines, il fallait analyser le milieu dans lequel vivaient les hommes. L'historien Lucien Febvre a qualifié cette démarche possibiliste, « la nature distribue les cartes, l'homme joue la partie » (J.-P. Alix, L'Espace humain) (possibilisme). Les évolutions épistémologiques des années 1960 ont fortement affaibli la géographie physique, des géographes tel qu'Yves Lacoste ont fortement critiqué une emprise trop forte de la géographie physique comme élément explicatif de l'organisation des sociétés humaines (déterminisme).\nLa géographie physique a aujourd'hui profondément changé. Elle s'intéresse de plus en plus au rôle de l'homme dans la transformation de son environnement physique. Parmi les concepts les plus utilisés, on trouve l'anthropisation (voir par exemple les atouts et les contraintes dans les travaux de J.-P. Marchand, université de Bretagne, sur le climat de l'Irlande).\nLa place de la géographie physique est débattue au sein même de la géographie. Certains voient en la géographie physique une science de la nature, d'autres comme J.-P. Marchand affirme : « géographie physique, science sociale ». L'unité de la discipline est souvent remise en question pour deux raisons. Certains géographes physiciens se sont fortement rapprochés des unités de recherches des sciences de l'environnement. Certains géographes humanistes rejettent au nom du déterminisme une explication physique de l'organisation des espaces humains.\nCertains géographes physiciens intègrent les concepts de la géographie humaine et des sciences sociales. Ils plaident pour un renouveau de la géographie physique parfois appelée, géographie de l'environnement. Les études dans le domaine du développement durable en sont des exemples. Yvette Veyret en géomorphologie, Martine Tabeaud en climatologie ou encore Paul Arnoud en biogéographie tentent de réconcilier géographie physique et géographie humaine en alliant études environnementales, prise en compte des acteurs géopolitiques et des aménagements.\n\n\tLa géographie physique inclut les sciences suivantes :\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n=== Géographie humaine ===\n\n\n==== Géographie générale ====\nLa géographie humaine est l'étude spatiale des activités humaines à la surface du globe, donc l'étude de l'écoumène, c'est-à-dire des régions habitées par l'homme. L'analyse de géographie humaine se fait à cette époque par le prisme de densités : on cherche à comprendre les préférences qui guident les hommes dans le choix du lieu où ils vont habiter. La géographie universitaire du début du XXe siècle insiste sur le poids de l'histoire. Dans cette approche, l'interaction entre les hommes et la nature au moyen de leurs connaissances et de leur histoire propre conduit à distinguer les sociétés et les régions en fonction de leur genre de vie.\nLa géographie humaine était au début du XXe siècle le parent pauvre de la discipline. Comme la géographie physique, c'était avant tout une discipline très descriptive et peu analytique. Dans les années 1920-1930, une approche économique de la géographie humaine se développe autour d'Albert Demangeon proche de l'école des Annales. Mais, c'est toujours la géographie régionale qui domine lors de cette période. Dans les années 1960 se développe la nouvelle géographie, ou analyse spatiale, qui a l'ambition de dégager des lois universelles à l'organisation de l'espace par l'homme. Cette approche positiviste occupera longtemps une place de choix au sein des courants géographiques. La géographie humaine est renouvelée à la fin des années 1970 par Yves Lacoste, créateur et fondateur de la revue Hérodote en 1976 (intitulée d'abord Stratégies géographies idéologies, puis en 1983 Revue de géographie et de géopolitique) et auteur de l'essai La Géographie, ça sert d'abord à faire la guerre. Il réhabilite alors une approche politique de la géographie, science dont il pense qu'elle peut être utilisée pour servir la cause des opprimés.\nUne certaine partie des géographes rejettent entièrement la géographie physique en affirmant la géographie comme une science sociale, cette vision est notamment relayée dans la revue Espace-Temps fondée en 1975 par Jacques Lévy et Christian Grataloup\nAujourd'hui, la géopolitique tend à analyser les conséquences de la mondialisation (géoéconomie) et la gestion des ressources naturelles (l'or ; l'or bleu - l'eau ; l'or noir - le pétrole ; l'or vert - la forêt) sont les objets les plus étudiés par la géographie humaine. La géographie humaine s'est aussi enrichie d'une approche culturelle (la géographie culturelle étudie les pratiques et les modes de vie des populations. La géographie du Genre héritière du postmodernisme et sous-branche de la géographie culturelle se développe en France depuis la fin des années 1990.\nJacqueline Beaujeu-Garnier soutient sa thèse en 1947 sur la géomorphologie des marges du Morvan (géographie physique) ainsi que sur la géographie humaine et régionale d'une vallée des Alpes autrichiennes, devenant la première femme docteure d'état en géographie en France. Elle est une pionnière en géographie de la population et en géographie urbaine.\n\n\n==== Géographie régionale ====\nLa géographie régionale est un courant géographique qui recherche à diviser l'espace en régions. La première étape de cette démarche consiste donc à regrouper sous cette appellation des lieux auxquels on attribue une certaine homogénéité. Ensuite, on pourra dire en quoi cette région est un individu géographique, en quoi elle se distingue des autres régions. Dès les années 1950 dans le monde anglo-saxon, puis avec un retard d'une dizaine, voire une vingtaine d'années en France, le paradigme de la région est vivement critiqué, notamment autour de la revue L'Espace géographique. Si l'approche régionale est considérée obsolète, c'est en vertu de bouleversements mondiaux comme la révolution des transports ou la mondialisation. Ces critiques vont favoriser l’émergence d'un courant qui se veut plus scientifique et objectif : l'école de l'analyse spatiale.\nDepuis les années 1970 et 1980, la géographie a vu se développer de nouvelles branches de sa discipline en accord avec une approche pluridisciplinaire (notamment l'utilisation des outils en provenance des disciplines économiques, mathématiques, sciences politiques, sociologiques, et informatiques), inspirée par les géographies scandinave, nord-américaine et anglaise, notamment à travers les approches variées de :\n\n\tLa géographie humaine inclut les sciences suivantes :\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n=== Champs relatifs ===\nL'économie spatiale est un domaine aux confins de la géographie économique et de la microéconomie. Elle étudie les questions de localisation économique, et les relations économiques entre le mondial (mondialisation) et le local (aménagement du territoire, pôle de compétence, délocalisation, etc.).\nLa notion d'échelle – ou approche multiscalaire – est essentielle en géographie : suivant que le géographe étudie toute la planète (petite échelle) ou seulement une partie de celle-ci (grande échelle), on parle de géographie générale ou de géographie régionale. De nos jours, on préfère toutefois parler de géographie thématique à la place de géographie générale et de géographie des territoires à la place de géographie régionale.\n\n\n=== Autres planètes ===\nAvant les années 1970, l'astronomie était une tout autre science. Depuis l'exploration spatiale, la géographie est aussi l'étude des caractéristiques physiques de tous les corps célestes ; aucun mot spécifique n'a été créé pour chacun. Depuis que leurs surfaces commencent à être connues, une même approche guide les études.\n\n\n==== Exception ====\nSeul sélénographie semble utilisé. Le terme aréographie pour Mars, par exemple, a bien été proposé, mais n'a rencontré que très peu de succès.\n\n\n== Méthodes et techniques ==\nLa géographie nécessite d'être capable de situer les différentes parties de la Terre les unes par rapport aux autres. Pour ce faire, de nombreuses techniques ont été développées à travers l'histoire.\nLongtemps[Combien ?] les géographes se sont posé quatre questions majeures lorsqu'ils regardaient la Terre, s'inscrivant en cela dans une démarche descriptive et analytique :\n\nQui : les individus et les sociétés produisent leur espace avec leurs valeurs et leurs modes de vie ;\nQuoi : l'impact de ces hommes, qu'il soit économique, social, ou environnemental, produit de leurs institutions, de la recherche, des techniques, des échanges ou encore de l'exploitation des ressources naturelles ;\nOù : le lieu de ces activités humaines ; plus généralement la raison des localisations ;\nQuand : la période historique où les individus ou les sociétés produisent des espaces qui s'ajoutent ou concurrencent les précédents.\nComme les interrelations spatiales sont la clé de cette science synoptique, les cartes sont un outil clé. La cartographie classique a été rejointe par une approche plus moderne de l'analyse géographique, les systèmes d'information géographique (SIG) informatisés.\nDans leur étude, les géographes utilisent quatre approches interdépendantes :\n\nSystématique - Regroupe les connaissances géographiques en catégories pouvant être explorées globalement.\nRégional - Examine les relations systématiques entre catégories pour une région ou un endroit spécifique de la planète.\nDescriptif - Indique simplement l'emplacement des entités et des populations.\nAnalytique - Demande pourquoi nous trouvons des caractéristiques et des populations dans une zone géographique spécifique.\n\n\n=== Cartographie ===\n\nLa cartographie étudie la représentation de la surface de la Terre et des activités humaines. Bien que d'autres sous-disciplines de la géographie s'appuient sur des cartes pour présenter leurs analyses, la réalisation de cartes est assez abstraite pour être considérée séparément. La cartographie est passée d'une collection de techniques de rédaction à une science réelle.[réf. nécessaire]\nLes cartographes doivent apprendre la psychologie cognitive et l'ergonomie pour comprendre quels symboles véhiculent les informations sur la Terre le plus efficacement, et la psychologie comportementale pour inciter les lecteurs de leurs cartes à agir sur l'information. Ils doivent apprendre la géodésie et les mathématiques assez avancées pour comprendre comment la forme de la Terre affecte la distorsion des symboles de carte projetés sur une surface plane pour la visualisation. On peut dire, sans trop de controverse, que la cartographie est la graine à partir de laquelle le plus grand domaine de la géographie a grandi.\n\nColette Cauvin, est une géographe des transformations cartographiques en France.\n\n\n=== Systèmes d'information géographique ===\nUn système d'information géographique (SIG) est un système d'information capable d'organiser et de présenter des données alphanumériques spatialement référencées, ainsi que de produire des plans et des cartes. Ses usages couvrent les activités géomatiques de traitement et diffusion de l'information géographique. La représentation est généralement en deux dimensions, mais un rendu 3D ou une animation présentant des variations temporelles sur un territoire sont possibles, incluant le matériel, l’immatériel et l’idéel, les acteurs, les objets et l’environnement, l’espace et la spatialité.\nL'usage courant du système d'information géographique est la représentation plus ou moins réaliste de l'environnement spatial en se basant sur des primitives géométriques : points, des vecteurs (arcs), des polygones ou des maillages (raster). À ces primitives sont associées des informations attributaires telles que la nature (route, voie ferrée, forêt, etc.) ou toute autre information contextuelle (nombre d'habitants, type ou superficie d'une commune par ex.). Le domaine d'appartenance de ces types de systèmes d'information est celui des sciences de l'information géographique.\nCet usage se vulgarise par la possibilité d'insérer facilement dans les pages d'un site Internet des cartes superposant des données à un fond cartographique, au moyen d'interfaces de programmation (API, pour Application Programming Interface). Les exemples les plus connus en sont Google Maps API, Microsoft®Bing Maps, etc. Pour les développeurs désireux d'intégrer les standards majeurs de l'information géographique, la bibliothèque libre JavaScript Leaflet réunit une large communauté d'organismes officiels et de spécialistes.\nMei Po Kwan fait partie, avec Susan Hanson de géographes qui promeuvent les Systèmes d’information géographique comme outils de la géographie féministe,.\n\n\n=== Télédétection ===\nLa télédétection désigne, dans son acception la plus large, la mesure ou l'acquisition d'informations sur un objet ou un phénomène, par l'intermédiaire d'un instrument de mesure n'ayant pas de contact avec l'objet étudié. C'est l'utilisation à distance de n'importe quel type d'instrument (par exemple, d'un avion, d'un engin spatial, d'un satellite ou encore d'un bateau) permettant l'acquisition d'informations sur l'environnement. On fait souvent appel à des instruments tels qu'appareils photographiques, lasers, radars, sonars, sismographes ou gravimètres. La télédétection moderne intègre normalement des traitements numériques mais peut tout aussi bien utiliser des méthodes non numériques.\n\n\n=== Méthodes de géographie quantitative ===\nLa géostatistique est une discipline à la frontière entre les mathématiques et les sciences de la Terre. Son principal domaine d'utilisation a historiquement été l'estimation des gisements miniers, mais son domaine d'application actuel est beaucoup plus large et tout phénomène spatialisé peut être étudié en utilisant la géostatistique.\nL'analyse des données géographiques : géographes, urbanistes et aménageurs utilisent de plus en plus de vastes tables de données fournies par les recensements ou par des enquêtes. Ces tables contiennent tant de données détaillées qu'une méthode est nécessaire pour en extraire les principales informations. C'est le rôle de l'analyse multivariée (appelée aussi, sous ses diverses formes : analyse des données, analyse factorielle ou analyse des correspondances ou Statistique multivariée). Il s'agit de transformer la table des données en matrice des corrélations des variables pour en extraire les vecteurs propres (ou facteurs ou composantes principales) et produire un changement de variables.\nPremier avantage : certaines variables du recensement (prix du sol, revenus, loyers, etc) seront remplacées par un facteur unique qui les résumera en opposant ménages riches/ménages pauvres dans la ville. Au lieu de dessiner plusieurs cartes redondantes, une carte du facteur représentant la structure sociale apportera une information synthétique. Deuxième avantage, l'expérience montre que l'opposition riches/pauvres constitue l'information fondamentale fournie par les recensements dans toutes les grandes villes analysées dans le monde. Toutes les cartes représentant des données socio-économiques répéteront cette structure. Mais il existe d'ordinaire d'autres phénomènes intéressants (opposition jeunes/vieux, retraités/actifs, quartiers récents/quartiers de peuplement ancien, quartiers ethniques, etc.) qui seront cachés par ce phénomène dominant. L'analyse multivariée produit de nouvelles variables orthogonales par construction, c'est-à-dire indépendantes. Ainsi, chaque facteur représentera un phénomène social différent. L'analyse permettra de reconnaître la structure cachée qui sous-tend les variables.\nCes méthodes sont très puissantes, indispensables mais offrent aussi de nombreux pièges. Différentes formes d'analyse multivariées sont utilisées, selon la métrique choisie (en général, métrique euclidienne usuelle ou Chi-deux, voir Test du χ²), selon la présence ou absence de rotations, selon l'utilisation de « communalités », etc. Aujourd'hui, l'utilisation d'ordinateurs puissants et de logiciels statistiques largement répandus rend ce type d'analyse tout à fait banal, ce qui multiplie les risques d'erreur.\nL’ethnographie est la science de l'anthropologie dont l'objet est l'étude descriptive et analytique, sur le terrain, des mœurs et des coutumes de populations déterminées. Cette étude était autrefois cantonnée aux populations dites alors « primitives ».\n\n\n== Métiers contemporains de la géographie ==\nLe Géographe de l'enseignement et de la recherche qui intervient essentiellement en tant que chercheur, Maître de conférences, expert ou professeur.\nLe Géographe « professionnel » qui intervient dans des domaines techniques et spécifiques de la géographie en tant que technicien (expert, généraliste, spécialiste) ou conseil.\nGéographie humaine\nAménageur : spécialiste en aménagement du territoire (urbanisme, développement économique, transport, environnement, paysage, etc.).\nGéopolitologue : spécialiste de l'étude de la géopolitique.\nUrbaniste : spécialiste en urbanisme (espaces urbains, planification territoriale).\nGéographie physique\nClimatologue : spécialiste de la climatologie.\nGlaciologue : spécialiste de glaciologie.\nGéomorphologue : spécialiste de géomorphologie.\nHydrologue : spécialiste d'hydrologie.\nOcéanographe : spécialiste d'océanographie.\nPédologue : spécialiste de pédologie.\nGéographie mathématique\nCartographe\nGéomaticien\nTopographe : spécialiste de la topographie.\nLe Géographe de l'armée est un professionnel chargé de réaliser la topographie et la cartographie militaire.\nL'urbaniste Pierre Merlin précise que « les géographes ont souvent eu tendance à considérer, en France notamment, l'aménagement (et en particulier l'aménagement urbain, voire l'urbanisme) comme un prolongement naturel de leur discipline. Il s'agit en fait de champs d'action pluridisciplinaires par nature qui ne sauraient être l'apanage d'une seule discipline quelle qu'elle soit. Mais la géographie, discipline de l'espace à différentes échelles, est concernée au premier chef ».\nIl convient par ailleurs de préciser que dans cette partie, les géographes dits « professionnels » et par conséquent spécialisés dans une science particulière ne sont pas ou ne se sentent pas toujours considérés comme des géographes selon la nature de leur formation et du rapprochement que l'on peut faire avec la géographie. En effet, la plupart de ces professions (et notamment celles de géographie physique et mathématique) ont été tellement approfondies pour devenir des sciences à part entière allant au-delà de la simple analyse spatiale, que l'on emploie des termes plus précis comme climatologue, océanographe, etc.\n\n\n== Organisation et diffusion du savoir ==\n\n\n=== Institutions, sociétés et centres de recherche ===\n\n\n==== Institutions, sociétés et centres de recherche du monde francophone ====\nSociété géographique royale du Canada (Canada)\nSociété de géographie (France) qui est la plus ancienne au monde.\nSociété de géographie de Genève (Suisse)\n\n\n==== Autres institutions, sociétés et centres de recherche ====\nSociété géographique royale du Canada (Canada)\nSociété américaine de géographie (États-Unis)\nNational Geographic Society (États-Unis)\nSociété hongroise de Géographie (Hongrie)\nSociété géographique italienne (Italie)\nSociété géographique royale d'Angleterre (Royaume-Uni)\n\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n=== Publications ===\n\n\n==== Revues de géographie ====\nSi les revues géographiques ont parfois des origines anciennes, bon nombre d'entre elles publient maintenant des versions électroniques.\n\nAnnales de géographie\nBulletin de l'Association des géographes français\nCahiers d'Outre-Mer\nCybergeo\nEchoGéo\nL'Espace géographique\nEspace populations sociétés\nGéocarrefour\nLa Géographie\nHérodote\nL'Information géographique\nJournal de la Société des océanistes\nMappemonde\nMéditerranée\nMosella\nNorois\nRevue de géographie alpine\nRevue géographique de l'Est\nSud-Ouest européen\nTerritoire en mouvement (remplace Hommes et Terres du Nord à partir de 2006)\nVertigO\nRevues internationales :\n\nThe Geographical Journal (en anglais)\n\n\n==== Bases de données ====\nPersée\n\n\n=== Enseignement scolaire ===\n\nL'enseignement de la géographie a fait l'objet de plusieurs études, notamment de la part de Jacques Scheibling ou d'Isabelle Lefort, montrant, depuis son apparition en tant que véritable discipline scolaire en France dans les années 1870 jusqu'à nos jours, son évolution en parallèle avec celle de la géographie savante.\nZonia Baber (1862-1956), est une géographe et géologue américaine connue pour avoir développé des méthodes pour l'enseignement de la géographie. Ses travaux mettent l'accent sur l'apprentissage par le terrain et l'expérimentation.\nDe 1870 à nos jours, de nombreuses réformes ont été mises en place faisant évoluer la discipline géographie dans l'enseignement secondaire et aussi à l'université. Ces réformes portent autant sur le contenu des programmes, qui évoluent en fonction des avancées de la géographie savante et du contexte social et historique (avec par exemple une domination de l'enseignement de la géographie régionale au début du XXe siècle, sous l'ère vidalienne), que sur les méthodes d'enseignement, l'aspect pédagogique, comme l'introduction dans les années 1960-1970 de manuels plus lisibles, avec de nombreuses photographies en couleurs. Aujourd'hui, l'enseignement de la géographie se définit plus en fonction de contraintes matérielles, comme les classes surchargées, la diminution du nombre d'heures, etc.\n\n\n== Géographes notoires ==\n\nAnaximandre (610 av. J.-C. - 546 av. J.-C.) : réalise l'une des premières cartes du monde connu.\nHérodote (480 av. J.-C. - 425 av. J.-C.).\nÉratosthène (276 av. J.-C. - 194 av. J.-C.) : calcul de latitudes et longitudes, calcul de la circonférence terrestre.\nStrabon (c. 60 av. J.-C. - c. 20 ap. J.-C.).\n\n\n== Notes et références ==\n\n\n=== Notes ===\n\n\n=== Références ===\n*\n\n\n== Voir aussi ==\n\n\n=== Bibliographie ===\n\n\n==== Dictionnaires ====\nAntoine Bailly, Robert Ferras, Denise Pumain (dir.), Encyclopédie de Géographie, Economica, Paris, 1992.\nAntoine Bailly & al., Les Concepts de la géographie, A. Colin, Paris, 1998.\nRoger Brunet & al., Les Mots de la géographie. Dictionnaire critique, 5e éd., Reclus-Doc. française, 2001  (ISBN 978-2-11-005943-7).\nJean-Paul Charvet & al., Dictionnaire de géographie humaine, Paris, Liris, 2000.\nPierre George, Dictionnaire de géographie, Presses universitaires de France, Paris, 1990.\nJacques Lévy, Michel Lussault, Dictionnaire de la géographie et de l'espace des sociétés, Belin, 2003.\nYves Lacoste, De la géopolitique aux paysages. Dictionnaire de la géographie, Collin, 2003.\nAntonio Da Cunha, Objet, démarches et méthodes : les paradigmes de la géographie, université de Lausanne, octobre 2006, avec la collaboration d'Olivier Schmid.\n\n\n==== Histoire ====\nPhilippe Pinchemel, Marie-Claire Robic, Jean-Louis Tissier (sous la direction de), Deux siècles de géographie française, Paris, Comité des travaux historiques et scientifiques, 1984, 380 p.\nPaul Claval, Histoire de la géographie, Paris, PUF, Que-sais-je ? no 65, 1995, 126 p.\nMarie-Claire Robic, Anne-Marie Briend, Mechtild Rössler (sous la direction de), Géographes face au monde. L’Union géographique internationale et les Congrès internationaux de géographie, Paris, L'Harmattan, 1996, 464 p.\nPaul Claval, Histoire de la Géographie française de 1870 à nos jours, Paris, Fernand Nathan, 1998, 544 p.\nJean-François Deneux, Histoire de la pensée géographique, Belin, 2006  (ISBN 978-2-7011-3767-4).\nMarie-Claire Robic (coordinatrice), Cyril Gosme, Didier Mendibil, Olivier Orain, Jean-Louis Tissier, Couvrir le monde. Un grand XXe siècle de géographie française, Paris, ADPF (Association pour la diffusion de la pensée française) - Ministère des Affaires étrangères, 2006, 232 p.\nPierre Singaravélou (dir.), L'Empire des géographes. Géographie, exploration et colonisation (XIXe-XXe s.), Paris, Belin, 2008.\n\n\n==== Essai ====\nEudes Girard et Thomas Daum, La géographie n'est plus ce que vous croyez, Talmont-Saint-Hilaire, Codex, 2010.\n\n\n==== Épistémologie ====\nJacques Scheibling, Qu'est-ce que la géographie?, Paris, Hachette, 1994, 200 p. ; 2e édition revue et augmentée, Hachette Supérieur, 2011, 256 p.\nAnne-Marie Gérin-Grataloup, Précis de géographie, Paris, Fernand Nathan, 1995 (réédité).\nRobert Marconis, Introduction à la géographie, Paris, Armand Colin, 1996, 222 p.\nPaul Claval, Épistémologie de la géographie, Paris, Fernand Nathan, 2001, 266 p.\nJean-Jacques Bavoux, La Géographie : objets, méthodes, débats, Paris, Armand Colin, 2002, 240 p.\nAlain Barré, Philippe Deboudt, Patrick Picouet, Réussir ses études en géographie. Méthodologie du travail universitaire, Paris, Belin, 2004, 223 p.\nArmand Frémont, Aimez-vous la géographie ?, Paris, Flammarion, 2005, 358 p.\n\n\n==== Enseignement de la géographie ====\nJacques Scheibling, Qu'est-ce que la géographie ?, Paris, Hachette, 1994, 200 p.\nIsabelle Lefort, La Lettre et l’esprit, géographie scolaire et géographie savante en France, Paris, Éditions du CNRS, 1992, 257 p.\nBernadette Mérenne-Schoumaker, Didactique de la géographie, organiser les apprentissages, Bruxelles, éditions De Boeck, 2012 (2e éd.), 301 p.  (ISBN 978-2-8041-7051-6).\n\n\n=== Articles connexes ===\n\n\n=== Liens externes ===\n\nRessource relative à la santé : Medical Subject Headings  \n\n\n==== Sites et revues scientifiques consacrés à la géographie de façon globale ====\nHypergéo : encyclopédie en ligne consacrée à l'épistémologie de la géographie.\nCybergeo, revue électronique de géographie (fondée en 1996).\n« Documentation photographique »(Archive.org • Wikiwix • Archive.is • Google • Que faire ?) (collection de la Documentation française fondée en 1949).\nFestival international de la géographie - Saint-Dié-des-Vosges.\nGeogir.fr, le site internet consacré à la géographie, à l'environnement et au tourisme.\nM@ppemonde, revue gratuite disponible uniquement en ligne, se présentant comme une « revue trimestrielle sur l'image géographique et les formes du territoire ».\n Portail de la géographie"
        },
        {
            "pageid": 187769,
            "ns": 14,
            "title": "Catégorie:Wikipédia:ébauche géographie",
            "content": "Cette catégorie est une catégorie d'ébauche.  Elle est remplie par le modèle « {{Ébauche}} » avec le paramètre « géographie » :"
        },
        {
            "pageid": 7690187,
            "ns": 14,
            "title": "Catégorie:Branche de la géographie",
            "content": ""
        },
        {
            "pageid": 1100919,
            "ns": 14,
            "title": "Catégorie:Liste en rapport avec la géographie",
            "content": ""
        },
        {
            "pageid": 15729212,
            "ns": 14,
            "title": "Catégorie:Catégorie d'un élément géographique",
            "content": ""
        },
        {
            "pageid": 8198144,
            "ns": 14,
            "title": "Catégorie:Géographie par lieu géographique",
            "content": "Cette catégorie permet de mener des études sur la géographie d'endroits organisés par type de lieu."
        },
        {
            "pageid": 16130471,
            "ns": 14,
            "title": "Catégorie:Géographie dans l'art et la culture",
            "content": ""
        },
        {
            "pageid": 4340876,
            "ns": 14,
            "title": "Catégorie:Distinction en géographie",
            "content": ""
        },
        {
            "pageid": 12233575,
            "ns": 14,
            "title": "Catégorie:Docteur en géographie",
            "content": ""
        },
        {
            "pageid": 130558,
            "ns": 14,
            "title": "Catégorie:Géographe",
            "content": ""
        }
    ],
    "Politique": [
        {
            "pageid": 2338,
            "ns": 0,
            "title": "Politique",
            "content": "Notion polysémique, la politique recouvre :\nla politique en son sens plus large, celui de civilité ou Politikos, désigne ce qui est relatif à l'organisation d'un État (en grec : polis, en latin : civitas) et à l'exercice du pouvoir dans une société organisée ;\nen général, la politique d'une communauté, d'une société, d'un groupe social, au sens de Politeia, se conforme à une constitution rédigée par ses fondateurs qui définit sa structure et son fonctionnement (méthodique, théorique et pratique). La politique porte sur les actions, l’équilibre, le développement interne ou externe de cette société, ses rapports internes et ses rapports à d'autres ensembles. La politique est donc principalement ce qui a trait au collectif, à une somme d'individualités ou de multiplicités. C'est dans cette optique que les études politiques ou la science politique s'élargissent à tous les domaines d'une société (économie, droit, sociologie…) ;\ndans une acception plus restrictive, la politique au sens de Politikè ou d'art politique, se réfère à la pratique du pouvoir, soit donc aux luttes de pouvoir et de représentativité entre des hommes et femmes de pouvoir, et aux différents partis politiques auxquels ils peuvent appartenir, tout comme à la gestion de ce même pouvoir.\n\n\n== Histoire ==\n\n\n=== Anthropologie ===\n\nSelon Georges Balandier, l'anthropologie politique « tend à fonder une science du politique, envisageant l'homme sous la forme de l'homo politicus et recherchant les propriétés communes à toutes les organisations politiques reconnues dans leur diversité historique et géographique ». Mais la question de savoir si on peut parler d'organisation politique dans des sociétés qui n'ont pas créé d'État fait encore débat.\nLes anthropologues ont distingué quatre systèmes politiques primitifs, qui se situeraient en dehors de toute logique étatique ou institutionnelle :\n\nles bandes de chasseurs-cueilleurs nomade. Du fait de leur taille réduite (guère plus d'une centaine d'individus), et de leur mobilité, ces bandes n'éprouveraient nul besoin d'instituer des autorités politiques permanentes ;\nles sociétés lignagères. Décrit par Evans-Pritchard, dans son étude classique sur les Nuer, ce système politique implique la réunion de plusieurs groupes familiaux, au sein desquels les aînés possèdent une légitimité particulière, qui les rend aptes à dénouer un conflit ;\nles sociétés à notables charismatiques. Ici, le pouvoir est représenté ponctuellement par quelques personnalités reconnues pour leurs qualités morales ou leurs attributs matériels. Toutefois, cette dignité demeure individuelle, et n'est pas transmissible héréditairement ;\nles sociétés à chefferies. Quelques individus exercent un pouvoir incontestable et héréditaire, dont l'étendue reste cependant variable.\n\n\n=== États proche-orientaux ===\nDurant la période d'Obeïd en Mésopotamie (6500 à 3750 av. J.-C.), vont apparaître et se combiner deux faits majeurs.\nLe phénomène de structuration économique initié au début de l'ère Néolithique atteint un niveau critique, qui entraîne l'émergence d'une nouvelle cellule sociologique, la ville, ainsi que l'avènement d'inégalités sociales. L'invention de l'écriture permet l'administration rationnelle d'un espace donné : « les écrits, en effet, permettent de porter au loin des messages, de noter des comptes, de conserver des archives, toutes ressources susceptibles d'aider au plus haut degré l'administration de l'État ». L'avènement conjoint de ce modèle sociologique et de cette technologie intellectuelle contribue à l'émergence d'une structure humaine nouvelle, l'État, et de son corollaire, la politique.\nÀ partir de l'an 3000 av. J.-C., les cités-États qui apparaissent en Mésopotamie semblent privilégier des régimes politiques assez proches de la monarchie constitutionnelle, voire de la république. Un poème sumérien étudié en particulier par Samuel Noah Kramer fait ainsi état de la présence de deux assemblées à Uruk, l'une, l'assemblée des anciens, s'apparentant à une sorte de sénat, l'autre à une assemblée du peuple. La légitimité du roi d'Uruk semble dépendre étroitement de ces deux assemblées : il ne déclare en effet la guerre à la cité de Kish qu'après avoir reçu au moins le soutien de l'assemblée du peuple. Qui plus est, l'attribution du pouvoir royal n'est que rarement héréditaire. Le terme sumérien pour désigner le roi, Lugal, consiste en effet en l'association de la racine Lu, homme et gal, grand. Ce qui importe ici avant tout ce sont les qualités morales et nullement l'appartenance héréditaire : Sargon d'Akkad n'obtient ainsi le trône que grâce à ses vertus royales.\nProgressivement le pouvoir monarchique se renforce, tout à la fois en puissance (les premières cérémonies de sacre, qui impliquent une légitimité de droit divin apparaissent au début du IIe millénaire av. J.-C.) et en étendue (les cités-États sont absorbées par de grands royaumes). Les institutions démocratiques et républicaines, notées par Kramer, tombent de fait en désuétude. Le renforcement de l'autorité monarchique va favoriser, au cours du premier quart du IIe millénaire av. J.-C., la mise en place d'une administration et d'une jurisprudence normalisées, évolution illustrées par les codes d'Ur-Nammu (vers -2100), de Lipit-Ishtar (vers -1930) et d'Hammurabi (vers -1750), ainsi que les Lois d'Eshnunna (vers -1760). Certes ces premiers corpus juridiques n'ont aucune visée exhaustive et s'apparentent plutôt par leur style à des recueils de prescriptions morales. Toutefois le fait qu'ils entendent corriger les traditions orales dans une optique rationnelle représente une rupture importante : « la situation dans une société proche-orientale est donc très différente de celle des sociétés sans histoires, où les coutumes sont absolument fixes et où le changement est subi et non organisé ».\nL'affermissement de grands États centralisés et rationalisés induit l'organisation de relations internationales. De la fin du IIe millénaire av. J.-C. à -1100, un espace allant de l'Égypte à Élam, et de l'Arabie au royaume Hittite est régi par un système diplomatique élaboré : le système d'Amarna. Fondé sur un relatif équilibre géopolitique entre quatre ou cinq grandes puissances, ce système dispose de sa lingua franca, l'akkadien, et de ses protocoles propres. Ainsi, les « différents rois entretenant des relations diplomatiques sont réputés appartenir à une seule et même grande famille ou grande maison (…) les rois de statut identique se traitent de frères, ceux de moindre envergure sont les fils ou les serviteurs des premiers ». Les invasions successives des peuples de la mer mettent fin à cette construction politique élaborée.\nIl semble ainsi que les États proche-orientaux ont forgé la quasi-totalité des formes et structures politiques. Pour autant si le politique est certes un objet bien établi, il ne s'agit en aucun cas d'une pensée ni d'une théorie politique : « jamais le mythe, la loi, la coutume dans leur ensemble ne deviendront objet de débat explicite, parce qu'elles continuent à relever du sacré, et du sacré seul ». Les hommes d'États proche-orientaux se préoccupent avant tout des politiques, de la gestion des affaires administratives, et fort peu de la Politique, de l'ordre étatique dans son ensemble — car l'ordre est partie prenante de l'ordre divin dans son ensemble, et ne saurait être contesté, discuté ou simplement considéré.\n\n\n=== Cité grecque ===\n\nEn dépit des précédents proche-orientaux, l'origine de la politique se confond généralement avec celle de la pensée politique et donc de fait, avec la Cité grecque. Ainsi l'helléniste anglais Moses Finley, a-t-il pu affirmer que la politique « est une des activités les moins répandues dans le monde pré-moderne ». Il s'agit en effet, « d'une invention grecque, ou, pour être plus précis, une invention que firent séparément les Grecs, les Étrusques ou les Romains ».\nTout au long du IIe millénaire av. J.-C., la Grèce apparaît comme une simple continuité périphérique du système d'Amarna. Comme le note en effet Jean-Pierre Vernant : « la Méditerranée ne marque pas encore de part et d'autre de ses rives, une coupure entre l'Orient et l'Occident. Le monde égéen et la péninsule grecque se rattachent sans discontinuité (…) d'une part au plateau anatolien (…) de l'autre (…) à la Mésopotamie et à l'Iran ». De la sorte, le premier État grec connu, le royaume mycénien, s'apparente par de nombreux traits aux monarchies proche-orientales contemporaines. Il s'agit en effet d'une royauté bureaucratique, caractérisée par une régulation quasi-maniaque de la vie sociale. De plus, le roi ou anax possède une autorité essentiellement militaire et religieuse. Aussi, la politique à l'ère mycénienne prend ainsi la forme d'une activité essentiellement administrative, inscrite dans un cadre cosmogonique plus large.\nEffectif à partir du XIIe siècle av. J.-C. le déclin du monde mycénien va entraîner un redéploiement complet des structures politiques initiales : l'anax disparaît et les potentats locaux, dits basileus ne conservent le plus souvent que des prérogatives religieuses. Le reflux de la souveraineté monarchique va favoriser deux forces sociales jusqu'ici quasiment exclues du jeu politique : « d'une part les communautés villageoises, et de l'autre une aristocratie guerrière ». Les dissensions fréquentes entre ces deux forces vont rendre nécessaire la mise en place du débat politique ou agôn, sur une place publique. Le pouvoir cesse dès lors de dépendre d'un centre unique, pour être le produit d'une délibération constante : « l'archè ne saurait plus être la propriété exclusive de qui que ce soit ; l'État est précisément ce qui a dépouillé tout caractère privé, particulier, ce qui, échappant au ressort des genè, apparaît déjà comme l'affaire de tous ».\nProgressivement s'instaure une entité politique d'un genre nouveau : la polis ou cité. Elle se caractérise par trois traits principaux : l'usage du discours rationnel, la publicisation des actes politiques, et la croyance en l'égalité des citoyens devant la loi (ou isonomie). Cette instauration invalide de fait les vieilles coutumes orales, qui régulaient jusqu'alors le jeu politique et social. Plusieurs législateurs, regroupés sous l'appellation générique de Sept sages vont promouvoir en conséquence une nouvelle éthique citoyenne, qui témoigne d'une volonté de rationaliser la justice : le criminel n'est ainsi plus jugé coupable vis-à-vis de sa victime, mais de la cité entière.\nPendant moral de cette éthique, la « sôphrosunè » ou modération, fait converger l'ensemble des structures sociales vers un « juste milieu ». Solon impose ainsi une égalité géométrique, ou homoneia, des corps de citoyens, en accord avec les rapports de types musicaux (2/1, 3/2, 4/3) : la première classe de citoyen reçoit ainsi cinq cents mesures de blé, quand la dernière classe n'en reçoit que deux cents. Par la suite les démocrates comme Clisthène généralisent le principe de l'égalité absolue, fondée sur le rapport 1/1 : chaque citoyen devient dès lors l'entité indivisible d'un corps unique : la cité. Afin de garantir ce principe, Clisthènes procède à une réforme profonde de l'espace civique athénien, en regroupant les quatre tribus traditionnelles en dix tribus : purement conventionnelle, cette division administrative achève de rationaliser la cité.\n\n\n=== Moyen Âge et époque moderne ===\n\nAu Moyen Âge, le régime politique le plus répandu est celui de la monarchie héréditaire ou élective. Le roi est alors le suzerain de ses vassaux. Dans certains États, le régime prend la forme de la monarchie absolue de droit divin dont l'archétype est, en France, le roi Louis XIV ; la Russie vit jusqu'en 1917 sous le régime de l'autocratie hérité de l'Empire byzantin tandis que la Pologne connaît une forme de république aristocratique, la Liberté dorée, qui s'achève avec les partages de la Pologne.\nDans les royaumes barbares du Haut Moyen Âge, le souverain est avant tout un chef de guerre et la fonction royale est pratiquement réservé aux hommes. Au cours du Moyen Âge central, la consolidation du système féodal permet parfois à une femme d'hériter de la souveraineté sur un État ou un fief. Le système successoral tend à privilégier les hommes : les femmes sont exclues de la succession dans certains pays (loi salique en France, Bulle d'or de 1356 dans le Saint-Empire) ou n'y accèdent qu'en l'absence d'héritier masculin (Angleterre, Espagne, Portugal, Russie jusqu'à la fin du XVIIIe siècle) ; les légistes ne font qu'entériner une vision générale de la femme médiévale comme inférieure à l'homme. Cependant, jusqu'au XIXe siècle, il n'est pas rare que la mère d'un souverain mineur ou absent exerce la régence. En France, le pouvoir monarchique prend fin sous une régente, Eugénie de Montijo, épouse de Napoléon III.\n\n\n=== Fin du XVIIe siècle - début du XXe siècle ===\n\nÀ partir de la fin du XVIIe siècle et au XVIIIe siècle, dans plusieurs pays occidentaux, la monarchie est remise en cause au bénéfice d'un régime parlementaire plus ou moins élargi qui évoluera vers la démocratie représentative.\nLa Grande-Bretagne est la première à adopter un régime de monarchie constitutionnelle lors de la Glorieuse Révolution de 1688 : la Déclaration des droits (Bill of Rights) de 1689 limite l'autorité du pouvoir royal tout en garantissant aux habitants un certain nombre de droits politiques et personnels. La guerre d'indépendance des États-Unis, de 1775 à 1783, permet aux colonies britanniques d'Amérique du Nord de devenir la première république démocratique moderne. En France, la Révolution française éclate en 1789. Elle se concrétise dans un premier temps par une Déclaration des droits de l'homme et du citoyen ; le pouvoir n'émane plus du monarque par l'intermédiaire de droits divins mais du peuple souverain et trouve sa légitimité dans les « droits naturels, inaliénables et sacrés », que possède tout homme (et toute femme) dès la naissance. Les privilèges accordés à la noblesse sont abolis lors de la nuit du 4 août 1789. Le régime passe à la monarchie constitutionnelle (1791), puis à la république (1792) ; la France oscille entre ces deux formes jusqu'à la victoire des républicains en 1879.\nEn Europe, le congrès de Vienne et la Sainte-Alliance de 1815 rétablissent un équilibre européen fondé sur le modèle monarchique qui évoluera, plus ou mois vite selon les pays, vers une forme constitutionnelle. Le libéralisme politique et économique, modèle largement dominant au Royaume-Uni et aux États-Unis, a plus de mal à s'imposer en France et en Allemagne où la révolution de 1848 est un échec ; les unifications allemande et italienne s'accompagnent de la construction d'un État national monarchique non dépourvu de penchants autoritaires, de même que le monde ibérique, tandis qu'en Russie, après l'échec du mouvement décembriste de 1825, la révolution russe de 1905 n'aboutit qu'à une ébauche inaboutie de régime constitutionnel.\nCes deux types de régimes (monarchie constitutionnelle et République) vont se répandre progressivement, avec plus ou moins de succès, dans une majorité d'états. Ainsi, des révolutions constitutionnelles se produisent en Iran en 1905-1911, dans l'Empire ottoman en 1908 et en Chine en 1911-1912.\n\nLe XXe siècle est marqué par une plus forte présence des femmes dans la vie politique bien que leur place reste minoritaire dans la plupart des pays.\n\n\n=== À partir de la fin de la Seconde Guerre mondiale ===\nAprès la Seconde Guerre mondiale, de nouveaux droits sont proclamés. En France, la Constitution de 1946 définit dans son préambule des droits à caractère essentiellement social (droit à obtenir un emploi, droit de grève, droit d'obtenir de la collectivité des moyens convenables d'existence). Ces droits sont conservés dans la Constitution de 1958.\nL'apparition et l'intensification des problèmes écologiques à partir des années 1970 soulèvent la question des droits et devoirs des citoyens en rapport à leur environnement. Les politiques des États commencent à prendre en compte des objectifs de développement durable, croisant les aspects économiques, sociaux, et environnementaux, selon la description donnée au Sommet de la Terre de Rio de Janeiro en 1992. L'Union européenne met en place une politique de développement durable. En France, les droits et devoirs liés à l'environnement sont proclamés dans la Charte de l'environnement de 2004, faisant de ce pays le premier État au monde à leur attribuer une valeur constitutionnelle.\nSelon les Nations unies, la proportion de sièges que les femmes occupent dans les parlements nationaux dépasse pour la première fois 25 % en 2020, les taux étant plus faibles dans la région du Moyen-Orient et de l’Afrique du Nord à 17,8 %, toutes chambres confondues, de même que dans le Pacifique, hormis en Nouvelle-Zélande. Au 1er janvier 2021, la parité est atteinte dans trois pays : Cuba, aux Émirats arabes unis et au Rwanda.\n\n\n== Philosophie ==\n\n\n=== Chine ===\nLa pensée politique chinoise émerge, comme en Grèce archaïque dans un contexte de crise. La décomposition des structures politiques traditionnelles suscite en effet dans les deux cas une prise de conscience philosophique et politique. Effectif à partir du VIIIe siècle av. J.-C., le déclin de l'empire des Zhou, permet aux divers fiefs et seigneuries de s'émanciper et de constituer de multiples royaumes indépendants.\n\n\n=== Grèce antique ===\n\nLa pensée politique de Socrate se résume à deux apports fondamentaux. Premièrement, le développement d'une méthode critique d'évaluation de la connaissance politique. À la différence de Protagoras, Socrate affirme que la vérité existe. Toutefois, cette vérité n'est pas dogmatique : on ne peut l'atteindre que par l'exercice constant d'un esprit critique. Rétif aux concepts, Socrate s'efforce d'instiller le doute quant à la moralité et l'efficacité des systèmes politiques : « en mettant ses interlocuteurs en contradiction avec eux-mêmes, il montre que l'opinion est (…) incapable de servir de base à la délibération et à la décision politique, ce qui ruine le postulat athénien (…) de l'universelle compétence des citoyens ». Deuxièmement, la conceptualisation de la morale comme un objet de science. Il y a, selon Socrate, des lois morales universelles, que l'on ne saurait découvrir que par une éducation véritablement philosophique. Rarement innée, la science du gouvernement s'apprend ; si bien que pour Socrate, la Politique apparaît comme un véritable métier.\nInitialement dérivée des théories socratiques, la philosophie politique de Platon repose sur la question du bien et des facultés de l'âme, question qui touche tant aux conduites humaines individuelles qu'à l'éducation : il n'y a pas, pour Platon, de vertu que l'on pourrait acquérir de manière individuelle, et la philosophie elle-même est une activité de la pensée qui suppose toujours une éducation et des conditions politiques qu'il reste à définir. Pour Platon, la philosophie politique est alors inséparable de la philosophie morale (comme c'est le cas pour toute la philosophie grecque ancienne), si bien que la politique, par le moyen de l'éducation, a pour but de prendre soin de l'âme des citoyens. Pour ces raisons, la politique est la science du bien en général, et elle est donc supérieure à toutes les autres sciences et techniques, c'est pourquoi Platon la désigne comme technique royale.\nPar opposition à Socrate qui part du monde des idées, duquel nos âmes viendraient, pour en déduire des applications concrètes, Aristote tendrait à vouloir s'appuyer sur l'observation du réel pour en déduire des principes théoriques. Cette approche aristotélicienne est aussi vraie en politique. Pour Aristote, l'homme est fait pour vivre en communauté politique. Pour lui, la Cité est voulue par la nature et est donc inhérente à tout groupe humain, selon le principe que l'homme est par nature un être destiné à vivre en cité (ἄνθρωπος φύσει πολιτικὸν ζῷον / anthropos phusei politikon zoon).\nDans son œuvre La Politique, Aristote analyse l'origine et le fonctionnement des différents régimes politiques de son époque, le IVe siècle av. J.-C., pour définir le meilleur d'entre eux, qui doit donner naissance à la Cité idéale. La Philosophie hellénistique va marquer un net retrait par rapport à ces préoccupations politiques.\n\n\n=== Théologie ===\n\n\n==== Judaïsme ====\n\n\n==== Islam ====\n\n\n==== Christianisme ====\n\n\n=== Renaissance et sécularisation ===\n\nMachiavel incarne une rupture absolue par rapport à la tradition politique chrétienne et, à ce titre, apparaît comme le premier penseur politique moderne. Selon lui, en effet, « un prince nouveau, dans une cité, ou une province conquise doit faire toute chose nouvelle »[réf. incomplète]. Pour Machiavel, trois principes doivent diriger le Politique : la force, le respect des lois, la ruse. Pour Machiavel, le prince n'a pas besoin de faire profession d'homme de bien.\nAussi, pour Machiavel, le prince se doit d'être efficient, autrement dit le prince se doit d'être utile. Ce qui est une révolution pour l'époque car il sous-entend que le prince n'est pas nécessairement utile, que le prince n'est pas une finalité en soi mais que sa place et sa fonction se doivent d'être méritées[réf. nécessaire].\n\n\n=== XVIIe – XVIIIe siècles ===\n\nLa question de l'État de nature et du contrat social s'inscrit dans un contexte particulier de la pensée occidentale. À partir du XVIIe siècle, s'amorce en effet une contestation des thèses politiques aristotéliciennes, à partir d'un contre-argumentaire humaniste. Pour Aristote en effet : « L'État est un fait de nature », et « Naturellement, l'homme est un animal politique », par le simple fait qu'il maîtrise le langage rationnel, et est ainsi apte, plus qu'aucun autre animal à se regrouper en société : « l'homme est infiniment plus sociable que tous les autres animaux qui vivent en groupe ». Il s'ensuit que « La Nature pousse donc instinctivement tous les hommes à l'association politique » et que « ἄνθρωπος φύσει πολιτικὸν ζῷον » — « l'homme est un animal politique ».\nA contrario, « pour l'âge moderne, l'humanité de l'homme ne dépend pas essentiellement de son rapport à autrui dans la construction d'un ordre juste ». Dans l'esprit de l'humanisme, la relation entre l'homme et la morale ou la nature n'est en effet pas d'ordre collectif, mais individuel. Dans la mesure où l'homme précède l'État, celui-ci ne saurait être un fait de nature, et n'a pu être instauré qu'à un moment précis de l'histoire humaine, pour répondre à des besoins non moins précis.\nUne telle position conventionnaliste existait déjà au temps d'Aristote. Outre un certain nombre de sophistes cités par ce dernier et dont l'œuvre n'a pas traversé les temps, tels que Lycophon, Épicure partageait ces conceptions. Pour ce dernier, l'État fut instauré par convention (Sunkhétai), afin de permettre aux philosophes de s'adonner à la science, sans redouter l'insécurité des rapports humains : « Épicure voit le fondement de la cité, et plus généralement des liens de droit, dans des contrats ou des conventions liant des sujets autonomes […] les hommes s'associent parce qu'ils ont éprouvé la douleur de subir des dommages […] l'homme n'est pas un animal naturellement politique ». Le hasard de la transmission des textes a contribué à occulter cette position conventionnaliste, alors relativement fréquente.\nRéhabilité par Hugo Grotius, qui établit l'existence, dans son Traité du droit de la guerre et de la paix, d'un droit naturel préexistant aux divers droits politiques, l'État de nature est exposé clairement par Samuel Pufendorf dans le premier livre du Droit de la nature et des gens. Pour celui-ci, l'État ne fait que confirmer positivement un système de droit et de devoir préexistant en l'homme : il existe des lois naturelles, telles que la loi de sociabilité, qui régissent les rapports humains. Toutefois, pour que ces lois naturelles puissent réellement être appliquées, l'intervention d'une autorité politique est nécessaire : « Le but des législateurs de cette terre est de régler les actions extérieures de chacun, le mieux qu'il est possible ».\n\n\n== Idéologie ==\n\nLa première mention du terme idéologie remonte à 1801, lors de la publication des Éléments d'idéologie par Antoine Destutt de Tracy. Toutefois, le sens que Tracy appliquait à ce néologisme n'avait rien de politique : il s'agissait d'une science des idées et des sensations : « je veux dans cet écrit, non pas vous enseigner, mais vous faire remarquer tout ce qui se passe en vous quand vous pensez, parlez, et raisonnez ». Elle ne recouvre en fait son sens actuel qu'à partir de l'Idéologie Allemande de Karl Marx, écrit en 1846, mais publié beaucoup plus tard.\n\n\n=== Libéralisme ===\n\nLe libéralisme est un courant de pensée de philosophie politique, né d'une opposition à l'absolutisme et au droit divin dans l’Europe des Lumières (XVIIIe siècle), qui affirme la primauté des principes de liberté et de responsabilité individuelle [réf. nécessaire] sur le pouvoir du souverain maître. Il repose sur l’idée que chaque être humain possède des droits fondamentaux qu'aucun pouvoir ne peut violer. En conséquence, les libéraux veulent limiter les obligations sociales imposées par le pouvoir et plus généralement le système social au profit du libre choix de chaque individu.\nLe libéralisme repose sur un précepte moral qui s'oppose à l'assujettissement de l'individu, d'où découlent une philosophie et une organisation de la vie en société permettant à chaque individu de jouir d'un maximum de liberté, notamment en matière économique. Pour la plupart des libéraux, la dichotomie entre « libéralisme économique » et « libéralisme politique » n'existe donc pas, puisqu'il s'agit de l'application d’une même doctrine dans des domaines différents.\nAu sens large, le libéralisme prône une société fondée sur la liberté d'expression des individus dans le respect du droit du pluralisme et du libre échange des idées. Elle doit joindre d'une part dans le domaine économique, l'initiative privée, la libre concurrence et son corollaire l'économie de marché, d'autre part, des pouvoirs politique et économique bien encadrés par la loi et les contre-pouvoirs. Elle valorise donc le mérite comme fondement de la hiérarchie. Cela suppose idéalement un état de droit où sont respectées les minorités jusqu'à la plus petite, l'individu, l'État n'étant que le garant de ce respect et devant rendre des comptes de son action.\nCependant en fonction des pays et du contexte politique, le libéralisme pourra se manifester de façon fort diverse, voire opposée. Le libéral pourra ainsi être, selon le lieu, voire en fonction des moments, celui qui exige de l'État qu'il brise un traditionalisme religieux ou social oppresseur pour l'individu (caste, statuts, discriminations et privilèges…) ou qu'il intervienne pour donner à chacun une véritable capacité d'action économique (bridée par un monopole, la pauvreté, le manque d'éducation de crédit ou autre), ou inversement celui qui s'oppose à l'intervention du pouvoir. Cela provient notamment de l'ambiguïté du terme entre l'anglais liberal qui désigne les progressistes partisans notamment d'un interventionnisme, et \"libéral\" terme français qui désigne le mouvement philosophique et opposé à l'intervention de l'État hors du régalien. S'ils se réfèrent tous deux aux Lumières, celui du monde anglo-saxon est héritier du libéralisme des années 1920 (comme celui de Beveridge, ou amorcé par John Stuart Mill, notamment dans son ouvrage De la liberté), quand il renvoie en France à l'école autrichienne et au tournant du libéralisme après les années 1970, souvent appelé néo-libéralisme.\nLes limites à fixer à l'action de l'État, ainsi que les modalités de l'action publique (notamment aux rôles respectifs de l'action administrative et de la loi), seront spécialement sujet à débat au sein même. La plupart des libéraux considèrent que l'action de l'État est nécessaire à la protection des libertés individuelles, dans le cadre de ses fonctions régaliennes, et nombre d'entre eux (comme Adam Smith, Raymond Aron, Karl Popper ou Benedetto Croce) acceptent et même recommandent certaines interventions de l'État dans l'économie, notamment en matière de contrôle et de régulation. À l'opposé, les libertariens (ou anarcho-capitalistes) refusent à l'État toute légitimité dans quelque domaine que ce soit.\n\n\n=== Socialisme ===\n\nLe socialisme est un type d'organisation sociale fondé sur la propriété collective (ou propriété sociale) des moyens de production,,, par opposition au capitalisme.\nIl est l'objectif de divers courants apparus et développés depuis le XIXe siècle, et ayant abouti aujourd'hui aux différents courants marxistes et anarchistes, ainsi qu'aux sociaux-démocrates. La répartition des biens et services peut se faire en fonction de la production de chaque individu (collectivisme, travail aux pièces) ou en fonction des besoins de chaque individu (communisme, prise au tas). Les États marxistes ont une économie collectiviste, alors que le communisme est préconisé par les anarchistes. Le mouvement socialiste recherche une justice sociale, condamne les inégalités sociales et l’exploitation de l’homme par l’homme, défend le progrès social, et prône l'avènement d'une société égalitaire, sans classes sociales.\nPour leur part, les universitaires Georges Bourgin et Pierre Rimbert définissent le socialisme comme « une forme de société dont les bases fondamentales sont les suivantes :\n\nPropriété sociale des instruments de production ;\nGestion démocratique de ces instruments ;\nOrientation de la production en vue de satisfaire les besoins individuels et collectifs des hommes ».\n\n\n=== Fascisme ===\n\nÀ l'origine, le fascisme (en italien fascismo) désigne un mouvement politique italien apparu à la fin de la Première Guerre mondiale. Le 23 mars 1919, Benito Mussolini réunit un certain nombre de dissidents du PSI, et entreprend de former un « Faisceau de combat » (fascio di combattimento). Par « Faisceau », Mussolini entendait alors un mouvement spontanéiste, dans la lignée du syndicalisme révolutionnaire italien. Le terme appartenait de fait à un vocabulaire d'extrême-gauche. En concurrence directe avec d'autres organisations révolutionnaires (dont le parti communiste naissant), les Fascii essaient de récupérer une clientèle de droite. Ces tentatives de récupération rassurent la bourgeoisie italienne, qui, à l'issue de la répression des mouvements ouvriers, considère ce mouvement comme un moindre mal.\nL'idéologie de ce mouvement est délicate à définir : on peut y voir schématiquement une synthèse du nationalisme et du syndicalisme révolutionnaire, mais de multiples contextes et mouvements idéologiques ont en fait préludé à sa création : le renouveau de l'irrationnel, le futurisme, l'antisémitisme… Du fait de sa nature composite, le fascisme a peiné à constituer une doctrine originale et nouvelle : « au début, le fascisme se distingue difficilement d'autres mouvements ultra-minoritaires ». Les contemporains eux-mêmes étaient sceptiques vis-à-vis d'un programme « attrape-tout », qui capte aussi bien des thématiques marxistes, nationalistes et réactionnaires.\nComme le note l'historien Pierre Milza, cette diversité idéologique nous oblige à penser le fascisme comme une pluralité : « Il n'y a pas un mais des fascismes ». Cette pluralité est d'abord spatiale : « Sur un fond commun (…) il y a éclosion de mouvements politiques d'un type nouveau, proches parents les uns des autres, mais en même temps dotés d'une spécificité qui tient au passé, aux traditions, aux structures des pays dans lesquels ils se développent ». Elle est aussi, et surtout, temporelle. Milza identifie ainsi quatre étapes de développement du fascisme :\n\nLe premier fascisme constitue une réaction spontanée de la classe moyenne face à des menaces diverses et contextualisées : prolétarisation, mouvements révolutionnaires.\nLe second fascisme résulte d'une alliance entre le premier fascisme et la grande bourgeoisie, ce qui suppose que cette dernière se sente également menacée. Cette alliance entraîne la liquidation de certains courants gauchisants (le squadrisme italien, les SA allemands…).\nLe troisième fascisme représente le fascisme de gouvernement. Il hérite des contradictions initiales du mouvement. À la différence des dictatures de droite classiques, le fascisme ne peut simplement consacrer la domination des élites en place. Il doit en effet satisfaire certaines classes sociales inférieures, qui ont constitué sa clientèle originelle : la petite bourgeoisie fournit ainsi les principaux cadres du nouveau régime, tandis que de multiples institutions sociales (corporations) s'efforcent d'intégrer le prolétariat à la société fasciste. Ces dispositions contradictoires ne peuvent se concilier que dans le cadre d'un grand dessein national. Aussi, par ses contradictions mêmes, le fascisme est acculé à la guerre.\nLe quatrième fascisme ou full fascism s'efforce de remplacer l'ordre bourgeois et libéral par un ordre nouveau. Ce remplacement présuppose la mise en place d'un pouvoir totalitaire (l'État-SS) et le conditionnement généralisé des individus.\nConsidéré sous l'angle de ces deux pluralités, le Fascisme devient un concept politique générique, qui, au-delà du régime de Mussolini caractérise le Nazisme de Hitler, la Ligue de Cuza de Codreanu, la Heimwehr autrichienne, le BUF d'Oswald Mosley, le PPF de Jacques Doriot… Il semblerait même que l'on puisse parler, après 1929, d'une internationale fasciste. En 1932, Mussolini affirme ainsi dans un discours tenu à Milan : « Dans dix ans, l'Europe sera fasciste ou fascisée ». Un peu plus tôt, l'un des caciques du régime, Asvero Gravelli, allait jusqu'à déclarer dans sa revue Antieuropa : « le fascisme est le fossoyeur de la vieille Europe. Voici que surgissent les forces de l'Internationale fasciste ». C'est dans cet esprit que Mussolini créa les CAUR (Comitati d'Azione per l'Universalità di Roma) en 1933, afin de fédérer les mouvements qui se réclament du fascisme italien. Cette initiative resta lettre morte : foncièrement nationalistes, les fascismes ne sauraient cohabiter. Ce n'est que par l'expansionnisme de quelques États fascistes, que le fascisme put s'imposer internationalement.\nÀ la fin de la Seconde Guerre mondiale, les mouvements fascistes cessent de constituer une alternative politique viable. Aussi bien leur compromission dans des crimes contre l'humanité que « l'avènement d'un système capitaliste infiniment plus internationalisé que par le passé » hypothèque définitivement leur avenir idéologique. Bien que « l'époque du fascisme » soit close, ces mouvements continuent, marginalement, d'exister.\n\n\n=== Néolibéralisme ===\n\nle néolibéralisme a pour idéologie de privatiser le secteur public et limiter l'intervention de l’État dans le système économique pour favoriser le profit du secteur privé. Ronald Reagan et Margaret Thatcher ont mis en avant ces pratiques durant leur mandat dans les années 1980.\nLes grands axes des thèses néolibérales ont aussi pour objet d'abaisser le coût du travail et de contrôler l'évolution de la masse monétaire pour prévenir les effets inflationnistes.\n\n\n== Organisation du pouvoir ==\n\n\n=== Régimes politiques selon la légitimité ===\nPour s'exercer sans rencontrer d'opposition, le pouvoir politique s'est toujours attaché à justifier de sa légitimité. Celle-ci peut reposer sur :\n\nla tradition et l'hérédité, cas des régimes traditionnels, des monarchies et de systèmes aristocratiques ;\nla volonté divine, cas des théocraties mais aussi de la monarchie de droit divin ;\nl'expression du droit des peuples et des individus (souveraineté populaire) ; c'est le cas des démocraties mais aussi de régimes autoritaires mais se réclamant de la volonté populaire (certains régimes fascistes) ;\nle mérite et la qualité des dirigeants. C'est la théorie induite par les régimes gouvernés par les « sages » (cas de certains pouvoirs locaux ou tribaux), d'oligarchie bourgeois (suffrage censitaire) ou techniciens ;\nle souci d'efficacité de l'action politique, officiellement pour le bien du peuple même si celui-ci n'est — temporairement ou de façon permanente — pas jugé apte à exercer le pouvoir. Ce sont les régimes inspirés du positivisme, les technocraties ;\nle hasard (stochocratie).\nHistoriquement, il semble que dans un certain nombre de premières civilisations, le pouvoir politique n'apparaît pas distinct du pouvoir religieux (voir par exemple la Politique dans l'Égypte antique). La confusion du pouvoir politique et religieux, ou la soumission du pouvoir politique au religieux, ou la très grande proximité des deux, s'appelle théocratie.\n\n\n=== Autres typologies de régimes politiques ===\nLa politique consiste d'abord en l'organisation du pouvoir dans la société. On distingue entre plusieurs Systèmes de prise de décision.\nOn distingue traditionnellement entre monarchies et républiques, une distinction institutionnelle finalement jugée peu pertinente de nos jours compte tenu du fait de la diversité des types de monarchie (de la monarchie parlementaire scandinave ou britannique à la théocratie saoudienne) et de types de républiques.\nLes distinctions actuelles reposent plus sur le degré de démocratie, la démocraticité, caractérisant le régime. On distingue ainsi les régimes démocratiques, autoritaires, ou totalitaires.\n\n\n=== Pouvoirs politiques ===\nLe pouvoir politique est constitué d'au moins deux fonctions distinctes :\n\nun pouvoir exécutif, qui prend des décisions et, une fois celles-ci adoptées, les applique et les fait appliquer au travers d'une administration ;\nun pouvoir législatif (une ou des assemblées), assurant la représentativité du peuple ou du moins de l'élite, qui accepte ou non les décisions de l'exécutif et peut parfois en proposer lui-même.\nÀ cela s'ajoutent des pouvoirs non directement « politiques » mais qui participent au système politique :\n\nle pouvoir judiciaire, chargé de juger ;\nle pouvoir médiatique, est souvent qualifié de quatrième pouvoir compte tenu de sa capacité supposée ou réelle à influencer l'opinion publique.\nDans la pensée politique démocratique occidentale (née en Grande-Bretagne puis formalisée par le philosophe français Montesquieu), qui sert actuellement[C'est-à-dire ?], au moins sur le papier, de modèle au niveau international, les pouvoirs doivent être séparés. Dans les démocraties on distingue ainsi entre :\n\nle régime présidentiel ;\nle régime parlementaire ou d'assemblée ;\ndes formes mixtes.\n\n\n=== Pouvoirs territoriaux ===\nLes modes d'organisation territoriale constituent un autre aspect de l'organisation du pouvoir. À cet égard, on distingue :\n\nl'État unitaire qui pratique la centralisation du pouvoir ;\nles États unitaires pratiquant une dose plus ou moins importante de décentralisation du pouvoir ;\nles États fédéraux, pratiquant le fédéralisme, conférant un pouvoir important aux divisions territoriales (appelées État, land, région, province, etc.).\nClassiquement, les États comprennent deux grands types de subdivisions territoriales :\n\nde larges entités régionales (au sens français) correspondant souvent à des entités historiques bien déterminées, ayant parfois connu au cours de leur histoire des périodes d'indépendance ou d'autonomie (telles, en Europe, la Bretagne, l'Écosse, la Catalogne, la Bavière, etc.) ;\nles municipalités ou villages, constituant historiquement la cellule de base de la vie locale.\nEntre les deux, existent parfois des échelons politique ou administratifs tels, en France, le département et les cantons.\nAu-dessus du cadre national, existent des structures politiques plus ou moins souples « régionales » (telle l'Union européenne) et mondiales (telle l'Organisation des Nations unies).\n\n\n== Politiké : art et pratique ==\n\n\n=== Vie politique ===\nLes modalités d'accession au pouvoir sont, comme l'organisation du pouvoir, déterminées par les institutions et sont une part du régime politique. Cependant, elles dépassent également la question de l'organisation du pouvoir pour les raisons suivantes :\n\nl'accession au pouvoir dépend également de la vie politique, c'est-à-dire notamment, dans les sociétés contemporaines, de la vie des partis politiques ; d'où également la question des relations entre le pouvoir et ses oppositions ;\nla question de l'accession au pouvoir dépasse également celle de son organisation puisque l'accession peut se produire par une forme qui n'a pas été prévue par les institutions. Ce sont toutes les formes de prises de pouvoir violentes : coup d'État et révolution.\nLa politique politicienne désigne la part du politique qui n'est pas conforme aux principes établis. Ce terme est employé par exemple lorsqu'un politique (individu ou parti) s'occupe de ses affaires, de celles de ses confrères et de celles de son parti, plutôt que de celles de la cité.\n\n\n==== Mode d'accession au pouvoir ====\n\nLes différents modes d'accession au pouvoir dépendent de la légitimité du régime en place (lire supra) ainsi que du type de régime (supra). Sur le papier, le système de l'élection, fondé sur le présupposé théorique de la démocratie, s'est imposé au XXe siècle comme le système standard international de désignation des dirigeants. Il existe des exceptions avec en particulier des monarchies (Arabie saoudite, sultanat de Brunei, etc.).\nAu sein du système démocratique, on distingue notamment entre :\n\ndémocratie directe ou démocratie indirecte avec mandat impératif ou représentatif (délégation de pouvoir) ;\ndifférentes de mise en œuvre, à travers divers types de système électoral.\n\n\n==== Modes d'action politique ====\nDans les régimes démocratiques, le mode normal d'accession au pouvoir est la participation aux élections.\nD'autres modes d'expression non violents existent également (manifestations, grèves, Non-violence, Désobéissance civile, Conflit non-violent, boycott, campagnes de presse, cybermouvements, etc.).\nLe domaine de la politique recouvre cependant également des modes d'actions politique violents : coup d'État, révoltes, Révolution. Certains actes violents sont considérés comme du Terrorisme par ceux contre lesquels ils sont destinés et des actes de Résistance par ceux qui le pratiquent.\n\n\n==== Partis politiques ====\nDes factions politiques opposées ont toujours existé au sein de tous les régimes, souvent plus fondés sur le soutien à une personnalité du régime (souvent un prince ou un grand seigneur au sein des monarchies). À partir de la Révolution française au moins (mais bien plus tôt en Angleterre avec les tories et whigs), s'est mis en place un modèle fondé sur des partis politiques ou mouvements politiques théoriquement plus soudés par des idées politiques plus que par le soutien à une personnalité.\nLes systèmes politiques multipartistes se sont répandus à travers le monde, introduisant des notions politiques nouvelles :\n\nl'alternance politique pacifique à la tête du gouvernement entre partis ;\nla distinction entre une majorité et une opposition ;\nla distinction entre deux principaux camps politiques : la droite et la gauche, ou, de manière caricaturale, les conservateurs et les réformateurs, etc.\nLes mouvements politiques peuvent être associés dans leur action avec des mouvements sociaux, des associations, etc. L'article 4 de la Constitution de 1958 régit l'organisation des partis politiques en France.\n\n\n==== Rôle politique des médias ====\nLes médias ont toujours joué un rôle important dans la vie politique, constituant un relais de la vie politique envers le public. L'influence des médias a conduit à appeler la presse le « quatrième pouvoir ». Le pouvoir politique a suivi les évolutions technologiques, utilisant la presse, la radio (les « causeries au coin du feu » de Franklin Delano Roosevelt), le cinéma (les films de propagande des régimes totalitaires), la télévision puis Internet et le marketing direct.\nEn France, l'utilisation de médias touchant directement le grand public, hormis la presse, était considérée au début du XXe siècle avec suspicion par les milieux républicains pour qui le lien direct de la tête de l'exécutif avec le peuple relevait de la tradition bonapartiste. L'utilisation de la radio tout d'abord (l'appel du 18 juin du Général de Gaulle) puis de la télévision par le général de Gaulle a brisé ces tabous.\nLe développement des médias a conduit à une modification des comportements des hommes politiques, une tendance appelée peoplelisation au début des années 2000. Il s'agit de montrer une autre image (non institutionnelle et plus intime) de l'Homme politique et de mettre en scène sa vie privée afin de créer une image favorable et un lien de proximité avec l'électeur potentiel. En France, on peut tracer ses prémices dans les années 1970 lorsque Valéry Giscard d'Estaing mit sa femme en scène et se fit filmer en train de jouer de l'accordéon.\n\n\n=== Exercice du pouvoir ===\nLa politique menée par un gouvernement recouvre l'ensemble de ses décisions prises à l'échelon politique ou à l'échelon administratif. Cette politique « générale » se subdivise en politiques sectorielles dont les principales sont la politique sociale, la politique économique, la politique étrangère, etc. Un concept qui peut être affiné (politique du logement, politique culturelle, politique agricole). L'action politique s'exerce concrètement à travers l'émission de règles (de niveau politique ou administratif) appliquées ou contrôlées par une administration.\n\n\n== Notes et références ==\n\n\n== Annexes ==\n\n\n=== Bibliographie ===\nPierre Pellegrin (dir.), « Les Politiques », dans Aristote, Œuvres complètes, Flammarion, 2015 (ISBN 978-2-0812-1810-9). \nPhilippe Nemo, Histoire des idées politiques dans l'Antiquité et au Moyen Âge, PUF, coll. « Quadrige », 2007\nSamuel Noah Kramer, L'Histoire commence à Sumer, Arthaud, 1986\nFrancis Joannès, Dictionnaire de la civilisation mésopotamienne, Éditions Robert Laffont, 2001\nPhilippe Raynaud, Dictionnaire de philosophie politique, PUF, 2006\nPierre Milza, Les Fascismes, Éditions du Seuil, 2001\nGuillaume Bernard, Jean-Pierre Deschodt et Michel Verpeaux, Dictionnaire de la politique et de l'administration, PUF, 2011\nJean-Pierre Vernant, Les Origines de la pensée grecque, Paris, Presses universitaires de France, coll. « Quadrige », 2007, 10e éd. (1re éd. 1962), 133 p. (ISBN 978-2-13-054565-1).\nLéo Strauss, Pensées sur Machiavel, Payot, 1982.\nDenis Langlois, La Politique expliquée aux enfants (et aux autres) dessins de Plantu, L'Atelier, 2002, Scup, 2017.\n\n\n=== Articles connexes ===\n\n\n==== Articles généraux ====\n\n\n==== Sciences autour de la politique ====\n\n\n==== Différents types de politiques ====\n\n\n==== Organisations para-politiques ====\n\n\n==== Politique internationale et diplomatie ====\n\n\n=== Liens externes ===\n\nRessource relative à la santé : Medical Subject Headings \nRessource relative à la bande dessinée : Comic Vine \nRessource relative à l'audiovisuel : France 24  \n\n Portail de la politique   Portail de la société   Portail de la sociologie   Portail du droit"
        },
        {
            "pageid": 494247,
            "ns": 100,
            "title": "Portail:Politique",
            "content": ""
        },
        {
            "pageid": 12002760,
            "ns": 0,
            "title": "Américanisme (politique)",
            "content": "L'américanisme politique est une tendance idéologique qui prône un engouement pour les États-Unis et tout ce qui est américain, soit l'adoption des idées et manières américaines.\nSon antonyme est l'antiaméricanisme.\n\n\n== Définitions ==\nLe terme « américanisme » est défini ainsi par le Centre national de ressources textuelles et lexicales (CNRTL) :\n\n« Caractère de ce qui est américain, état d'esprit américain »\n\nLe Larousse en donne cette définition :\n\n« Engouement pour les États-Unis, leurs idées, leurs manières de vivre. »\n\n\n== Historique ==\n\n\n=== Aux États-Unis ===\n\nÀ la suite de la Première Guerre mondiale, le patronat, qui mit en œuvre un « plan américain » pour revenir sur les récentes avancées du mouvement ouvrier, comme les xénophobes, qui doutaient de la loyauté des immigrés, défendaient « un américanisme à 100 % ».\nSi, comme l’a écrit Hofstadter, le destin des États-Unis était « de ne pas avoir des idéologies, mais d’en être une », l’américanisme américain fut toujours en débat et susceptible d’être l’enjeu de luttes. L’américanisme n’a jamais été l’exclusive propriété des conservateurs, xénophobes, anti-syndicalistes ou expansionnistes, car il fut aussi employé par les progressistes et même par des radicaux.\n\n\n=== En France ===\n\nPour tenter de comprendre l'américanisme politique en France, il faut remonter à l'une de ses sources : la Première Guerre mondiale. Depuis la fin du mois de juin 1917, date des premiers pas de l’armée américaine en France, les États-Unis connaissent un prestige croissant aux yeux des Européens. La démonstration de leur force militaire et industrielle peut véhiculer l’image d’une nation jeune, puissante, se révélant capable de concurrencer, voire de surpasser, le pouvoir économique et politique de la « vieille Europe ». On assiste alors, côté européen, à la construction d’un mythe américain, celui d’une « terre de modernité » qui agit à la fois de modèle et de repoussoir en particulier pour la société française qui se trouve alors profondément affectée par la guerre.\nLe « modernisme » représenté par l’Amérique se double de l’idée d’un mode de vie hédoniste, trépident, industriel, qui correspond aux nouvelles « valeurs » mises de l’avant par les intellectuels et l’avant-garde au sortir de la Grande Guerre. En effet, la crise que connaît la civilisation française se traduit par une négation de l’ancien monde qui aurait mené aux horreurs des tranchées, et la quête de nouveautés qui anime les artistes témoigne d’une société en recherche d’elle-même.\nLes États-Unis, victorieux et prospères, deviennent ainsi un modèle social et politique dès le tournant des années vingt. Modèle qui sera, selon les discours, adopté ou rejeté dans les décennies qui suivront, dans l’entreprise de redéfinition de la culture française.\nLa thèse du chercheur Richard Kuisel traite spécialement de l'américanisme social et politique dans la France de 1945 à 1970 ; il écrit :\n\n« l'américanisation a constitué un dilemme pour les Français parce qu'elle posait des questions de fond concernant l'indépendance et l'identité de la France. Elle posait la question suivante : Comment les Français peuvent-ils atteindre les niveaux de prospérité que connaissaient les États-Unis, tout en gardant leur identité ? En particulier, comment peuvent-ils accepter l'aide économique et les conseils des Américains ; emprunter des pratiques économiques américaines ; consommer des produits américains ; imiter la politique sociale américaine ; s'habiller, lire, parler et, peut-être pire encore, manger comme les Américains, sans perdre leur originalité française ? Pour les Français de l'après-guerre, se lancer dans le consumérisme américain, c'était, semble-t-il, mettre en question l'idée que la nation avait d'elle-même ou son identité, autant que son indépendance vis-à-vis de l'hégémonie américaine. Pour reprendre les termes d'un journaliste : « Un bistro qui sert du coca au lieu de Beaujolais est-il encore français ? » »\n\nEn France, comme dans d'autres pays d'Europe de l'Ouest, l'américanisme politique est intimement lié à une politique économique expansionniste américaine. Comme par exemple au moment de l'introduction de la boisson Coca-Cola sur le marché français en 1949-1950.\nAinsi, le plan Marshall fut un levier majeur d'américanisation de la vision politique et industrielle française au sortir de la Seconde Guerre mondiale. Afin de mieux comprendre la portée du Plan Marshall, ouvrons ici une courte parenthèse sur son historique : contrairement à certains a priori populaires, le Plan Marshall n'a pas pour déclencheur les dégâts infligés par la guerre mondiale et un désir de réparation, mais tire son application politique de la guerre civile grecque qui, tout au long de la guerre et particulièrement après guerre, oppose communistes ex-résistants d'un côté et économico-libéraux et monarchistes de l'autre. Ces résistants communistes, qui ont tenu les maquis contre les forces du IIIe Reich quelques années plus tôt, se sentent légitimes pour former un gouvernement d'après-guerre en Grèce. Les prémices du plan Marshall (ou, en anglais, ERP \"European Recovery Program\") vient principalement soutenir l'armée grecque contre les forces communistes et non la reconstruction économique du pays. À une échelle européenne, le Plan Marshall, plus grand levier économique institutionnel de l'américanisme jamais produit à l'extérieur des frontières américaines, sert en partie à soutenir les opposants idéologiques des formations communistes et ex forces de résistance, partout en Europe de l'Ouest et en Méditerranée. Selon le président des États-Unis Harry S. Truman, il s'agit avant tout \"d'endiguer le communisme\" et sa politique est en partie influencée par le politologue américain George F. Kennan.\n\nAu début des années 1950, l'incitation à la productivité industrielle sur le modèle américain est donc accrue par des visites de plusieurs milliers d'hommes d'affaires, de dirigeants syndicaux et d'hommes politiques français aux États-Unis, afin d'y copier les techniques et d'en importer les principaux produits manufacturiers.\nGrâce à des sondages d'opinion effectués au début des années 1950, nous disposons de données concernant les sentiments populaires vis-à-vis de l'Amérique et de l'américanisation. Ces enquêtes montrent que, dans l'ensemble, les Français considéraient l'Amérique comme une force constructive en Europe. Cependant, la politique étrangère américaine en Europe était aussi fortement considérée comme une menace pour l'indépendance de la France. La présence militaire américaine était tolérée plutôt que désirée – deux personnes sur trois souhaitaient que l'influence de l'Amérique diminue.\nCette vague d'américanisme d'après-guerre fut néanmoins tempérée, voire contrée, en France par l'arrivée à la présidence de la République du général de Gaulle en janvier 1959, qui instaura peu à peu pendant une dizaine d'années des outils économiques et structurels en faveur d'une plus grande indépendance du pays.\n\nLe secteur de la presse et des médias est lui aussi affecté par dʼimportantes évolutions auxquelles lʼinfluence américaine nʼest pas étrangère. Dès les années trente, certaines formules telles que le magazine photographique ou le magazine féminin, inventés aux États-Unis, avaient été reprises par la presse populaire hexagonale. L'après-guerre ne fait que confirmer le mouvement, avec la création et le succès de journaux tels que Paris Match en 1949, L'Observateur en 1950, L'Express en 1953, ou, dans le secteur culturel proprement dit, Jazz Magazine, fondé en 1954.\nLa mise en perspective des analyses historiques et sociologiques suggère que l'américanisme politique en France est historiquement renforcé ou influencé par chaque mission militaire menée par les forces armées américaines. Depuis l'après Seconde guerre mondiale, cette influence est reportée sur les secteurs économiques et financiers.\n\n\n=== En Allemagne ===\nL'américanisme politique en Allemagne est intimement lié aux facteurs économiques nationaux.\nDès 1921, dans les principales universités allemandes, plusieurs cours sur l'américanisme ont été mis en place.\nIdéologiquement, au XIXe siècle, une partie importante de la philosophie allemande reste en accord avec la politique extérieure expansionniste américaine, comme en témoignent les écrits d'Engels sur le sujet.\nIl est important de noter que, sur le plan de la politique internationale, il fait alors écho à l’idée américaine de la Manifest Destiny, telle qu’elle ressort lors de la guerre contre le Mexique : grâce à la « valeur des volontaires américains », « la splendide Californie a été arrachée aux indolents Mexicains, qui ne savaient pas quoi en faire » ; mettant à profit les gigantesques conquêtes nouvelles, « les énergiques Yankees » donnent une nouvelle impulsion à la production et à la circulation de la richesse, au « commerce mondial », à la diffusion de la « civilisation ».\n\nÀ l'issue de la Seconde Guerre mondiale, l'Allemagne est le quatrième plus gros bénéficiaire du plan Marshall, avec presque 1,4 milliard de dollars, dont plus de 1,1 milliard de « dons ».\nPlus tard, au cœur de la guerre froide, les relations germano-américaines des années 1980 constituent une priorité pour la diplomatie américaine de Ronald Reagan. S'ouvre alors une guerre des idées pour la \"conquête des esprits\" en Allemagne de l'Ouest.\nDans un climat d’insécurité marqué par la crise des euromissiles (1983), les Allemands de l’Ouest faisaient l’objet d’une surveillance particulière de la part des agents d’information américains, en partie afin de \"contrer\" l'influence russe (TASS) sur le territoire.\nUn certain américanisme est ainsi visible dans la politique économique allemande depuis la chute du mur. L'ordolibéralisme germanique dépend en effet en grande partie de l'accès au marché américain. Avec une préférence historique pour les traités multilatéraux, stables et à long terme, l'élection de Donald Trump à la présidence américaine a quelque peu atténué les possibilités de vision à moyen-terme de l'Allemagne envers les États-Unis. Certains analystes allemands projettent même une scission des volets économiques et politiques de la politique américaniste allemande.\nNéanmoins, un manifeste de 2017 publié dans l'hebdomadaire Die Zeit prévoit :\n\n« La politique allemande requiert à présent quelque chose dont elle n'avait pas besoin auparavant: une stratégie américaine. Cette politique responsable envers les États-Unis doit se construire sur la durée, au-delà de l'ère Trump. »\n\nEn 2018, les États-Unis possèdent une ambassade et cinq consulats en Allemagne (Berlin, Düsseldorf, Francfort, Hambourg, Leipzig et Munich). Étant l'une des plus grosses ambassades américaines d'Europe, celle de Berlin a un impact tant symbolique que sur le terrain diplomatique.\n\n\n=== Aux Pays-Bas ===\nLes maîtres mots de la politique étrangère hollandaise ont été, historiquement, ceux de commerce et de navigation, mais aussi de neutralisme et d'abstention, d'idéalisme international. Ce positionnement politique fut quelque peu abandonné après la Seconde Guerre mondiale: les « alliés » américains furent accueillis comme des libérateurs et sont demeurés depuis, aux yeux des gouvernements néerlandais, les piliers de la politique étrangère et de défense néerlandaise.\nDans le domaine de la coopération économique, l'intégration européenne fut considérée comme un moyen de favoriser les intérêts commerciaux, mais aussi comme un instrument politique pour préserver et renforcer l'unité européenne.\nNéanmoins, la volonté néerlandaise de faire adhérer le Royaume-Uni à la Communauté s'expliquait presque exclusivement par des arguments économico-politiques, indépendamment de l'OTAN.\nMilitairement, l'américanisme néerlandais prend donc sa source dans la victoire « alliée » de la Seconde Guerre mondiale. Mais elle inclut également le soutien logistique des Pays-Bas aux missions américaines ou de l'OTAN bien au-delà de son propre territoire.\nPour comprendre les réactions suscitées par ces interventions militaires, qu’elles aient été menées à l’initiative de l’OTAN, des Nations unies ou des États-Unis, il faut les replacer dans le contexte de la question de la récente contribution des Pays-Bas à des opérations internationales de maintien de la paix qui renvoie, en particulier, à l’implication du pays dans le massacre de Srebrenica (Bosnie-Herzégovine), perpétré en juillet 1995. Alors en effet, les troupes serbes sous le commandement du général Mladić avaient déporté et tué 8 000 Musulmans (dits également Bosniaques) sans que les Casques bleus néerlandais, censés protéger la ville, ne leur opposent de résistance.\nÉconomiquement, il est à noter que le Groupe Bilderberg, un des plus influents groupement du patronat, de politiciens et de médias transatlantiques, fut fondé en 1954 dans la ville néerlandaise de Oosterbeek par le prince Bernhard des Pays-Bas et le milliardaire David Rockefeller. Le caractère secret et la non-publication des rapports de ces conventions laisse parfois place à des critiques sur la nature synarchique de ce groupement.\nDe plus, de nombreuses entreprises américaines exerçant sur le territoire de l'Union européenne possèdent un bureau aux Pays-Bas, notamment pour des raisons fiscales.\n\n\n=== En Grande-Bretagne ===\nLes relations historiques qu'entretiennent la Grande-Bretagne et les États-Unis d'Amérique sont complexes et profondes. La population américaine ayant, dans son ensemble, adopté la langue anglaise depuis le XIXe siècle, les États-Unis n'en ont pas moins développé une politique d'influence politique et économique sur l'ancienne puissance coloniale dont ils ont été libérés par la guerre d'indépendance.\nCe lien est appelé « la relation spéciale » (The Special Relationship, en anglais) par les politiciens et experts des relations anglo-américaines des deux côtés de l'Atlantique. Elle fut tissée notamment par le premier ministre britannique Winston Churchill et le 33e président des États-Unis Harry S. Truman.\nContrairement à ce que laisse parfois entendre la mythologie qui entoure cette relation spéciale, celle-ci n’avait en 1945 aucun caractère évident ni immédiat. Certes, une collaboration très étroite entre les administrations civiles et militaires des deux pays s’était mise en place une fois que les États-Unis étaient entrés en guerre après l’attaque de Pearl Harbor (décembre 1941). Elle avait abouti à l’établissement d’un état-major conjoint (Combined Chiefs of Staff en anglais) destiné à la préparation des opérations militaires successives et qui permit la participation des scientifiques britanniques à l’élaboration de la première arme atomique, au sein du projet Manhattan. Mais au lendemain de la guerre, les priorités des deux gouvernements divergeaient quelque peu. Le gouvernement américain de Truman, et surtout le Congrès américain, entendait ramener les troupes au pays au plus vite afin de retrouver une complète indépendance d’action. De leur côté, les nouveaux dirigeants travaillistes à Londres n’étaient pas convaincus au départ de la nécessité de garder des liens étroits avec les États-Unis une fois ce conflit mondial terminé.\nEn effet, les Anglais nourrissaient encore l’espoir de pouvoir garder des relations cordiales avec l’Union soviétique de Staline, et la fin brutale du Prêt-Bail le 17 août 1945 semblait montrer que les Américains n’étaient pas des alliés fiables. En effet, dès l’annonce radiodiffusée de la capitulation japonaise, le président Truman suspendait immédiatement l’aide financière et militaire américaine, aide qui dans le cadre de la loi Prêt-Bail de mars 1941 avait, seule, permis d’éviter l’effondrement économique du Royaume-Uni face aux coups des puissances de l'Axe. Toutes les illusions entretenues des deux côtés de l’Atlantique (aussi bien chez les conservateurs et les travaillistes britanniques que chez les démocrates ou les républicains américains) sur une possible reconversion en douceur et coordonnée avec les États-Unis de l’économie de guerre anglaise furent balayées de manière aussi brutale qu’inattendue.\n\n\n==== Âge d’or de la « relation spéciale » (1947-1962) ====\nLe contexte international dans lequel s’inscrivaient les liens anglo-américains se modifia perceptiblement au cours de l’année 1946, lorsque les relations de l’Union soviétique avec l'ensemble de l'Europe de l'Ouest et les États-Unis se détériorèrent peu à peu. Le début de la guerre froide est traditionnellement attribué à l’année 1947, au cours de laquelle le président Truman et ses principaux conseillers élaborèrent la doctrine d'« endiguement » des partis et pays communistes partout dans le monde. Il fut alors décidé que le plan Marshall soit étendu de la Grèce vers ce qui allait être toute l’Europe de l’Ouest.\nLa guerre froide provoqua un net rapprochement entre Washington et Londres, fondé sur une vision commune du « danger communiste » et de la nécessité de renforcer leur projet de défense de l’Europe occidentale.\nC’est à travers cette vision que doit se comprendre la signature du traité de Bruxelles entre le Royaume-Uni, la France, l’Italie et les pays du Benelux en 1948, qui prévoyait une telle coopération.\nLa crise de Berlin de 1948 a permis le retour d’une collaboration militaire active sur le terrain. Les avions britanniques prirent une part importante au pont aérien qui permit de ravitailler l’ancienne capitale du Reich entourée par les troupes soviétiques. À cette occasion, 90 Super-forteresses B29 américaines furent stationnées sur le sol anglais. Elles précédaient la signature d’une série d’accords de défense, appelés Burns-Templer, signés entre les États-Unis et le Royaume-Uni, qui prévoyaient notamment l’échange d’informations et de renseignements militaires entre les deux pays. C'est le commencement de la centralisation des données de renseignement des 5 pays anglophones nommés Five Eyes (les cinq yeux).\nLa guerre de Corée, entre 1950 et 1953, donna un nouvel élan à cette collaboration. Londres, tout en prêchant une certaine modération auprès des Américains, notamment rétrospectivement concernant le bombardement de Corée du Nord, participa au conflit en envoyant un contingent de la Royal Navy sur place. L’État-major commun de la Seconde Guerre mondiale (Combined Chiefs of Staff Committee) fut réactivé, tandis que les Britanniques se lançaient dans un vaste programme de réarmement conventionnel, malgré les difficultés économiques qu’ils traversaient. Programme qui fut partiellement financé par les États-Unis : 4,7 milliards de livres anglaises furent dépensés au total en trois ans par les Américains.\nEn soutenant l’effort américain en Asie malgré des désaccords sur la politique à conduire dans la région, les gouvernements de Clement Attlee (1945-1951) puis Churchill (1951-1955) entendaient surtout obtenir un appui réciproque des États-Unis en Europe. En 1950, Londres avait refusé, malgré les pressions américaines, de participer au projet franco-allemand de Communauté européenne de défense (CED), qui aurait abouti à la création d’une armée supranationale d'Europe de l'Ouest et permis de régler le délicat problème du réarmement allemand (auquel la France était opposée si l'Allemagne se réarmait dans un cadre purement national, mais que souhaitaient vivement les Américains pour renforcer la sécurité du continent). Le projet de CED fut finalement repoussé par l’Assemblée nationale française en 1954, et le problème du réarmement allemand fut réglé la même année selon les souhaits d’Anthony Eden : la RFA retrouva sa complète souveraineté et devint membre de l’OTAN, tandis que les Européens de l’Ouest créaient l’Union de l'Europe occidentale (UEO) pour prendre la suite de l’Organisation du traité de Bruxelles. L’UEO avait vocation à s’occuper des questions de sécurité européenne, mais en pratique fut totalement vidée de substance par l’OTAN jusqu’à la fin des années 1980.\nL'américanisme militaire britannique de la guerre froide trouva son paroxysme en 1957, lorsque les Soviétiques envoyèrent dans l’espace un premier satellite Spoutnik, qui prouvait qu’ils étaient capables d’envoyer des missiles intercontinentaux, et ce plusieurs années avant les États-Unis, et donc d’atteindre potentiellement le sol américain. Des capacités balistiques américaines furent alors installées au Royaume-Uni, notamment en Écosse et sur des sous-marins britanniques porteurs de la dissuasion nucléaire.\n\n\n==== Le déclin de la « relation spéciale » (1962-1979) ====\nPlusieurs facteurs expliquent un certain déclin relatif de la relation spéciale au cours des années 1960 et 1970, après l’apogée apparente de 1962. La principale raison, quoique progressive, fut le déclin politique et économique du Royaume-Uni qui apparut au grand jour dans les années 1960, particulièrement sous le gouvernement du premier ministre Harold Wilson (1964-1970). La perte progressive de l’Empire avait déjà entraîné une diminution de la présence britannique dans le monde, ce qui avait forcé les États-Unis à remplir des vides qu’auraient autrement exploités des partis communistes (en Asie, en Afrique et en Amérique du Sud). Ce retrait fut accéléré par les difficultés économiques grandissantes du Royaume-Uni, notamment le déficit chronique de sa balance des paiements, qui aboutirent à la dévaluation de la Livre Sterling de 1967. Alors qu’en 1951 le PNB du Royaume-Uni le classait au troisième rang mondial, il n’était plus que sixième vingt ans plus tard, dépassé par le Japon, la RFA et la France. En 1967, Wilson annonça la fermeture de toutes les bases militaires situées à l’Est de Suez, essentiellement en Asie et dans le golfe Persique, et la concentration des moyens britanniques sur la défense du continent européen. Cette décision ne pouvait que déplaire aux Américains, puisqu’elle les privait des moyens d’un proche allié.\n\n\n==== Le renouveau de la relation spéciale ? (1979-1990) ====\nMargaret Thatcher et Ronald Reagan, arrivés respectivement au pouvoir en 1979 et 1981, partageaient une vision similaire du monde, fondée sur la nécessité de répondre avec force à la « menace soviétique », et les mêmes valeurs de libéralisme politique et économique. Ils étaient soucieux de ne pas laisser les pays occidentaux se faire distancer dans la course aux armements par l’URSS et ses alliés. Cette proximité idéologique, ajoutée à une bonne entente personnelle entre les deux leaders, aboutit donc à un renforcement de la relation spéciale, accentué par une certaine méfiance du Premier ministre britannique vis-à-vis des institutions de la Communauté européenne,.\nUne des nombreuses manifestations de l’intensité de la relation spéciale à cette époque fut le soutien qu’apporta Margaret Thatcher au projet américain d’Initiative de défense stratégique (IDS), surnommé « guerre des étoiles », qui fut annoncé en 1983 par le gouvernement américain. L’idée était d’établir un bouclier anti-missiles balistiques dans l’espace, et des recherches furent entamées dans cette direction. Les Européens continentaux de l'Ouest (France, Allemagne, Italie, Belgique, Espagne, Suisse…) avaient des doutes sur cette idée et sur la remise en cause de la dissuasion nucléaire classique qu’un tel projet, s’il voyait jamais le jour (ce qui ne fut pas le cas), pourrait entraîner.\nEn 1982, l'Angleterre reçut un soutien logistique et en matière de renseignement de la part de Washington lors de la guerre des Malouines, qui se déroulait à des milliers de kilomètres des Îles britanniques, près des côtes de l'Amérique du Sud. Ce soutien fut donc précieux, même s’il fut à l’époque discret. À la fin de la décennie, le gouvernement Thatcher (puis John Major) devait à son tour appuyer la grande opération militaire américaine contre l’Irak dans la guerre du Golfe, en envoyant 45 000 soldats combattre sous commandement opérationnel des États-Unis.\n\n\n=== Au Danemark ===\nEn ce qui concerne la politique étrangère du Royaume du Danemark, le pays possède une certaine prédisposition à l'américanisme, c'est-à-dire une propension à agir en harmonie avec la politique américaine. Après le 11 septembre 2001, la politique étrangère et de sécurité intérieure danoise serait même devenue « super atlantiste ». Cet atlantisme s'oppose au « continentalisme » européen, défini comme la tendance à suivre les politiques des grandes puissances continentales européennes : la France, l'Allemagne et la Russie.\n\nMilitairement, cet américanisme politique s'exprime par un soutien logistique et humain aux opérations militaires contre l'Irak durant le printemps 2003. En Europe, seuls la Grande-Bretagne, la Pologne et le Danemark firent ce choix immédiatement. Simultanément à la guerre en Irak, vint l'« Initiative pour un grand Moyen-Orient », destinée à combattre le terrorisme à sa racine en « propageant la démocratie » dans cette région. Ces actions ont été effectuées en coopération directe avec les États-Unis et ont été conçues en dehors du cadre de l'Union européenne ou de l'Organisation du traité de l'Atlantique nord (OTAN).\n\n\n=== Au Japon ===\n\nIl faut chercher les racines de l'américanisme contemporain au Japon dans la défaite d'août 1945, à la suite des bombardements atomiques américains de Hiroshima et Nagazaki. C'est ici, dès la défaite de l'axe, un américanisme forcé et pensé par l'occupant américain, de 1945 à 1952 (date du retrait officiel des troupes américaines du pays). Ces années de « reconstruction » du pays se déroulent sous la tutelle du général américain Douglas MacArthur. Ce chef de guerre expérimenté de 65 ans en 1945 va jouer le rôle de chef d'état au Japon. Côté japonais, le bushido fut un des éléments culturels et politiques majeurs qui permit le maintien d'un lien étroit entre envahisseur américain et empire japonais vaincu.\nAinsi cette condition de peuple colonisé et administré par des étrangers, condition d'abord subie, fut par la suite de plus en plus intégrée à la vie quotidienne des japonais. À tel point que le député français Paul Bastid, en visite au Japon en 1949, s’étonne de cette attitude : « Les Japonais sont des occupés modèles, c’est-à-dire qu’ils ne protestent pas, ils font même l’éloge de leur vainqueur […]. Ils se répandent en panégyriques sur la générosité des Alliés. […] Même les éléments communistes, qui sont évidemment hostiles aux Américains, ne se manifestent pas au Japon. »\nPlusieurs faits peuvent expliquer cette relative volonté d'intégration de l'élément politique et culturel américain dans la plupart des villes japonaises:\n\nGéopolitiquement, MacArthur résistera à la volonté de Staline d'occuper l'Île japonaise d'Hokkaido, à la suite des accords de Yalta.\nSocialement, l'arrivée de garnisons entières de militaires américains n'allant pas sans criminalité durant les premières semaines d'occupation (viols, pillages) l'administration japonaise autorise rapidement l'installation de maisons closes pour endiguer les activités illicites en dehors des quartiers spécifiques servant aux distractions des militaires.\nÉconomiquement parlant, le japon est aux abois avec 80% de son industrie détruite, une agriculture réduite à néant faute de bras, 2,5 millions d’habitants perdus pendants la guerre et 6 millions d'hommes éparpillés parmi les ex-troupes impériales sur des milliers d'îles du Pacifique qui ne reviendront que petit à petit, parfois des années plus tard. MacArthur saisi l'opportunité de communiquer sa bonne volonté dans les médias et auprès de la population : il fait transporter et distribuer gratuitement des stocks américains de nourriture entreposés aux Philippines pour les GIs (hamburgers déshydratés, beurre de cacahuète, etc.) afin de nourrir quelques milliers de personnes pendant plusieurs semaines.\nAinsi, cet américanisme forcené semble participer du « miracle » économique japonais entre 1950 et 1980 : alors qu'en 1945 le revenu médian par habitant (par an) n'est que de 20 dollars, il est de 300 dollars en 1956 et de 1 000 dollars en 1967.\nOn estime malgré tout que plus de 1 million de japonais sont morts de sous-alimentation pendant la période d'occupation américaine (1945-1949).\nSelon le journaliste et homme politique Jean-Jacques Servan-Schreiber, le Japon aurait importé un nombre considérable de technologies et de méthodes entrepreneuriales des États-Unis entre les années 1960-1980, ce qui permit au pays de faire face – macro-économiquement parlant – aux crises boursières et contextuelles répétées de la seconde moitié du XXe siècle, tout en renforçant son tissu industriel, portuaire et boursier, la plupart du temps au prix de sacrifices faits par les employés du secteur privé (réductions salariales, augmentations du temps de travail, licenciements...).\nÀ partir des années 1950, un « American way of life » (style de vie américain) fait son apparition dans le pays (Coca-Cola, MacDonalds, consommation de chewing gums, Walt Disney, industrie hollywoodienne, etc.) alliant image de modernité technique, d'abondance alimentaire et de renouveau économique.\nAinsi, l'américanisme japonais de l'après seconde guerre mondiale conduit-il aussi à des changements majeurs d'urbanisme et d'organisation de l'espace public : à partir du milieu des années 1960, les immeubles en ossature acier (d'inspiration américaine) remplacent et détrônent en nombre les immeubles en béton armé (d'inspiration allemande) de la génération précédente (1940-1960). Des quartiers d'affaires, nouveaux centres de la cité (ou « city »), apparaissent sur le modèle des métropoles américaines.\nDepuis la seconde guerre mondiale, et plus particulièrement depuis la seconde guerre sino-japonaise, les États-Unis ont trouvé au Japon un allié « naturel » contre la puissance militaire et économique chinoise (« communiste »).\nDepuis la fin du XXe siècle, le Japon a plutôt renforcé ses liens commerciaux et militaires avec les États-Unis, sans toutefois adopter un américanisme « inconscient » ou sous-jacent dans toute la société, comme on pourrait l'observer dans certains pays d'Europe ou anglo-saxons (Grande-Bretagne, Allemagne, France, Pays-Bas, Belgique, Suisse) à différents niveaux décisionnels.\nAujourd'hui encore (années 2020) l'Île d'Okinawa – au sud de l'archipel japonais – est criblée de bases militaires américaines, dû à la position stratégique de l'île : plusieurs accès portuaires et aéroportés sur le Pacifique, mais faisant également face à la mer de Chine orientale et située à environ 500km linéaires des côtes taïwanaises.\n\n\n=== En Corée du Sud ===\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nPlan Marshall\nOTAN\nNouvelle-France\nAntiaméricanisme\n\n\n== Pages externes ==\nPascal Boniface: les Etats-Unis, un danger stratégique pour la planète, entrevue de Pascal Boniface par le journal Médiapart (2019)\nDe Trump à Biden : quel leadership américain ? - Le Dessous des cartes, Arte (2021)\nWoke, Cancel Culture, Gender studies… Assiste-t-on à une américanisation des idées ?, France Culture (2021)\n Portail de la politique   Portail des États-Unis   Portail de l’Union européenne   Portail de l’Europe   Portail de l’économie   Portail des relations internationales"
        },
        {
            "pageid": 1426388,
            "ns": 0,
            "title": "Amnistie",
            "content": "L'amnistie, dont l'origine grecque ἀμνηστία / amnēstía signifie « oubli », est une notion de droit pénal, qu'on peut définir comme « l'acte qui dispose que des fautes passées devront être oubliées, et qui interdit à quiconque de les rechercher ou de les évoquer sous peine de sanctions ».\n\n\n== Histoire ==\nDes clauses d'amnistie se trouvent depuis l'Antiquité dans tous les traités de paix qui concluent une guerre étrangère (La première loi d'amnistie, celle de Thrasybule, remonte à l'an 405 avant Jésus-Christ lors de la guerre du Péloponnèse) et depuis le Moyen Âge dans tous les édits de pacification qui terminent une guerre civile. Elles ont pour objet, une fois le règlement du conflit terminé, d'empêcher que la recherche de nouveaux griefs ne rallument les hostilités entre les belligérants. L'amnistie n'englobe jamais la sanction des troupes régulières par les autorités militaires dont elles dépendent. C'est une mesure d'apaisement à la fin d'un conflit.\nLa première loi d'amnistie française qui s'oppose ainsi au droit de grâce royale, abolie le 5 juin 1791, est le décret du 14 septembre 1791 qui instaure une amnistie générale pour les révolutionnaires, les contre-révolutionnaires, ainsi que le roi pour sa fuite des 20-21 juin. Elle vise à préserver la Constitution de 1791 : le 13 septembre, Louis XVI écrit à l'Assemblée qu'il est prêt à accepter la Constitution mais qu'il souhaite une réconciliation générale ; le marquis de La Fayette élabore un décret en trois articles pour répondre à cette demande. Au XIXe siècle, le droit de grâce est le privilège du président de la République française alors que l'amnistie est celle de l'Assemblée Nationale. Selon Léon Gambetta, l'amnistie des Communards conduit à fonder la République sur l'apaisement des conflits en mettant un terme au conflit, la démocratie politique, le suffrage universel (masculin) et l'instruction (lois Jules Ferry) rendant, selon lui, la violence illégitime.\nPlus récemment afin de briser l'omertà ou de lever les craintes de poursuites judiciaires, des garanties d'amnistie ont été offertes à des repentis (statut de « collaborateur de justice » dans la loi italienne) dans le cadre de la lutte contre le trafic de drogue, contre les activités mafieuses (Cosa Nostra, 'Ndrangheta, Camorra, etc.) et d'autres formes de criminalité organisée ou dans le cadre d'enquêtes internes faites par des industriels par certains groupes confrontés à un scandale financier ou industriel, par exemple chez Siemens en 2007-2008 quand le groupe était confronté à une affaire de corruption (l'enquête interne avait été perçue comme signe de volonté de régler le problème ce qui a conduit les autorités américaines a fortement réduire le montant de l’amende prévue). La promesse d'amnistie en échange d'informations et aveux de salariés ou pour protéger des lanceurs d'alerte a aussi été efficacement utilisée en 2012 par Thyssenkrupp alors presque en faillite à cause de pannes en séries (une dizaine de salariés avaient alors saisi l'occasion de parler). Dernièrement (2015) la procédure a été reprise lors du scandale Volkswagen par l'entreprise elle-même durant quelques semaines.\nDans un autre cadre, des gouvernements ont édicté des lois d'amnistie fiscale pour tenter de faire revenir dans le pays des « capitaux plus au moins sales ».\n\n\n== En France ==\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nAmnistie internationale (pour les canadiens francophones)\nCommission de la vérité et de la réconciliation\nCrimes contre l'humanité\nJuridiction universelle\nClémence\n\n\n=== Bibliographie ===\nB. Cassin, « Amnistie et pardon : pour une ligne de partage entre éthique et politique », dans Cassin B, Cayla O, Salazar Ph.-J. (dir.) Vérité, réconciliation, réparation, Paris, Seuil, 2004, 43, 37-57.\nBourguet, C. (1998). Entre amnistie et imprescriptible. Autrement. Série mutations, (177), 93-109 (résumé).\nP. Brown, « Vers la naissance du purgatoire : Amnistie et pénitence dans le christianisme occidental de l'Antiquité tardive au Haut Moyen Âge », Annales, 1997, p. 1247-1261, lire en ligne.\nMark Greengrass, « Amnistie et oubliance : un discours politique autour des édits de pacification pendant les guerres de Religion », dans Paul Mironneau et Isabelle Pébay-Clottes (dir.), Paix des armes, paix des âmes : actes du colloque tenu au Musée national du château de Pau et à l'Université de Pau et des Pays de l'Adour les 8, 9, 10 et 11 octobre 1998, Paris, Imprimerie nationale, 2000, 502 p. (ISBN 2-7433-0377-8), p. 113-123.\nGrunvald, S., Herzog-Evans, M., & Le Gall, Y. (2008). Prescription, amnistie et grâce en France. Dalloz.\nLefranc S (2002) Politiques du pardon. ISP - Institut des Sciences sociales du Politique ; Presses universitaires de France, pp.Inconnu, 2002\nTutu, D., Ferrarri, F., & Reffait, C. (1997). Pas d'amnistie sans vérité : Entretien avec l'archevêque Desmond Tutu. Esprit (1940-), 63-72 (résumé)\n Portail du droit   Portail des droits de l’homme   Portail de la politique"
        },
        {
            "pageid": 6377418,
            "ns": 0,
            "title": "Antiétatisme",
            "content": "L'antiétatisme est l'opposition à l'étatisme, c'est-à-dire à l'intervention de l'État dans les affaires personnelles, sociales et économiques. L'antiétatisme peut rejeter l'État complètement ainsi que le pouvoir et la domination en général, comme dans l'anarchisme ; il peut éventuellement réduire la taille et la portée de l'État à un minimum, par exemple dans le minarchisme ; ou il peut plaider en faveur d'une société sans État comme un objectif lointain, par exemple dans le marxisme. Il peut également, comme dans le cas du libéralisme, rejeter non pas l'État mais l'idéologie étatiste en tant que système de croyances plaçant l'État au-dessus de la société civile, percevant l'État comme la solution systématique aux problèmes économiques et sociaux, refusant de l'envisager comme une cause possible de ces problèmes.\n\n\n== Catégories générales ==\nLes antiétatistes diffèrent grandement en fonction des opinions qu'ils ont « en plus » de l'antiétatisme. Ainsi, les catégories de pensée antiétatistes sont parfois classés comme collectivistes ou individualistes.\nLa difficulté importante à déterminer si un penseur ou de la philosophie est antiétatiste est le problème de la définition de l'État lui-même. La terminologie a changé au fil du temps, et les écrivains du passé ont souvent utilisé le mot « État » dans un sens différent de celui que nous l'utilisons aujourd'hui. Ainsi, l'anarchiste Mikhail Bakounine utilise le terme pour signifier tout simplement un organisme gouvernant. D'autres auteurs utilisent le terme « État » pour désigner toute organisation ayant pour but de légiférer ou d'appliquer la loi. Karl Marx définit l'État comme l'institution utilisée par la classe dirigeante d'un pays pour maintenir les conditions de sa domination. Selon Max Weber, l'État est une organisation avec un monopole de l'utilisation de la force légal dans une zone géographique particulière.\nThoreau exprime cette évolution de la perception de l'antiétatisme :\n\n« J'accepte de tout cœur la devise « Que le meilleur gouvernement est celui qui gouverne le moins » et j'aimerais bien qu'il en soit ainsi le plus rapidement et le plus systématiquement possible. Poussé à fond, cela se ramène à ceci, auquel je crois également, « Que le meilleur gouvernement est celui qui ne gouverne pas du tout » et quand les hommes et les femmes y seront préparés, ce sera le genre de gouvernement qu'ils auront. »\n\n— Henry David Thoreau, La Désobéissance civile.\n\n\n== Philosophies antiétatistes ==\n\n\n=== Complètement antiétatistes ===\nAvec l'antiétatisme comme but immédiat.\n\nAnarchisme\nNihilisme\nLibertarianisme\nAgorisme\n\n\n=== Partiellement antiétatiste ===\nAvec l'antiétatisme comme idéal ou but dans le futur.\n\nLibéralisme classique\nPaléo-conservatisme\nPaléo-libertarianisme\nPhilosophies politiques liées au libéralisme classique et au minarchisme.\nPhilosophies politiques liées au marxisme et au communisme.\n\n\n== Écrits antiétatistes ==\n1574 : Étienne de La Boétie, Discours de la servitude volontaire ou le Contr’un. [lire en ligne]\n1793 : William Godwin, Enquiry concerning Political Justice, and its Influence on General Virtue and Happiness. [(en) lire en ligne]\n1825 : Thomas Hodgskin, Labour Defended against the Claims of Capital. [(en) lire en ligne]\n1840 : Pierre-Joseph Proudhon, Qu'est-ce que la propriété ? ou Recherche sur le principe du Droit et du Gouvernement. [lire en ligne]\n1844 : Max Stirner, Der Einzige und sein Eigentum (L'Unique et sa propriété). [lire en ligne]\n1849 : Henry David Thoreau, Civil Disobedience. [(en) lire en ligne]\n1849 : Frédéric Bastiat, La Loi. [lire en ligne]\n1849 : Gustave de Molinari, Les Soirées de la rue Saint-Lazare. Entretiens sur les lois économiques et défense de la propriété. [lire en ligne]\n1851 : Herbert Spencer, The Right to Ignore the State. [(en) lire en ligne]\n1866 : Mikhaïl Bakounine, Катехизис революционера. [(ru) lire en ligne]\n1867-1870 : Lysander Spooner, No Treason. [(en) lire en ligne]\n1886 : Benjamin Tucker, State Socialism and Anarchism: How far they agree, and wherein they differ. [(en) lire en ligne]\n1902 : Pierre Kropotkine, Mutual Aid : A Factor of Evolution. [(en) lire en ligne]\n1922 : Sante Ferrini, Il Governo, Paris, s. é.\n1935 : Albert Jay Nock, Our Enemy, the State. [(en) lire en ligne]\n1962 : Murray Rothbard, Man, Economy, and State. [(en) lire en ligne]\n1982 : Murray Rothbard, L'Éthique de la Liberté. [(en) lire en ligne]\n1983 : Samuel Edward Konkin III, New Libertarian Manifesto (en). [(en) lire en ligne]\n1985 : Anthony de Jasay, The State. [(en) The State lire en ligne]\n2001 : Kevin Carson, The Iron Fist Behind the Invisible Hand. [(en) lire en ligne]\n\n\n== Notes et références ==\n\n\n=== Notes ===\n\n\n=== Références ===\n(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Anti-statism » (voir la liste des auteurs).\n\n(en) Carolyn Gallaher, « Anti-statism », dans Peter Shirlow (dir.) et al., Key Concepts in Political Geography, SAGE, 2009, 392 p. (ISBN 1-41294672-7 et 978-1412946728, présentation en ligne, lire en ligne)\n(en) Henry David Thoreau, Civil Disobedience, Forgotten Books, 1964 (1re éd. 1849), 56 p. (ISBN 1-60620-025-9 et 978-1606200254, présentation en ligne, lire en ligne)\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nAntinationalisme\nÉtatisme\n\n Portail de la politique   Portail de l’anarchisme   Portail du libéralisme   Portail de l’économie"
        },
        {
            "pageid": 5123594,
            "ns": 0,
            "title": "Antiparlementarisme",
            "content": "L'antiparlementarisme désigne l'opposition ou l'hostilité envers le parlementarisme, qu'il s'incarne sous forme républicaine ou monarchique, critiquant le plus souvent le coût de fonctionnement du Parlement, la corruption des parlementaires, leur absentéisme et le vote godillot.\nL'antiparlementarisme se retrouve dans de nombreux courants politiques. L'extrême droite est traditionnellement antilibérale et antiparlementaire.\nL'un des fondements du fascisme est ainsi le rejet du libéralisme, dont le parlementarisme.\nEn France, l'antiparlementarisme structurel (déniant le parlementarisme, tel le boulangisme ou le nationalisme intégral) de la Troisième République se distingue de l'antiparlementarisme politique de la Quatrième République qui critique les excès du parlementarisme (notamment de ses élites qui ont confisqué le pouvoir ou du gerrymandering) sans le refuser, tel le poujadisme.\n\n\n== Bibliographie ==\n\nCollectif, Siècles. Revue du Centre d'histoire « Espaces et cultures », n° 32, « L'antiparlementarisme entre continuité et mutations (XVIIIe-XXIe siècles) », 2013, lire en ligne.\nJulien Bouchet, « La République du conciliabule ou la souveraineté confisquée », Siècles. Revue du Centre d'histoire « Espaces et cultures », no 32 « L'antiparlementarisme entre continuité et mutations (XVIIIe-XXIe siècles) »,‎ 2013 (lire en ligne).\nJean-Claude Caron, « L'antiparlementarisme, une culture politique partagée ? », Siècles. Revue du Centre d'histoire « Espaces et cultures », no 32 « L'antiparlementarisme entre continuité et mutations (XVIIIe-XXIe siècles) »,‎ 2013 (lire en ligne).\nJean-Étienne Dubois, « De l'anticartellisme à l’antiparlementarisme dans la France des années vingt », Siècles. Revue du Centre d'histoire « Espaces et cultures », no 32 « L'antiparlementarisme entre continuité et mutations (XVIIIe-XXIe siècles) »,‎ 2013 (lire en ligne).\nJean-François « Maxou » Heintzen, « « Regardez-les donc sauter, c’est nos députés ! » : L'antiparlementarisme en chansons, 1880-1934 », Siècles. Revue du Centre d'histoire « Espaces et cultures », no 32 « L'antiparlementarisme entre continuité et mutations (XVIIIe-XXIe siècles) »,‎ 2013 (lire en ligne).\nÉdouard Lynch, « Mobilisations paysannes et antiparlementarisme dans la première moitié du XVe siècle », Siècles. Revue du Centre d'histoire « Espaces et cultures », no 32 « L'antiparlementarisme entre continuité et mutations (XVIIIe-XXIe siècles) »,‎ 2013 (lire en ligne).\nJohann Chapoutot, « Les nazis et la République Allemande », Parlement[S], Revue D'histoire Politique, 1, p. 45.\nJean-Claude Caron, « Un mal français ? Quelques remarques sur la généalogie de l'antiparlementarisme », Parlement[S], Revue d'histoire politique, n° 3, 2013, 23.\nPierre Triomphe, « L'antiparlementarisme sous la Restauration », Parlement[S], Revue d'histoire politique, n° 3, p. 35.\nJean El Gammal, « L'antiparlementarisme et les événements de mai 1968 : essai de mise en perspective », Parlement[S], Revue d'histoire politique, n° 3, 2013, 127.\nDavid Bellamy, « Le gaullisme fut-il une critique du régime d'Assemblée ? », Parlement[S], Revue D'histoire Politique, n° 3, 2013, 113.\nJean Garrigues, « Le boulangisme est-il antiparlementaire ? », Parlement[S], Revue D'histoire Politique, n° 3, 2013, 49.\nCarl Bouchard et Kevin Audet-Vallée, « « Faites un roi, sinon faites la guerre » : l’Action française durant la Grande Guerre (1914-1918) » (2012): 2012-09-24T19:02:38Z.\nJean Defrasne, L'antiparlementarisme en France, Paris, Presses universitaires de France, coll. « Que sais-je ? » (no 2536), 1990, 127 p. (ISBN 978-2-13-043188-6, BNF 37662626).\nGiulia Guazzaloca, Fine secolo : gli intellettuali italiani e inglesi e la crisi fra Otto e Novecento, n.p.: Il Mulino, 2004.\nArmand Patrucco, \"The critics of the Italian parliamentary system : 1860 - 1915.\" (1992): New York u.a. : Garland, 1992.\nBenoît Jeanneau, « L'antiparlementarisme d’hier à aujourd’hui », Pouvoirs, Paris, Seuil, no 64 « Le Parlement »,‎ février 1993, p. 23-34 (ISSN 0152-0768, lire en ligne).\nJean-Yves Mollier, « Les racines de l'antiparlementarisme au début de la troisième République (1870-1900) », dans Michel Vovelle (dir.), Révolution et République : l'exception française, Paris, Kimé, 1994, 699 p. (ISBN 2-908212-70-6), p. 546-557.\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nCésarisme\nPopulisme\n\n\n=== Liens externes ===\n\n Portail de la politique"
        },
        {
            "pageid": 1094913,
            "ns": 0,
            "title": "Arbitraire",
            "content": "L’arbitraire (adjectif ou nom) est ce qui n'est pas motivé par une (bonne) raison : au sens moral ce qui n'est pas juste ou bon, social ou bon pour le monde (l'arbitraire du pouvoir). Au sens factuel ou logique ce qui n'est pas rationnel, justifié par l'empirisme ou la raison.\n\n\n== Étymologie ==\nNom formé à partir du verbe latin arbitror, arbitrari qui signifie « juger, penser, croire ».\n\n\n== Arbitraire en politique et en droit ==\nL'arbitraire est l'application de la subjectivité d'une personne détenant du pouvoir aux dépens d'une autre qui en est démunie. C'est le pouvoir autoritaire dans son application. \nL'arbitraire est la mise en pratique de l'absolutisme, de l'injustice ou de la tyrannie.\nUne décision de justice pénale prononcée « pour l'exemple » relève de l'arbitraire car son auteur ne s'en tient pas strictement aux règles du droit, se laissant influencer par une circonstance passagère ou extérieure aux faits.\n\n\n=== Suisse ===\nEn droit suisse, l'arbitraire est interdit par la Constitution fédérale : « Toute personne a le droit d’être traitée par les organes de l’État sans arbitraire et conformément aux règles de la bonne foi ».\nL'arbitraire désigne notamment des normes ou des décisions qui ne sont pas fondées sur des motifs sérieux et objectifs, qui sont dépourvues de sens ou d'utilité, ou qui opèrent des distinctions injustifiables.\n\n\n== Arbitraire du signe linguistique ==\nSelon Ferdinand de Saussure, le signe linguistique est arbitraire, c'est-à-dire qu'il n'existe aucun rapport naturel entre le signifié (le concept) et le signifiant (l'image acoustique), en d'autres termes entre le sens et sa réalisation visuelle et acoustique (le mot). Cet arbitraire du signe explique que, pour désigner un même concept (par exemple \"chat\"), il soit possible d'utiliser différentes réalisations graphiques et phoniques, telles que chat [ʃa] en français, 猫 [neko] en japonais, kucing [kut͡ʃɪŋ] en malais, et cætera.\nLa question de l'arbitraire des signes est un thème qui fait l'objet de discussions importantes en sémiotique et spécialement en sémiotique visuelle. Des théoriciens comme Umberto Eco et le Groupe µ ont largement contribué à ces discussions.\nBien que l'hypothèse de l'arbitraire du signe soit généralement acceptée parmi les linguistes, il existe aussi des contre-exemples. Certains contre-exemples sont cités dès l'Antiquité. Dans le dialogue le Cratyle de Platon, le philosophe Cratyle cite le cas des lettres de l'alphabet grec. Les noms de ces lettres (c'est-à-dire leur signifiant) contiennent généralement le son qu'elles désignent (c'est-à-dire leur signifié) : par exemple, le signifiant phonétique de la lettre β, [bɛta], contient le signifié de cette lettre, à savoir le son [b].\nAu-delà de ces contre-exemples anecdotiques, des recherches plus récentes ont montré qu'il existait des violations plus systématiques du principe de l'arbitraire du signe. Par exemple, une étude statistique de mots provenant d'environ deux tiers des langues parlées dans le monde a révélé que des langues sans lien génétique ont tendance à utiliser (ou à éviter d'utiliser) les mêmes sons pour exprimer des concepts spécifiques. Par exemple, à travers les langues, les mots pour désigner l'organe de la langue ont tendance à contenir le son [l] et à éviter le son [k]. Les mots désignant la petitesse contiennent souvent le son [i]. Une autre étude a trouvé qu'il existait une corrélation faible mais néanmoins significativement positive entre distance orthographique et distance sémantique pour les lexiques d'un ensemble de 100 langues : dans une langue donnée, plus les mots ont tendance à apparaître dans les mêmes contextes, plus ils ont tendance à être similaires orthographiquement. Ces résultats remettent en cause l'hypothèse qu'il n'existe aucun lien entre signifiant et signifié.\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nBonne foi\nDéni de justice\nDétention arbitraire\nSigne linguistique\n Portail du droit   Portail de la philosophie"
        },
        {
            "pageid": 1580370,
            "ns": 0,
            "title": "Assimilationnisme",
            "content": "L'assimilationnisme est un mouvement d'idées ayant pour objectif de faire disparaître tout particularisme culturel et d'imposer l'assimilation culturelle aux minorités d'un pays.\nElle a constitué le centre de la politique française d'intégration de la IIIe République jusqu'aux années 1970-1980. D'après Gilles Ferréol, en France : « Depuis les années 1980, la philosophie politique sous-jacente est celle de la citoyenneté (désormais élargie, plurielle et davantage active), le vieux modèle assimilationniste français étant de plus en plus questionné ».\n\n\n== Histoire ==\n\n\n=== Apparition du concept ===\nLe concept d’assimilation apparaît dans le débat public au XIXe siècle sur la question de la place des juifs dans la société française, non des étrangers, et « l’assimilation est présentée comme la réponse apportée à une « guerre des races » annoncée par les intellectuels antisémites ».\nAlexis de Tocqueville lors de sa mission en Algérie, en 1840-1842, a eu pour objectif d’étudier les structures tribales algériennes afin de mesurer le degré d’assimilabilité de ces populations. Selon Thomas Lacroix, « les conclusions de la mission hantent encore aujourd’hui les débats sur l’intégration des étrangers : l’assimilabilité est d’abord en fonction du degré d’islamisation. Les tribus arabes, dépeintes comme profondément religieuses, mais aussi polygames, nomades et peu enclines au travail, sont considérées comme les plus éloignées du standard national ».\nSelon Danièle Lochak, l'assimilation « suppose l’abandon de tout élément de la culture originelle de l’étranger qui doit se fondre dans la communauté d’adoption. Ce que ne sous-entend pas le terme d’intégration. La tendance assimilationniste est une constante de l’histoire de France, qu’il s’agisse du jacobinisme éradicateur des différences culturelles ou de la politique menée dans les colonies ».\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nAssimilation culturelle\nIntégration culturelle\nIntégration sociale\nAcculturation\nCommunautarisme\nEthnocide\n\n\n=== Liens externes ===\n\n Portail de la politique   Portail des minorités"
        },
        {
            "pageid": 13032061,
            "ns": 0,
            "title": "Bloc élitaire",
            "content": "Le terme de bloc élitaire est forgé durant la campagne présidentielle de 2017 par le sondeur et analyste politique Jérôme Sainte-Marie, qui en fait, avec le bloc populaire, l’un des deux pôles actifs de la recomposition politique française.\n\n\n== Concept ==\nSelon cet auteur, à la place du clivage gauche-droite, en crise ouverte depuis le milieu du quinquennat de François Hollande, la vie politique française serait structurée par une opposition bloc contre bloc, directement articulée sur un conflit entre classes sociales.\nSelon cette analyse, le bloc élitaire est dirigé par l’élite réelle, c’est-à-dire les catégories effectivement dirigeantes, à la fois de l’État et du capitalisme français. Sa deuxième composante est l’élite aspirationnelle, c’est-à-dire les cadres, qui s’identifie aux valeurs et aux objectifs de l’élite réelle. Ils se distinguent par un vote plus important pour Emmanuel Macron au premier tour de l’élection présidentielle de 2017 et lui manifestent un soutien notable durant les premières années de son quinquennat. C’est également le cas des retraités, troisième élément du bloc élitaire, qui s’apparentent à une élite par procuration, déléguant aux représentants de l’élite réelle la défense de leurs intérêts, c’est-à-dire essentiellement la garantie du versement de leurs pensions de retraite.\nDans Bloc contre bloc. La dynamique du macronisme, Jérôme Sainte-Marie expose l'idée selon laquelle l'idéologie du bloc élitaire serait constituée par la réunion du libéralisme économique et du libéralisme culturel, reprenant ainsi l'analyse déjà formulée par le philosophe Jean-Claude Michéa. Son expression politique est le vote en faveur de La République en Marche.\nLa notion de bloc élitaire est inspirée du concept de bloc historique forgé par Antonio Gramsci. Elle se fonde sur la représentation de la société et de la politique développée par Karl Marx, illustrée notamment dans Le 18 Brumaire de Louis Bonaparte. \nDans leur ouvrage, L’Illusion du bloc bourgeois, Bruno Amable et Stefano Palombarini développent une notion proche de celle du bloc élitaire, mais en insistant davantage sur la dimension économique de sa formation, tout en valorisant la notion de gauche et en postulant l’échec, ou l’illusion, de ce bloc. Dans son livre L’Archipel français, paru en mars 2019, Jérôme Fourquet évoque également la constitution d’un « bloc libéral-élitaire » autour d’Emmanuel Macron, sans toutefois envisager la constitution d’un pôle antagoniste, qui serait, selon Jérôme Sainte-Marie, le bloc populaire.\n\n\n== Notes et références ==\n\n Portail de la politique française"
        },
        {
            "pageid": 5967566,
            "ns": 0,
            "title": "Boussole électorale",
            "content": "La boussole électorale est un outil d'éducation politique interactif qui permet aux utilisateurs d'explorer leurs positions dans le paysage politique lors d'une élection donnée. La Boussole électorale est conçue afin de fournir aux utilisateurs une évaluation personnalisée, facile à comprendre et à comparer. Les opinions d'un individu sur un ensemble d'enjeux politiques sont agrégés et illustrés dans un espace idéologique bidimensionnel qui comprend aussi la position des différents partis politiques.\nLa Boussole électorale est également le nom légal d'un organisme canadien sans but lucratif qui est responsable de développer et gérer l'outil du même nom.\nL'outil de la Boussole électorale a été pour la première fois lancée lors des élections fédérales canadiennes de 2011. La Société Radio-Canada a parrainé et fait la promotion du projet lors de la campagne électorale. Durant la première semaine en fonction, l'outil de la Boussole électorale a attiré plus d'un million de répondants. À la fin des élections, près de deux millions de répondants l'avaient utilisée. Depuis, différentes versions de la Boussole électorale, toutes commandées la Société Radio-Canada, ont également été lancées lors de l'élection générale ontarienne de 2011, de l'élection générale albertaine de 2012, de l'élection générale québécoise de 2012 et de l'élection générale britanno-colombienne de 2013. En 2012, une version de la Boussole électorale en vue des l'élection présidentielle américaine a été lancée en collaboration avec The Wall Street Journal. Une Boussole électorale a été utilisée en Australie, en collaboration avec le diffuseur public Australian Broadcasting Corporation (ABC), notamment pour les élections de 2013,. Depuis 2017, la Boussole est utilisée en France.\n\n\n== Historique ==\nLa Boussole électorale fut fondée par Clifton van der Linden, un doctorant de l'Université de Toronto en science politique qui en est aussi le directeur exécutif. La conception de l'outil et les opérations de l'organisme sont assurées par Yannick Dufresne, Gregory Eady, Jennifer Hove et Clifton van der Linden.\n\n\n== Notes et références ==\n\n\n== Liens externes ==\nBoussole électorale Canada\nBoussole électorale France\n\n\n== Sources existantes ==\nUniversité McGill,\nUniversité Laval\n[1],\n[2],\n[3]),\nConseil de Presse du Québec,\nHuffington Post,\nL'Actualité,\nSource - The canadian journalism project,\nLa Presse,\nEstrie Plus,\nLe Droit,\nMacleans,\nToronto Life,\nFonds de recherche, société et culture, Gouvernement du Québec,\nGuelph Mercury,\nUniversity of Waterloo,\nToronto Sun,\nNational Post.\nun exemple\n Portail de la politique   Portail de la politique canadienne"
        }
    ],
    "Société": [
        {
            "pageid": 603999,
            "ns": 0,
            "title": "Société",
            "content": "La société (du latin socius : compagnon, associé) est un groupe d'individus unifiés par un réseau de relations, de traditions et d'institutions.\n\n\n== Sens global de « collectivité d'individus » ==\nen sciences humaines et sociales, la société se rapporte à l'ensemble des individus qui partagent certaines mœurs et coutumes, ou parfois à ces coutumes elles-mêmes ;\nen zoologie, on parle de sociétés (de fourmis, d'abeilles, etc.) pour désigner des cas d'eusocialité, un mode d'organisation sociale chez certains insectes.\n\n\n== Formes ou aspects de sociétés ==\nSociété civile\nSociété de l'information\nSociété de la connaissance\nSociété de consommation\nSociété d'Ancien Régime\nSociété féodale\nSociété de masse\nSociété américaine\n\n\n== Sens d'« association » ==\nSociété savante\nSociété secrète\n\n\n== Sens d'« entreprise » ==\nEn économie, une société est la forme juridique la plus répandue des entreprises ; c'est un terme souvent utilisé pour désigner une entreprise. Voir aussi :\n\nDroit des sociétés\nDroit des sociétés en Belgique\nDroit des sociétés en France\nSociété en droit français\nSociété extraterritoriale\n\n\n== Voir aussi ==\n\nÎles de la Société : archipel situé dans l'océan Pacifique au sud de l'équateur.\nSociété des Nations\nSociété de Jésus\nSociété, une marque commerciale appartenant à l'entreprise agro-alimentaire française Lactalis.\nSocial : adjectif correspondant (relatif à la société).\nJeu de société : jeu qui se joue à plusieurs personnes.\nSociété écran\nSociétés, revue de sciences humaines et sociales\n Portail de la société   Portail des entreprises"
        },
        {
            "pageid": 2484938,
            "ns": 100,
            "title": "Portail:Société",
            "content": ""
        },
        {
            "pageid": 466731,
            "ns": 14,
            "title": "Catégorie:Wikipédia:ébauche société",
            "content": "Cette catégorie est une catégorie d'ébauche.  Elle est remplie par le modèle « {{Ébauche}} » avec le paramètre « société » :"
        },
        {
            "pageid": 4899337,
            "ns": 14,
            "title": "Catégorie:Liste en rapport avec la société",
            "content": ""
        },
        {
            "pageid": 1659601,
            "ns": 14,
            "title": "Catégorie:Société par pays",
            "content": ""
        },
        {
            "pageid": 221421,
            "ns": 14,
            "title": "Catégorie:Académie",
            "content": "Une académie est une assemblée de gens de lettres, de savants et/ou d'artistes reconnus par leurs pairs, qui a pour mission de veiller aux usages dans leurs disciplines respectives et de publier des ouvrages tels que des dictionnaires, des grammaires, etc."
        },
        {
            "pageid": 10691415,
            "ns": 14,
            "title": "Catégorie:Affaires sociales",
            "content": ""
        },
        {
            "pageid": 107777,
            "ns": 14,
            "title": "Catégorie:Aide humanitaire",
            "content": ""
        },
        {
            "pageid": 87520,
            "ns": 14,
            "title": "Catégorie:Alimentation",
            "content": "Voir aussi les catégories Gastronomie et Cuisine.\nCette catégorie aborde la nourriture d'un point culturel et de ses produits. Pour les aspects biologiques et médicaux, consulter la catégorie nutrition.\nLa nourriture est normalement mangée ou bue par les organismes vivants. Les termes nourriture ou alimentation incluent aussi les aliments liquides et boissons. Consultez la sous-catégorie aliment pour les différents types de produits alimentaires, ingrédients, arômes et épices, ou la conservation des produits alimentaires."
        },
        {
            "pageid": 6322817,
            "ns": 14,
            "title": "Catégorie:Armée et société",
            "content": ""
        }
    ],
    "Économie": [
        {
            "pageid": 984989,
            "ns": 0,
            "title": "Économie (discipline)",
            "content": "L'économie, ou science économique, provenant du grec ancien οἰκονομία / oikonomía, contraction de οἶκος / oikos (« maison ») et de νόμος / nomós (« loi »), ce qui signifie « administration d'un foyer », est une discipline des sciences sociales qui étudie l'économie en tant qu'activité humaine, qui consiste en la production, la distribution, l'échange et la consommation de biens et de services.\nSi dans l'Antiquité Xénophon et Aristote ont chacun écrit un traité sur l'économie, c'est à partir du XVIe siècle que se développe la pensée économique moderne, avec le mercantilisme, puis au XVIIIe siècle avec les physiocrates. L'économie politique débute à la fin du XVIIIe siècle avec Adam Smith, puis David Ricardo ou encore Jean-Baptiste Say (les classiques) au début XIXe siècle. C'est avec la révolution marginaliste à la fin du XIXe siècle que l'économie se constitue comme une discipline scientifique et s'institutionnalise.\nAu sein de la discipline, on distingue deux grandes approches : la macroéconomie, qui étudie les grands agrégats économiques (épargne, investissement, consommation, croissance économique), et la microéconomie, qui étudie le comportement des agents économiques (individus, ménages, entreprises) et leurs interactions, notamment sur les marchés.\nComme dans d'autres disciplines, l'économie se décline selon un spectre depuis la théorie économique, qui vise à construire un corpus de résultats fondamentaux et abstraits sur le fonctionnement de l'économie, jusqu'à l'économie appliquée, qui utilise les outils de la théorie économique et des disciplines connexes pour étudier des domaines importants comme l'environnement, le travail, la santé, l'immobilier, l'organisation industrielle ou encore l'éducation.\n\n\n== Définition ==\nL'économie en tant que discipline a reçu des définitions différentes au cours du temps. En 1776, Adam Smith conçoit, dans ses Recherches sur la nature et les causes de la richesse des nations, l'économie comme d'emblée politique, en ce qu'elle serait une branche de la science du législateur. En 1815, Jean-Baptiste Say écrit que l'économie « nous enseigne comment les richesses sont produites, distribuées, consommées dans une société »,.\n\nCertaines définitions de l'économie insistent sur la relation entre l'économie et la rareté, et fait de la discipline celle de l'étude de la gestion de la rareté. Lionel Robbins, dans son Essai sur la nature et la signification de la science économique (1932), écrit que « l'économie est la science qui étudie le comportement humain comme une relation entre des fins et des moyens rares à usages alternatifs ». Raymond Barre, dans son Économie politique (1976), écrit que « l'économie politique est la science de l'administration des ressources rares dans une société humaine. Elle étudie les formes que prend le comportement humain dans l'aménagement onéreux du monde extérieur, en raison de la tension qui existe entre des désirs illimités et les moyens limités des agents économiques ».\n\n\n== Histoire ==\n\n\n=== Origines de la pensée économique ===\nLes origines de la pensée économique remontent aux civilisations mésopotamienne, grecque, indienne, chinoise, perse et arabe.\n\n\n==== Civilisations mésopotamiennes ====\nÀ partir de la fin du VIe millénaire av. J.-C. les cités-États de Sumer ont développé leurs commerces et leurs économies à partir des marchés de matières premières.\nLes premiers codes de loi de Sumer pourraient être considérés comme les premiers écrits économiques, dont de nombreux attributs sont encore en usage dans la valorisation des prix d'aujourd'hui tels les montants codifiés d'échange d'argent lors des échanges commerciaux (taux d'intérêt), amendes, règles d'héritage, lois concernant la façon dont la propriété privée doit être imposée ou divisée, etc..\n\n\n==== Monde grec ====\nDans la Grèce antique, le terme économique apparaît comme le titre d'un traité de Xénophon (Économique) et d'un ensemble de traités attribués à Aristote (Économiques), dont l'objet est la connaissance et la formulation des lois (« nomos ») permettant d'optimiser l'utilisation des biens d'une maison (« oikos »), considérée comme unité collective de production d'une famille élargie ou d'un clan.\nChez Aristote, la richesse est considérée du point de vue de l'abondance des biens produits et de leur utilité, non de l'accumulation de monnaie par l'usure ou le négoce dont les procédés font l'objet d'une autre discipline qu'Aristote appelle chrématistique (de khréma (la richesse) et -atos (degré superlatif)) et qu'il considère comme des activités stériles, voire déshonorantes dans l'Éthique à Nicomaque).\nL'Économique est explicitement distingué du terme Politique, laquelle fait l'objet d'un autre traité d'Aristote et vise à établir l'harmonie et la justice entre les différentes classes de personnes et de familles qui constituent la cité.\n\n\n==== Inde ====\nLe philosophe indien Vishnugupta Chânakya (340-293 av. J.-C.), conseiller auprès du trône de l'Empire maurya de l'ancienne Inde, développe de nombreux concepts économiques, notamment dans son œuvre principale Arthashastra (La Science des richesses et du bien-être),,.\n\n\n==== Moyen Âge ====\n\nAu Moyen Âge les penseurs économiques sont avant tout des théologiens comme Thomas d'Aquin ou Ibn Khaldoun.\nDans sa Somme théologique, Thomas d'Aquin examine de nombreuses questions de nature économique, dont la justification de la propriété privée, du commerce et du profit.\nRaisonnant dans le cadre du droit naturel, les penseurs scolastiques, ils préfigurent l'économie moderne dans le domaine de la politique monétaire, de l'intérêt, et la théorie de la valeur dans le cadre du droit naturel.\nLeur apport majeur est la critique de l'usure, critique largement contestée par l'économie moderne, particulièrement par Eugen von Böhm-Bawerk.\n\n\n=== Les débuts de l'économie comme discipline à part ===\n\n\n==== XVe siècle - XVIIe siècle ====\nÀ partir de la seconde moitié du XVe siècle et jusqu'au milieu du XVIIIe siècle, la pensée économique se structure autour de la doctrine du mercantilisme. Ce courant correspond à l'émergence de la notion d'État face au pouvoir papal et au système féodal. Le rôle de l'économiste est alors de permettre l'enrichissement de son souverain.\nLes penseurs mercantilistes prônent le développement économique par l'enrichissement des nations au moyen du commerce extérieur qui permet de dégager un excédent de la balance commerciale grâce à l'investissement dans des activités économiques à rendement croissant. Ils accordent un rôle primordial à l'État et prône des politiques protectionnistes établissant des barrières tarifaires et encourageant les exportations tout en visant à l'unification du marché national. Cette doctrine économique connaît son apogée du XVIe  au  XVIIIe siècle. Elle estime que la richesse d'une nation dépend de l'importance de sa population et de l'accumulation d'or et d'argent. Les nations qui n'ont pas accès aux mines peuvent obtenir l'or et l'argent en favorisant leur outil productif et en stimulant leurs exportations,.\nC'est au XVIIe siècle qu'apparaît la notion d'économie politique avec la publication d'Antoine de Montchrestien Traité d'économie politique (1615).\n\n\n==== XVIIIe siècle ====\n\nDans la seconde moitié du XVIIIe siècle, la doctrine mercantiliste est remise en cause par les physiocrates d'une part et par la naissance de l'économie classique avec Adam Smith d'autre part.\nInspirés en particulier par des ouvrages comme celui de Richard Cantillon, Essai sur la nature du commerce en général (1755) les physiocrates considèrent que la seule activité réellement productive est l'agriculture. La terre multiplie les biens : une graine semée produit plusieurs graines. Au bout du compte, la terre laisse un produit net ou surplus. L'industrie et le commerce sont considérés comme des activités stériles car elles se contentent de transformer les matières premières produites par l'agriculture.\nLes physiocrates s'attachent à la recherche des lois naturelles qui régissent les activités des hommes. Ils ont notamment schématisé l'économie comme un flux de revenus et de dépenses améliorant le modèle de Boisguilbert,.\n\nEn 1776, Adam Smith publie Recherches sur la nature et les causes de la richesse des nations, considéré comme l'ouvrage fondateur de l'économie classique. Cette publication propose une synthèse cohérente des connaissances économiques de cette époque. Si Adam Smith est aujourd’hui surtout connu en tant qu’économiste, il se considérait avant tout comme professeur de philosophie morale (qu’il avait enseignée à Glasgow). Ainsi, la Richesse des nations ne traite pas seulement d’économie (au sens moderne), mais aussi d’économie politique, de droit, de morale, de psychologie, de politique, d’histoire, ainsi que de l’interaction et de l’interdépendance entre toutes ces disciplines. L’ouvrage, centré sur la notion d'intérêt personnel, forme un ensemble avec la Théorie des sentiments moraux, où il avait exposé la sympathie inhérente à la nature humaine.\n\n\n=== XIXe siècle : l'affirmation de la discipline économique ===\n\n\n==== Économie politique classique ====\n\nPour Adam Smith, l'augmentation de la population est synonyme d'augmentation de la richesse. Thomas Malthus, pasteur chargé de l'aide aux pauvres dans sa commune, est frappé par la misère engendrée par les mauvaises récoltes de 1794 à 1800. Il s'intéresse alors aux problématiques du progrès, de la croissance de la population et de la richesse. Son ouvrage principal, Essai sur le principe de population (1798), connait une grande popularité et conduit à un des premiers recensements de la Grande-Bretagne.\n\nAvec la publication Des principes de l'économie politique et de l'impôt (1817), l'économiste et philosophe britannique David Ricardo développe et enrichit les thèses de la valeur, du libre-échange popularisées par Adam Smith.\nPour Daniel Villey, « les bases essentielles du système ricardien — la loi de la population, la loi des rendements décroissants, la théorie de la rente — viennent de Malthus ». Pour Malthus, la population a tendance à augmenter géométriquement alors que la production de denrées alimentaires ne s'accroît que de manière arithmétique. Pour rétablir l'équilibre, la Nature dresse des obstacles efficaces (famines, épidémies, etc.) mais inhumains. Pour Malthus, un pasteur, il conviendrait plutôt de limiter la reproduction par des moyens artificiels. Il y a chez lui un certain pessimisme sur les capacités d'augmenter la production du fait de la loi des rendements décroissants, de la limitation des ressources naturelles et de la propension des humains à proliférer, qui devraient conduire à des famines. Les travaux de Malthus conduisent Thomas Carlyle à qualifier l'économie de « science lugubre ». Malthus conteste également qu'une économie de marché conduise automatiquement au plein emploi comme le fera également Keynes plus tard.\nAlors qu'Adam Smith s'intéressait à la production de revenus, David Ricardo axe ses recherches sur la distribution des revenus entre les propriétaires fonciers qui perçoivent des rentes, les travailleurs qui reçoivent des salaires (qui sont liés au minimum nécessaire pour subsister et donc au prix du blé) et les capitalistes dont les revenus sont constitués par les profits. Au centre de la problématique ricardienne se trouve le problème de la rente foncière (pour lui, la croissance de la population et des capitaux se heurte à une offre inchangée de terre qui pousse la rente foncière vers le haut et entraîne une baisse des salaires et des profits. L'œuvre de Ricardo se situe dans le contexte de l'abolition des corn laws qui favorisent les propriétaires terriens et de la conversion de l'Angleterre au libre-échange dont Ricardo avec la loi des avantages comparatifs est l'un des grands théoriciens.\nÀ la fin de la tradition classique, John Stuart Mill (1806-1873) se distingue des économistes antérieurs de cette école sur la question de la redistribution des revenus produits par le marché. Il attribue deux rôles au marché : une capacité à répartir des ressources et une capacité à répartir les revenus. Si le marché est efficace dans l'allocation des ressources, il l'est moins dans la distribution des revenus, ce qui oblige la société à intervenir.\n\nLa théorie de la valeur est un concept important dans la théorie classique. Adam Smith écrit que le prix réel de chaque chose est le labeur ou le travail et la peine de l'acquérir sous influence de sa rareté. Il soutient que, avec les rentes et les profits, les frais autres que les salaires entrent aussi dans le prix d'un produit. David Ricardo a systématisé et simplifié cet aspect de la pensée smithienne en élaborant ce qui a été appelé la « théorie de la valeur travail » qui a été plus tard reprise par Karl Marx alors que les néo-classiques lui ont substitué la théorie de l'utilité marginale.\n\nHarriet Martineau entame un projet de vulgarisation des concepts de la science économique naissante. À travers des anecdotes et des dialogues, elle fait découvrir à ses lecteurs les principes de l'économie de marché et la pensée d'Adam Smith, David Ricardo, Thomas Malthus et Jeremy Bentham. Elle publie d'abord Illustrations of Political Economy (1832-1834), puis Poor Laws and Paupers Illustrated (1833-1834) et Illustrations of Taxation (1834).\n\n\n==== Révolution marginaliste ====\n\nLa révolution marginaliste survient vers 1870-1871 quand Léon Walras, William Stanley Jevons et Carl Menger introduisent le concept d'utilité marginale centré sur la valeur pour le consommateur et récusent la valeur travail. Toutefois, entre les trois fondateurs du marginalisme, il est possible de relever de fortes différences.\n\nLéon Walras adopte une approche hypothético-déductive et propose un système d'équilibre général très abstrait.\n\nStanley Jevons, tout comme Léon Walras, veut également mathématiser l'économie mais il est plus inductif, il veut partir de l'étude des faits, des réalités, en raisonnant dans un cadre qui reste utilitariste (raisonnement sur le plan du plaisir et de la peine ou des avantages et inconvénients). Cette démarche aura une forte influence sur l'économie notamment aux débuts du vingtième siècle et marque toute l'économie appliquée actuelle.\n\nCarl Menger rejette l’usage des mathématiques et considère l’utilisation d’équations simultanées « à la Walras » comme incapable de mettre en lumière les relations causales ainsi que de rendre compte de la fugacité des échanges. Il trouve qu'il y a quelque chose de collectiviste chez le fondateur de l'école de Lausanne ; ce que cherche Menger, c’est une science capable de rendre compte du comportement des agents, de saisir l’essence des phénomènes économiques.\n\n\n==== Institutionnalisation de la discipline ====\nÀ la fin du XIXe siècle et au début du XXe siècle, les sciences économiques se structurent comme une discipline académique avec la création de départements d'économie dans les universités, de revues académiques spécialisées et d'associations professionnelles.\nPar exemple aux États-Unis, le département d'économie de l'université Harvard est créé en 1897 et le département d'économie de l'université de Californie à Berkeley est créé en 1903,. L'American Economic Association est créée en 1885, le Quarterly Journal of Economics en 1886, le Journal of Political Economy en 1892 et l'American Economic Review en 1911.\nAu Royaume-Uni, la British Economic Association, ancêtre de la Royal Economic Society est fondée en 1890, la revue The Economic Journal en 1891 et la London School of Economics en 1895.\nCharlotte Perkins Gilman publie en 1898 le premier ouvrage théorique sur les femmes et l'économie intitulé Women and Economics.\n\n\n=== Développement de la discipline au XXe siècle ===\nDans les années 1930, la science économique connaît deux grandes révolutions avec l'apparition de la macroéconomie et de l'économétrie.\nAvec la publication de la Théorie générale de l'emploi, de l'intérêt et de la monnaie (1936), John Maynard Keynes crée le champ de la macroéconomie.\n\nLes années 1930 sont aussi marquées par le développement de l'économétrie. Ragnar Frisch crée la société d'économétrie en 1930 et la revue Econometrica en 1933. Le développement de l'économétrie conduit à un usage de plus en plus important des statistiques dans la science économique. Les modèles économétriques peuvent aussi bien être utilisés pour calibrer un modèle économique existant que pour tester sa validité empirique.\nDans les années 1940 et 1950, les sciences économiques sont marquées par le développement des théories de la croissance économique avec le modèle de Harrod-Domar et surtout le modèle de Solow (Solow 1956), le développement des fondements de la théorie des jeux avec l'ouvrage fondateur de John von Neumann et Oskar Morgenstern (von Neumann et Morgenstern 1944) et les travaux de John Nash, et l'accomplissement des recherches sur l'équilibre général en concurrence parfaite avec les travaux de Kenneth Arrow et Gérard Debreu qui montrent les conditions d'existence et d'unicité de l'équilibre général imaginé par Léon Walras.\nDans les années 1960, les sciences économiques explorent de nouveaux sujets comme l'éducation, la criminalité ou encore la famille. Les travaux de Gary Becker sont emblématiques de cette tendance à utiliser la théorie économique pour analyser des sujets hors du domaine traditionnel de l'économie. En macroéconomie, les années 1960 sont marquées par les débats sur l'arbitrage inflation-chômage mis en évidence par la courbe de Phillips (Phillips 1958). La remise en cause de cette courbe avec le phénomène de stagflation conduit à formuler différentes hypothèses sur les anticipations des agents (anticipations adaptatives puis anticipations rationnelles).\nDans les années 1970 se développent les modèles économiques en information imparfaite comme le modèle de George Akerlof sur les asymétries d'information dans un marché (Akerlof 1970).\nEn macroéconomie, la fin des années 1980 et le début des années 1990 est marqué par un renouvellement des travaux sur la croissance économique autour de la notion de croissance endogène.\nLes années 1990 et 2000 sont caractérisées par une part de plus en plus importante de travaux empiriques dans la recherche en économie. Cette évolution est particulièrement vraie en économie du travail, en économie de l'éducation ou encore en économie du développement,. Le développement de l'économétrie appliquée dans ces années-là est notamment lié au développement d'un champ de recherche autour de l'inférence causale (voir notamment le modèle causal de Neymann-Rubin) et la diffusion de protocoles de recherche comme la méthode des variables instrumentales, la méthode des doubles différences ou encore la régression sur discontinuité. Les travaux de David Card sur l'effet de l'immigration sur le marché du travail (Card 1990) ou de David Card et Alan Krueger sur l'effet du salaire minimum sur l'emploi (Card et Krueger 1994) sont représentatifs de ce champ de recherche.\n\n\n== Courants et écoles de pensée ==\n\nL'économie compte de nombreux courants et écoles de pensées. Certains courants comme l'économie scolastique, le mercantilisme, la physiocratie et l'école classique ont été importants dans l'histoire de la pensée économique mais ne sont plus vraiment représentés aujourd'hui.\nLes quatre paradigmes en économie sont : le libéralisme, le marxisme, le keynésianisme et le schumpeterianisme.\nParmi les écoles de pensées, certaines se définissent comme hétérodoxes. Ce sont certains membres de institutionnalisme américain comme Hale Walton Hamilton qui ont été les premiers à revendiquer cette étiquette.\nParmi les autres écoles hétérodoxes, les plus souvent citées sont l'école autrichienne, le marxisme, le post-keynésianisme, mais aussi l'économie féministe, l'économie évolutionniste, la théorie de la dépendance, l'économie structuraliste, la théorie des systèmes mondiaux, ou encore l'école de la régulation et l'économie des conventions.\nLes courants ou écoles de pensée qui ne se revendiquent pas de l'hétérodoxie sont qualifiés d'orthodoxes. Parmi ces courants, on compte l'école classique, l'école néoclassique, le keynésianisme, l'école de la synthèse, le monétarisme, la nouvelle économie classique ou encore la nouvelle économie keynésienne.\n\n\n=== Scolastique ===\nLa scolastique est historiquement le tout premier courant de pensée économique, dont les fondations datent d'Aristote. Ses membres les plus importants sont Aristote, Augustin d'Hippone, Gilles de Rome et Christine de Pizan.\nL'école scolastique est un courant qui se fonde sur le respect de l'ordre social et de la hiérarchie, le rejet de l'usure et le devoir d'aider les démunis. Ce courant de pensée base particulièrement sa pensée sur la morale et sur les écrits aristotéliciens et bibliques.\n\n\n=== École classique ===\n\nL’école classique regroupe des économistes du XVIIIe siècle et du XIXe siècle. Ses membres les plus importants sont, en Grande-Bretagne, Adam Smith (1723-1790), David Ricardo (1772-1823), Thomas Malthus (1766-1834), John Stuart Mill (1806-1873), et en France, Étienne Bonnot de Condillac (1715-1780), Anne Robert Jacques Turgot (1727-1781), Jean-Baptiste Say (1767-1832) et Frédéric Bastiat (1801-1850).\nLe terme a été employé pour la première fois par Karl Marx dans Le Capital.\nLes auteurs postérieurs ont donné des définitions différentes de l'école classique. Par exemple, Karl Marx définit l’école classique par l’adhésion au concept de la valeur travail, il nomme les économistes ne faisant pas partie de cette école des économistes \"vulgaires\".\nCarl Menger caractérise lui aussi l’école classique par la notion de valeur travail.\nJohn Maynard Keynes définit l’école classique par l’adhésion à la « loi des débouchés » ou loi de Say dans la version popularisée par James Mill.\n\n\n=== École autrichienne ===\n\nL'école autrichienne d’économie est une école de pensée économique hétérodoxe qui prend comme point de départ l'individualisme méthodologique, rejette l’application à l’économie des méthodes employées par les sciences naturelles telles que sa mathématisation (voir l'économétrie), s’intéresse aux relations causales entre les événements, dont l’origine est l’action des individus et développe également une conception subjective de la valeur fondée sur la théorie de la valeur-utilité, et l'importance du marché comme révélateur des préférences individuelles et régulateur de la société.\nOn la fait généralement débuter en 1871 avec la publication par Carl Menger de ses Principes d'économie. Ses principaux représentants sont Carl Menger, Eugen von Böhm-Bawerk, Ludwig von Mises, Friedrich Hayek, Murray Rothbard et Jesús Huerta de Soto. L'expression « école autrichienne » a été utilisée pour la première fois vers 1870 par les économistes allemands de l'école historique, comme expression de mépris envers les thèses de Carl Menger, opposées aux leurs.\nLes partisans de l'école autrichienne défendent généralement des idées très libérales en matière économique et plus généralement d’organisation de la société. L'autrichianisme met en avant l'idée que les crises que subit le capitalisme sont exogènes : elles seraient le produit d'une intervention des pouvoirs publics, il faut donc que chaque domaine économique puisse être le plus possible régulé par le marché : cela peut s'agir de la finance sous tous ses aspects (pour Böhm-Bawerk), de l'émission de monnaie (pour Hayek), de la drogue (pour Rothbard), etc. Cependant, la majorité des autrichiens reconnait un rôle à jouer pour l’État, l'école autrichienne est loin d'être un courant l'anarcho-capitalisme, bien qu'il en soit l'inspiration majeure.\n\n\n=== Économie néoclassique ===\n\nL'école néoclassique a constitué le mainstream de la pensée économique jusqu'à l'avènement du keynésianisme. Le premier à utiliser le mot néo-classique est Thorstein Veblen en 1900 pour désigner l'économie marshallienne ; son usage s'est imposé à travers la redécouverte de Walras par Hicks, un article de George Stigler de 1941 et surtout par l'emploi de ce terme par Samuelson dans son manuel d'économie qui était alors le plus répandu au monde.\nL'économie néoclassique a recours de manière systématique au mécanisme d'offre et de demande pour déterminer les quantités et les prix à l'équilibre et pour étudier comment cela affecte la répartition de la production et la redistribution des revenus. Les marginalistes refusent la théorie de la valeur-travail héritée de l'économie classique et lui substituent l'utilité marginale (l'utilité de la dernière unité consommée).\nPar la microéconomie, l'économie néoclassique présente les incitations et les coûts comme jouant un rôle omniprésent dans l'élaboration de la prise de décision. Par exemple, la théorie du consommateur et la demande individuelle isolent la façon dont les prix (les coûts) et le revenu touchent la quantité demandée. En macroéconomie, ceci se traduit par une rapide et durable synthèse néoclassique,.\nBien que la plupart des néoclassiques admettent l'existence de défaillances de marché, le rôle des pouvoirs publics doit toujours être secondaire à celui du Marché. Milton Friedman notamment affirme que les banques centrales peuvent être bénéfiques pour l'économie si et seulement si ces institutions laissent le Marché s'autoréguler.\n\n\n=== Courant marxiste ===\n\nL'Économie marxiste résulte des travaux de Karl Marx (notamment des trois livres constituant Das Kapital, publiés en 1867, 1885 et 1894) et de Friedrich Engels. L'économie n'y est pas dans cette optique une science complètement séparée de la sociologie, de l'histoire, ou de l'anthropologie, mais se fonde sur le matérialisme historique, qui vise à unifier toutes les sciences sociales dans une science de la société. Trois points essentiels caractérisent ainsi l'économie du point de vue marxiste : le travail salarié, l'exploitation du prolétariat et les crises liées à l'accumulation de capital.\nMarx reprend la théorie de la valeur travail de Ricardo, mais cherche à analyser comment le système capitaliste a émergé et comment cela a donné aux capitalistes le pouvoir et la capacité d'exploiter les travailleurs qui n'ont que leur force de travail à vendre. Les crises s'inscrivent dans le cadre des lois de l'évolution du mode de production capitaliste.\nL'approche marxiste de l'économie cherche à analyser des lois de l'évolution du capitalisme, telles que la propension des capitalistes à accumuler, la tendance à des révolutions technologiques constantes, la soif inextinguible des capitalistes pour la plus-value, la tendance à la concentration, la tendance du capital à devenir de plus en plus « organique » (c'est-à-dire à moins recourir au capital variable qu'est la force de travail), la tendance au déclin du taux de profit, la lutte des classes, la tendance à une polarisation sociale croissante, la tendance à ce que les salariés soient employés dans des entreprises de plus en plus grandes et soient de plus en plus exploités et enfin, l'inéluctabilité des crises dans le système capitaliste. Les crises sont dans ce cadre toujours des crises de surproduction alors que les crises précapitalistes étaient des crises de sous-production (crises frumentaires). Les crises sont vues par les marxistes comme un moyen pour le capitalisme de se renouveler.\nL'économie marxiste se veut critique du système capitaliste, compris comme des rapports sociaux de domination du facteur travail par le facteur capital. Ses concepts sont : le mode de production, une théorie de la valeur, la superstructure, la formation de la plus-value puis du profit, la théorie de l'exploitation, et la baisse tendancielle du taux de profit.\nL'économie marxiste a un versant normatif, en ce qu'elle conteste la propriété privée lucrative des moyens de production et soutient que la maîtrise de la production devrait revenir directement aux salariés eux-mêmes ou en propriété d'état de manière transitoire selon les courants,.\nLa théorie marxiste a fait l'objet d'ajouts et de développements. Rosa Luxemburg, dans L'Accumulation du capital, introduit une analyse de l'économie internationale afin de montrer la dégradation des termes de l'échange.\nContrairement au marxisme orthodoxe, le courant marxien conteste l'inéluctabilité de l'effondrement du système capitaliste, dans cette école se trouvent notamment l'école de la régulation et Pierre Bourdieu. Les marxiens ne remettent cependant pas en question les autres caractéristiques de l'analyse marxiste.\nParmi les branches contemporaine marxistes on trouve en France les économistes de l'école de la régulation avec des chercheurs et théoriciens comme Michel Aglietta, Robert Boyer, et Gérard Destanne de Bernis.\nUne autre approche néomarxiste centrée sur la notion de valeur travail dénommée critique de la valeur fait son apparition en Allemagne dans les années 1980 basée sur la critique de la fétichisation de la marchandise,.\n\n\n=== Keynésianisme ===\n\nPour John Maynard Keynes (1883-1946), une économie de marché ne possède pas de mécanismes qui la conduisent de façon automatique vers le plein emploi de ses ressources, d'où la possibilité d'un chômage involontaire qui rend nécessaire une intervention extérieure au marché. Keynes raisonne d'emblée sous l'angle macroéconomique d'offre globale et de demande globale. Dans son cadre macroéconomique, la production, et donc l'emploi, dépend des dépenses. Si la demande n'est pas suffisante, les entreprises ne produiront pas assez et n'emploieront pas tous les salariés (demande effective) d'où la nécessité pour le gouvernement de conduire des politiques de soutien à la demande, c'est-à-dire de soutien à la consommation et/ou à l'investissement. Keynes insiste particulièrement sur l'investissement. En effet, ce dernier est la source du multiplicateur keynésien.\n\nAu cœur de la révolution keynésienne se trouve la réfutation de la « loi dite des débouché » de Jean-Baptiste Say qui énonce que l'offre crée sa propre demande. Cette loi fonde ou plutôt exprime l'optimisme et aussi le naturalisme de l'économie classique qui veut qu'il ne puisse y avoir de crise de surproduction durable.\nParmi les économistes keynésiens, il faut noter l'Américain Howard Bowen, qui a joué un rôle fondateur dans l'émergence de la notion de responsabilité sociétale des entreprises (corporate social responsibility en anglais), à travers son ouvrage Social Responsibilities of the Businessman (1953),.\n\n\n=== Post-keynésianisme ===\nLe keynésianisme de Keynes a eu trois successeurs. Les post-keynésiens, souvent associés à l'université de Cambridge et à Joan Robinson, mettent l'accent sur les rigidités macroéconomiques et d'ajustement. Les keynésiens de la synthèse néoclassique ont dominé la période des Trente Glorieuses et de nos jours la nouvelle économie keynésienne met davantage l'accent sur les comportements humains et les imperfections des marchés. Au niveau des théories de la croissance, ils utilisent des modèles de croissance endogènes.\n\n\n=== Institutionnalisme ===\nL'école institutionnaliste rejette de nombreux postulats de l'école néoclassique, comme l'hédonisme individuel justifiant la notion d'utilité marginale, ou l'existence d'un équilibre stable vers lequel l'économie converge naturellement. Thorstein Veblen en publie le premier ouvrage en 1899, avec « Why is Economics not an Evolutionary Science? »\nL'École institutionnaliste comprend des héritages de l'école historique allemande. elle se développe principalement aux États-Unis, où ses représentants sont : John Roger Commons, Arthur R. Burns, Simon Kuznets, Robert Heilbroner, Gunnar Myrdal, John Kenneth Galbraith.\n\n\n=== Nouvelle économie keynésienne ===\n\nNé dans les années 1980, ce courant de pensée économique est une réponse à la nouvelle économie classique. Il cherche à fournir des fondements microéconomiques solides à la macroéconomie de la synthèse néoclassique.\nL'un des fondateurs et des représentants les plus connus de ce courant est Joseph Stiglitz. En 2008, le président de la République française, Nicolas Sarkozy, lui confie une mission de réflexion (Commission Stiglitz) sur le changement des instruments de mesure de la croissance économique française.\n\n\n== Branches ==\n\nParmi les branches de l'économie, certaines, comme la microéconomie, la macroéconomie, l'économétrie ou l'économie de l'environnement (qui joue un rôle particulier dans une approche de développement durable), constituent des approches transverses de l'économie. D'autres branches de l'économie comme l'économie du travail, l'organisation industrielle, l'économie internationale ou l'économie de l'éducation, sont centrées sur une thématique ou un sujet particulier.\n\n\n=== Microéconomie ===\n\nPour Paul Krugman et Robin Wells, « l'un des thèmes majeurs de la microéconomie est la recherche de la validité de l'intuition d'Adam Smith, à savoir que des individus cherchant à satisfaire leurs intérêts propres contribuent souvent à promouvoir les intérêts de la société dans son ensemble. » En effet, ce qui intéresse la microéconomie, c'est tout d'abord l'étude des choix des agents économiques, c’est-à-dire de la manière dont ils procèdent à des « arbitrages » entre différentes options possibles, en comparant leurs avantages et leurs inconvénients pour la poursuite de leurs objectifs ou la satisfaction de leurs intérêts, postulat utilitariste.\nLa microéconomie examine les interactions existant sur les marchés en fonction de la rareté de l'information et la réglementation gouvernementale. On distingue le marché d'un produit ou service, par exemple celui du maïs frais, des marchés des facteurs de production, capital et travail. La théorie compare les agrégats de la quantité globale demandée par les acheteurs et la quantité fournie par les vendeurs et détermine ainsi le prix. Elle bâtit des modèles pour décrire comment le marché peut atteindre l'équilibre en matière de prix et de quantité ou comment réagir aux changements du marché au fil du temps, c'est ce qu'on appelle le mécanisme de l'offre et de la demande. Les structures de marché, telles que la concurrence parfaite, le monopole ou l'oligopole, sont analysées en fonction des conséquences sur le plan du comportement et de l'efficacité économique. L'analyse d'un marché unique se fait à partir d'hypothèses simplificatrices : rationalité des agents, équilibre partiel (c'est-à-dire qu'on suppose les autres marchés ne sont pas affectés). Un raisonnement en équilibre général permet d'analyser les conséquences sur les autres marchés, et peut permettre de comprendre les interactions et les mécanismes qui peuvent ramener à l'équilibre.\n\n\n==== Théorie microéconomique traditionnelle ====\nLa théorie microéconomique standard suppose que les agents économiques, ménages ou entreprises, sont « rationnels » c’est-à-dire qu'ils sont censés disposer de capacités cognitives et d'informations suffisantes pour pouvoir, d'une part, construire des critères de choix entre différentes actions possibles et identifier les contraintes pesant sur ces choix, contraintes tant « internes » (leurs capacités technologiques s'il s'agit d'entreprises, par exemple), qu'« externes » (c’est-à-dire résultant de leur environnement économique), et, d'autre part, maximiser leur satisfaction sous contraintes. C'est le paradigme de l'Homo œconomicus qui n'implique pas a priori que les critères de choix des individus soient purement égoïstes, ces derniers pouvant parfaitement être « rationnellement » altruistes.\nCette théorie doit son existence à la synthèse opérée par l'économie mathématique néoclassique des années 1940 et 1950 entre les apports du courant marginaliste du XIXe siècle et la théorie de l'équilibre général de Walras et de Pareto. John Hicks et Paul Samuelson sont considérés comme « les pères » de la microéconomie traditionnelle actuelle. Celle-ci s'organise autour de quatre volets :\n\nLa théorie du consommateur, qui étudie le comportement de ménages devant effectuer des choix de consommation de biens sous contraintes budgétaires ;\nLa théorie du producteur, qui étudie le comportement d'entreprises qui veulent maximiser leur profit sous contraintes technologiques ;\nLa théorie de l'échange sur des marchés, ces marchés pouvant être concurrentiels ou non concurrentiels ;\nLa théorie de l'optimum économique, qui mobilise le concept d'optimum de Pareto pour juger de l'efficacité économique collective des interactions entre agents au travers des échanges.\nLa théorie traditionnelle s'inscrit dans la perspective de l'équilibre général walrassien et a tendance « à assimiler le fonctionnement réel de la société à celui du modèle abstrait d'équilibre général ».\n\n\n==== Marché et défaillances du marché ====\n\nLa poursuite de l'intérêt particulier conduit souvent à l'intérêt général mais pas toujours.\nDès le début du XXe siècle et les travaux de Arthur Cecil Pigou, le concept de défaillance du marché s’est imposé dans la théorie économique orthodoxe. C’est un cas dans lequel le marché échoue dans l'allocation optimale des ressources économiques et des biens et services. Si la théorie économique décrivait déjà des situations de monopole (ou d'un cartel), ce concept décrit également d’autres situations, comme celle où coexistent chômage et pénurie de main d'œuvre (logements vides et personnes sans logements, etc.), ou encore la présence de pollution.\nUne défaillance de marché, qui concerne l'allocation économique, est une notion différente de celle plus financière d'anomalie de marché, au sens de non efficience du marché. Cette dernière concerne plutôt une anomalie du rendement financier (et une anomalie de prix, puisque le rendement a pour dénominateur le prix) due à des phénomènes comportementaux. Les deux phénomènes peuvent toutefois être les causes ou la conséquence l'un de l'autre, ou résulter de causes communes.\nLa notion de défaillance est éminemment politique et donc matière à controverse, dans la mesure où elle sert à justifier des interventions politiques visant à « corriger », voire à supprimer, le marché. Toutefois, la plupart des économistes l'utilisent, mais plutôt par rapport à des cas où le fonctionnement réel d'un marché donné s'éloigne significativement du marché idéal, sous l'effet de trois principales séries de causes :\n\ndes structures de marché sous-optimales (manque de transparence, délais, etc.) ;\nla non internalisation de coûts (voir externalité, Bien public et Asymétrie d'information : Sélection adverse, Aléa moral et Problème principal-agent) ;\ndes inefficiences de prix (non prise en compte de l'information par les prix) dues à des biais comportementaux.\n\n\n==== Prise en compte de la concurrence imparfaite ====\nÀ partir des années 1970, le paradigme dominant de la microéconomie connaît une forte inflexion de façon à mieux intégrer toutes les défaillances et imperfections du marché. Pour Pierre Cahuc « la nouvelle microéconomie s'est constituée progressivement, à partir de critiques éparses, souvent initialement isolées, du modèle walrasien ». Plus généralement, pour l'économiste Anne Perrot, l'édifice théorique de la microéconomie traditionnelle laissait « désarmé l'économiste à la recherche d'une représentation positive du fonctionnement du marché ».\nLe cadre général de la nouvelle microéconomie est davantage réduit à l'analyse d'un seul marché et sa démarche scientifique est plus axée sur la recherche de constat jugé représentatif du fonctionnement de l'économie (constats appelés « faits stylisés »). « Ces approches relèvent […] certains des défis que l'économie hétérodoxe, « institutionnaliste », a longtemps adressés à la théorie néoclassique ».\n\nLa nouvelle microéconomie met l'accent sur les problèmes d'incitations, d'information et sur la théorie des jeux. Par « incitation », on entend toute action d'un agent économique (qui peut être l'État) conduisant certains agents économiques à adopter tel ou tel type de comportement. Cette notion prend tout son sens si l'on considère que l'information disponible est inévitablement limitée pour un agent économique soucieux d'inciter d'autres agents à se comporter dans le sens de ses intérêts (lui donner les « bonnes » incitations de son point de vue).\nLa théorie des jeux, quant-à-elle, est une branche des mathématiques appliquées qui étudie les interactions stratégiques entre agents. Dans cette théorie les agents choisissent les stratégies qui maximiseront leurs bénéfices étant donné les stratégies que les autres agents choisiront. Elle fournit une modélisation formelle des situations dans lesquelles ceux qui prennent des décisions interagissent avec d'autres agents. La théorie des jeux généralise l'approche maximisatrice développée d'abord pour l'analyse des marchés, elle a été développée à partir du livre de 1944 Theory of Games and Economic Behavior, de John von Neumann et Oskar Morgenstern.\nL'extension de l'approche microéconomique a également conduit au développement de la « théorie des contrats ». Cette théorie conçoit les organisations, les institutions, les familles ou les entreprises, comme des ensembles de contrats (des « nœuds de contrats » dans le jargon économique),. Une entreprise est, par exemple, un nœud composé de contrats de travail, liant l'entreprise à ses salariés, de contrats la liant à ses clients et à ses fournisseurs, de contrats d'engagements bancaires et financiers, de contrats légaux la liant à son État ou ville de résidence en matières fiscale et règlementaire. Les marchés sont un autre cas particulier de tels nœuds de contrats, ici des contrats d'échange. Les États, au sens des organisations politiques gérant des espaces géographiques déterminés, sont un autre exemple de nœud contractuel, les Constitutions (ou les Chartes) se présentant comme des contrats généraux liant ces organisations aux peuples qu'ils gouvernent.\nUn aspect important de ces contrats est d'être généralement « incomplets », c'est-à-dire incapables de spécifier entièrement les engagements des parties dans tous les cas possibles. Le développement de cette théorie a naturellement entraîné un approfondissement des théories de la négociation et de la renégociation. En effet, son propos est non seulement d'expliquer comment et pourquoi se forment des contrats entre les agents, mais aussi les raisons pour lesquelles ils les remettent, ou pas, en cause au cours du temps.\nLa nouvelle microéconomique peut être utilisée par l'économie industrielle, l'économie du travail et l'économie publique du fait de son aptitude à se rapprocher des préoccupations pratiques des autorités de régulation et de certains industriels.\n\n\n=== Macroéconomie ===\n\nLa macroéconomie étudie l'économie dans son ensemble pour expliquer les grands agrégats (indicateurs économiques) et leurs interactions, en utilisant une forme simplifiée de l'équilibre général. Ces agrégats comprennent le revenu national, la production, le taux de chômage, les prix, l'inflation et d'autres agrégats comme la consommation totale et les dépenses d'investissement et leurs composants. Elle étudie également les effets de la politique monétaire et de la politique budgétaire.\nDepuis au moins les années 1960, la macroéconomie a été caractérisée par une recherche d'intégration dans les modèles du comportement de l'individu, y compris la rationalité des acteurs, l'utilisation efficace de l'information sur le marché et la concurrence imparfaite.\nL'analyse macroéconomique traite également des facteurs affectant la croissance du revenu national à long terme. Ces facteurs comprennent l'accumulation de capital, le changement technologique et la croissance de la population active.\n\n\n==== Croissance économique ====\n\nLes théories explicatives de la croissance économique ont été systématisées relativement récemment dans l'histoire de la pensée économique. Ces théories cherchent à expliquer pourquoi il y a croissance économique, c'est-à-dire augmentation de la production par habitant d'un pays sur une longue période ou encore pourquoi il existe des différences de PIB per capita (« par tête ») entre pays et pourquoi certains pays se développent plus rapidement que d'autres. En général, trois facteurs explicatifs sont utilisés : le travail c'est-à-dire la mobilisation de la main d'œuvre, le capital et le progrès technique.\nLe modèle de Harrod-Domar a ouvert la voie et a été suivi par le modèle de Solow. Alors qu'Harrod Domar raisonne avec une fonction de production à coefficient fixe, c'est-à-dire où il ne peut y avoir substitution capital travail, l'approche de Solow met l'accent sur la substitution capital travail et sur le progrès technique. L'opposition entre les modèles sur la substitution capital-travail est à replacer dans le contexte des années 1950 et 1960 et de l'opposition entre deux courants keynésiens : les post-keynésiens pour qui l'économie est relativement « rigide » et les tenants de la synthèse néo-classique plus libéraux. Pour Robert Solow, c'est grâce au progrès technique que la production peut augmenter et qu'il y a croissance sur la longue période. Toutefois, cette théorie explique mal d'où provient ce progrès qu'elle considère comme exogène,.\nLes nouvelles théories de la croissance économique cherchent précisément à construire des modèles expliquant l'apparition de ce facteur, c'est-à-dire à l'endogéneïser. Ces modèles ont été développés à partir de la fin des années 1970, notamment par Paul Romer, Robert E. Lucas et Robert Barro. Ils se fondent sur l'hypothèse que la croissance crée par elle-même le progrès technique. Ainsi, il n'y a plus de fatalité des rendements décroissants : la croissance engendre un progrès technique qui permet que ces rendements demeurent constants. La croissance, si elle crée du progrès technique, n'a donc plus de limite. À travers le progrès technique, la croissance constitue un processus qui s'auto-entretient.\n\n\n==== Cycle économique ====\nGénéralement, on associe la naissance de la macroéconomie au cycle économique de la grande dépression. Il fallait donc expliquer le processus conduisant à une telle chute de l'activité économique. C'est ainsi que John Maynard Keynes a écrit un livre intitulé Théorie générale de l'emploi, de l'intérêt et de la monnaie, expliquant ce phénomène qui à l'époque était resté sans éclaircissement convaincant. Keynes a soutenu que la demande globale de biens pourrait être insuffisante en période de ralentissement économique ce qui conduirait à un chômage élevé lié à des baisses de la production.\nKeynes préconise des réponses politiques actives - mesures de politique monétaire par la banque centrale et de la politique budgétaire - de la part du secteur public par le gouvernement pour stabiliser la production au cours du cycle. Ainsi, une conclusion centrale du keynésianisme est que, dans certaines situations, le marché n'arrive pas automatiquement à résoudre le problème du sous-emploi ; il faut donc une intervention externe. Le modèle IS/LM constitue le cadre théorique qui a servi à étayer la théorie de Keynes.\nSi la pensée keynésienne a dominé la scène durant les Trente Glorieuses, elle a aussi suscité une vive opposition. Milton Friedman et le monétarisme ont soutenu que l'action de l'État notamment en matière monétaire est inutile, voire nuisible.\nAu fil des ans, la compréhension du cycle économique s'est diversifiée dans plusieurs écoles, liées à ou opposées au keynésianisme. La jonction sera faite entre les deux principaux courants de l'époque au sein de ce que Paul Samuelson a qualifié de synthèse néo-classique. Cette synthèse indique que le keynésianisme est d'application à court terme, mais à long terme la théorie néoclassique explique aisément le cycle.\nLa nouvelle école classique, qui doit notamment aux critiques de Milton Friedman, est un courant de pensée économique qui s'est développé à partir des années 1970. Elle rejette le keynésianisme et se fonde entièrement sur des principes néoclassiques. Sa particularité est de reposer sur des fondations micro-économiques rigoureuses, et de déduire des modèles macroéconomiques à partir des actions des agents eux-mêmes modélisés par la micro-économie. Elle postule une rationalité des agents (qui cherchent à maximiser leur utilité), une anticipation rationnelle et qu'à chaque instant, l'économie possède un équilibre unique (avec plein emploi et pleine utilisation des capacités de production) et cet équilibre est atteint par un mécanisme d'ajustement des prix et des salaires.\nLa théorie des cycles réels, qui tente d'expliquer les fluctuations de court terme des économies comme le résultat de chocs fréquents et de faibles ampleur affectant les techniques de production constitue l'apport théorique majeur développée par ce courant.\nÀ l'opposé de ces deux dernières écoles, la nouvelle économie keynésienne conserve l'hypothèse des anticipations rationnelles, mais intègre une série de défaillances du marché. En particulier, les nouveaux keynésiens supposent une viscosité des prix et salaires, ce qui signifie qu'ils ne s'ajustent pas instantanément aux changements des conditions économiques.\n\n\n==== Politique monétaire et inflation ====\n\nLa politique monétaire est l'action par laquelle l'autorité monétaire, en général la banque centrale, agit sur l'offre de monnaie dans le but de remplir son objectif de stabilité des prix (limiter l'inflation). Elle tâche également d'atteindre les autres objectifs de la politique économique, qualifié de triangle keynésien : la croissance, le plein emploi, l'équilibre extérieur.\nLa politique monétaire se distingue de la politique budgétaire. Ces deux politiques interagissent et forment ensemble le policy-mix.\nD'après la théorie économique moderne, le but de la banque centrale est de maximiser le bien-être économique des ménages (Mishkin). Ainsi, on attribue généralement deux objectifs principaux à la politique monétaire : la stabilisation des prix et la stabilisation de l'activité économique. Ces deux objectifs sont étroitement liés, et non incompatibles comme on pourrait le penser, la stabilité des prix étant un préalable à une activité économique soutenue.\nCependant, conformément à la théorie quantitative de la monnaie, il n'existe pas d'arbitrage à long terme entre stabilité des prix et activité économique car la monnaie est, sur ce laps de temps, neutre (la « courbe de Phillips » de long terme est verticale).\nOn distingue généralement à l'heure actuelle trois types de politiques monétaires : la fixité des taux de change, le ciblage de la croissance des agrégats monétaires et le ciblage de l'inflation.\nAu demeurant, on note quatre niveaux au sein des dispositifs mis en place par les politiques monétaires : les objectifs finaux, les objectifs intermédiaires (agrégats de la monnaie ou les taux de change), les indicateurs (inflation, etc.) et les instruments (taux de facilité de prêt marginal, taux de facilité de dépôt, opérations d'open market […]).\n\n\n==== Politique budgétaire et fiscale ====\nLa politique budgétaire consiste à utiliser les leviers de l'imposition et des dépenses publiques. Combinée à la politique monétaire, elle forme un policy-mix avec la politique budgétaire dans le cadre d'une politique conjoncturelle.\nDans la pratique, en cas de creux économique, de récession ou de dépression, il est surtout question de politique budgétaire de stimulation de l'activité. Elle peut prendre la forme d'une baisse de l'imposition ou d'une hausse de certaines dépenses. Dans tous les cas, cela conduit à une dégradation du solde public. À l'inverse, dans les périodes de croissance économique élevée (y compris en période de bulle spéculative), la discipline budgétaire doit permettre de réduire le déficit public, voire de constituer des excédents, qui seront mobilisables ultérieurement.\nEn anglais, le terme fiscal policy regroupe à la fois la politique budgétaire et la politique fiscale. Ce dernier terme désignant en français les modifications de l'imposition, la redistribution etc. de façon à modifier les comportements individuels (encouragement au retour à l'emploi par un impôt négatif, crédit d'impôt en faveur de la recherche etc.).\nEn raison de la difficulté d'anticiper la conjoncture économique, de la lenteur d'action de la politique budgétaire, les économistes comptent généralement sur les stabilisateurs automatiques. L'idée est que quand la situation économique se dégrade, les recettes fiscales sont moins bonnes (l'État prélève donc moins) tandis que les dépenses augmentent (allocation chômage, etc.), de sorte que le déficit se creuse et soutient de façon automatique l'activité.\nLa nouvelle macroéconomie classique s'est opposée aux politiques budgétaires « discrétionnaires » qui à la fin des années 1970 et 1980 n'ont pas été couronnées de succès et ont contribué à accroître la dette publique. Avec la crise économique de 2008-2009, des politiques budgétaires de soutien ont été pratiquées partout à travers le monde. Si elles ont permis d'éviter que la récession ne se transforme en dépression, elles ont aussi engendré une hausse de la dette publique qui pose aujourd'hui problème notamment dans la zone euro.\n\n\n=== Économie des organisations ===\n\nL'économie des organisations étudie les organisations avec les outils de l'analyse économique. L'économie des organisations fait partie de la théorie des organisations.\nAu XVIIIe siècle, Adam Smith est un des premiers auteurs à réfléchir au fonctionnement de l'entreprise. Il met en valeur le fait que la division du travail par la spécialisation en fonction des compétences permet de dynamiser le marché (exemple de la manufacture d'épingle). Dans cette optique, l'entreprise et donc l'organisation reste une « boîte noire » dans la mesure où il n'analyse pas les phénomènes se produisant en son sein, mais simplement les motivations des individus et les conséquences sur le marché.\n\nPour Ronald Coase, mais aussi pour Oliver Williamson, la firme existe quand les coûts de coordination internes sont moins élevés que les coûts de transaction sur le marché, c'est-à-dire, par exemple, si pour produire un bien, il est moins cher d'engager des salariés et de les faire travailler que d'acheter le produit sur le marché.\nLa question de savoir si la firme constitue le royaume de la direction bureaucratique protégée du marché ou si la firme est simplement une « fiction légale », un nœud de relations contractuelles entre les individus, comme l'avancent la théorie de l'agence ainsi que Jensen et Meckling 1976, dépend « de la complétude des marchés et de la capacité des forces du marché à pénétrer les relations intra-firme ».\n\n\n=== Économie publique ===\n\nL'économie publique est une branche de l'économie qui étudie la production de biens publics et la fourniture des biens collectifs gratuits dont les coûts sont financés par les impôts et les emprunts publics.\n\n\n=== Économie du bien-être ===\nL'économie du bien-être est une branche normative de l'économie (c'est-à-dire qui cherche à déterminer ce qui, d'après elle, devrait être). Elle utilise à cet effet les outils de la microéconomie pour déterminer l'efficacité allocative et la distribution des revenus qui lui est associée. Elle veut mesurer le bien-être en examinant les activités des individus qui composent la société.\n\n\n=== Analyse économique du droit ===\nL'analyse économique du droit (Economic Analysis of Law ou Law and Economics selon l’appellation américaine) est la discipline qui cherche à expliquer les phénomènes juridiques grâce aux méthodes et concepts de la science économique. Entre la théorie juridique et la science économique, l’analyse économique du droit emprunte à ces deux disciplines pour expliquer d’une nouvelle façon les phénomènes juridiques.\n\n\n=== Économie internationale ===\nL'économie internationale est la branche des sciences économiques qui s'intéresse aux relations commerciales et économiques entre pays. Charles Kindleberger soulignait que « le seul fait que des nations souveraines existent entraîne des complications qui nous obligent à modifier nos instruments habituels d'analyse économique, si nous voulons les appliquer aux questions économiques internationales. » Paul Krugman au contraire estime que l'économie internationale recourt aux mêmes méthodes analytiques que les autres branches de l'économie. Toutefois, il insiste lui aussi sur le fait que « la matière de l'économie internationale porte sur les problèmes résultant des interactions entre États souverains ».\nL'économie internationale traite d'abord des théories du commerce international : avantages comparatifs, modèle Heckscher-Ohlin-Samuelson, les nouvelles théories du commerce international.\nPuis, elle s'intéresse à la réalité, c'est-à-dire à ce que font réellement les États, c'est ce qu'on appelle les politiques commerciales. Dans son ouvrage cité précédemment, Charles Kindleberger, s'intéressait également à ce qu'il appelle l'économie politique des barrières commerciales, c'est-à-dire au jeu des groupes de pression et de ceux qui recherchent des rentes.\nAutre point important de l'économie internationale : la monnaie vue sous l'angle du taux de change, de la balance des paiements et des prix relatifs. Puis viennent les problèmes de coordination au niveau international des politiques macro-économiques. Cette partie peut inclure les mouvements globaux de capitaux. Néanmoins, Kindleberger préférait les traiter à part dans ce qu'il appelait les mouvements des facteurs qui comprenaient, outre les flux financiers, les migrations de main d'œuvre.\n\n\n=== Finance ===\n\nLa finance désigne les méthodes et les institutions qui permettent aux entreprises et aux particuliers d'obtenir les capitaux nécessaires et aux épargnants de placer leurs capitaux. Les acteurs de la finance sont donc tous les agents économiques qui recherchent des capitaux ou qui souhaitent les placer.\nLa finance est devenue largement de nos jours un négoce d'instruments et de transfert d'anticipations de revenus et de risques dont les prix peuvent être négociés sur des marchés ou auprès d'institutions. Les risques, en particulier, peuvent être transférés à ceux qui sont disposés à les prendre (contre des revenus espérés), et les intermédiaires financiers peuvent pratiquer une compensation des risques inverses (par exemple, le risque de change d'un importateur est inverse de celui d'un exportateur, le risque de taux d'un prêteur est inverse de celui d'un emprunteur), la diversification des risques, etc.\nLa finance comprend plusieurs volets :\n\nla finance d'entreprise : c'est-à-dire la gestion financière des entreprises, notamment de leurs investissements et de leurs financements. C'est le domaine d'activité du directeur financier ;\nla finance de marché : c'est-à-dire le fonctionnement des grands marchés sur lesquels il est possible d'investir, de se couvrir, ou d'utiliser des instruments financiers complexes, comme les options.\n\n\n=== Économie du développement ===\n\nL'économie du développement est une branche de l'économie qui applique des techniques de l’analyse macroéconomique et microéconomique à l’étude des problèmes économiques, sociaux, environnementaux et institutionnels que rencontrent les pays dits en développement. Elle s'intéresse aux déterminants de la pauvreté et du sous-développement ainsi qu'aux politiques à mettre en œuvre pour sortir les pays en développement de leur sous-développement.\nL'origine de l'économie du développement moderne est liée à l'industrialisation de l'Europe de l'Est après la Seconde Guerre mondiale. Parmi les auteurs importants, on compte notamment Paul Rosenstein-Rodan, Kurt Mandelbaum Ragnar Nurkse, et Sir Hans Wolfgang Singer. À partir de cette même période d'autres auteurs vont s'intéresser à de nombreux pays d'Asie, d'Afrique et d'Amérique latine dont certains sont créés à la suite du mouvement de décolonisation. Au cœur de ces études, on trouve des auteurs comme Simon Kuznets et W. Arthur Lewis qui développent non seulement une stratégie de croissance économique mais aussi de changements structurels.\nCette phase pionnière qui reflète les préoccupations des années 1950 sera suivie à partir des années 1980 par une approche plus orientée vers le marché, préconisée alors par la Banque mondiale. Depuis la fin des années 1990, certains économistes du développement (notamment Michael Kremer et Esther Duflo) ont développé des outils permettant d'appréhender les effets des politiques économiques au niveau microéconomique et ont mis l'accent sur l'analyse d'expériences de terrain. Ils ont développé la théorie de la randomisation, de l'évaluation aléatoire et insistent sur l'importance des micro-projets dans les stratégies du développement. Pour certains, la randomisation aurait revitalisé la discipline de l'économie du développement de sorte que parfois, on parle d'économie du micro-développement. Toutefois, l'approche macroéconomique et institutionnaliste demeure dynamique avec des auteurs comme Daron Acemoglu, William Easterly, Douglass North ou Dani Rodrik.\n\n\n=== Économie du travail ===\n\nL'économie du travail analyse le fonctionnement du marché du travail défini comme le lieu de rencontre des travailleurs et des employeurs.\nDans le cadre d'une économie capitaliste, les « offreurs » de force de travail sont les travailleurs et les demandeurs sont les employeurs. L'économie du travail cherche à analyser la fixation des salaires, le taux d'emploi et le chômage, et permet de déterminer les meilleures politiques de l'emploi à mettre en place.\nIl y a deux approches possibles pour étudier le marché du travail. L'économie du travail peut être analysée à l'aide des techniques microéconomiques ou macroéconomiques. Les techniques macroéconomiques s'intéressent aux interactions entre le marché du travail et les autres marchés (bien, monnaie, commerce international). Il s'agit de savoir comment ces interactions influencent les variables macroéconomiques telles que le niveau de chômage, le taux de participation au marché du travail, le revenu agrégé et le produit intérieur brut.\nDepuis les années 1970, l'économie du travail s'est profondément renouvelée en lien avec la nouvelle microéconomie. Elle s'est orientée vers la prise en compte des imperfections informationnelles, de la concurrence imparfaite, et elle a intégré des éléments appartenant au courant hétérodoxe : étude du syndicalisme, segmentation des marchés. Elle a utilisé des théories plus récentes : la théorie des contrats implicites, la théorie du salaire d'efficience, la segmentation du marché du travail et la théorie des insiders-outsiders. L'économie du travail s'est aussi enrichie des travaux liés à l'économie de l'immigration.\n\n\n=== Économie de l'environnement ===\n\nLa défaillance du marché est une notion centrale à l'économie de l'environnement. La défaillance du marché signifie que les marchés ne parviennent pas à allouer de manière efficiente les ressources. Il existe donc un écart entre ce qu'une personne privée est prête à payer sur le marché pour l'environnement et ce que la société peut investir. Les formes courantes de défaillance du marché incluent certaines externalités négatives ou la gratuité apparente du capital naturel et des services écologiques.\nUne externalité est la conséquence de l'interdépendance des agents économiques qui échappe au système d'appréciation du marché, par exemple les pollutions engendrées par certains qui entraînent des dépenses diverses de protection chez ceux qui en sont victimes. Naît donc un besoin de réglementation du marché qui analyse les productions humaines selon divers critères comme la non-rivalité et le niveau d'excludabilité (comme les biens publics).\nL'économie de l'environnement est intimement liée à la notion de développement durable, puisque celui-ci est constitué des trois piliers environnemental, social et économique. La 6e édition d'un ouvrage des économistes américains Tom Tienteberg et Lynne Lewis, publiée en 2009, montre les multiples aspects de l'économie de l'environnement liés à la notion de durabilité : équité intergénérationnelle, économie des ressources naturelles épuisables ou renouvelables, énergie, gestion de l'eau, zones de pêche et espèces recherchées pour leur valeur commerciale, gestion des pollutions, changement climatique, pollution de l’air à l’échelle locale et régionale, gestion des déchets, etc.\nAinsi, l'économie joue un rôle important dans le développement durable.\n\n\n=== Économie de la culture ===\n\nL'économie de la culture est une branche de l'économie s'intéressant aux aspects économiques de la création, de la distribution et de la consommation d'œuvres d'art. Longtemps cantonné aux beaux-arts, aux spectacles vivants et au patrimoine historique dans la tradition anglo-saxonne, son spectre s'est élargi depuis le début des années 1980 à l'étude des particularités des industries culturelles (cinéma, édition de livres ou de musique) ainsi qu'à l'économie des institutions culturelles (musées, bibliothèques, monuments historiques).\nLa délimitation de l'économie de la culture pose le même problème que la délimitation de la culture elle-même. Le cœur de l'économie de la culture, et historiquement son premier domaine, est donc l'étude des beaux-arts et des spectacles vivants (théâtre, opéra). Ces thèmes constituent encore une part importante des articles de recherche.\nToutefois, un fait saillant des XIXe et XXe siècles est l'apparition de la culture de masse par le biais de biens à contenu culturel, mais produits selon des méthodes industrielles. La naissance de la grande industrie a eu des répercussions considérables sur les modes de pensée et d'action. L'apparition de l'objet de série, qui détrône la pièce unique autrefois sacralisée par l'art, va profondément modifier les comportements. Les économistes de la culture ont fait valoir la difficulté à faire des distinctions dans ce domaine, qui relèvent le plus souvent de jugements de valeurs subjectifs. Ils ont également mis en avant des spécificités dans la sélection des produits, leur fabrication et leur demande qui permettaient de différencier les biens culturels. Ainsi, ces derniers ont pour trait commun d'incorporer un élément créatif dans leurs caractéristiques essentielles. Cependant, cette caractérisation est trop large. L'importance croissante du design fait que pour certains produits pouvant difficilement être considérés comme culturels (vêtements, baladeurs numériques), la dimension de créativité fait l'essentiel de la valeur.\nC'est pourquoi les économistes du champ ont adopté le concept d'industries de contenu pour désigner l'ensemble du secteur produisant des biens dont l'essentiel de la valeur tient à leur contenu symbolique plutôt qu'à leurs caractéristiques physiques. Ainsi, un livre est un bien culturel, que le texte soit relié ou non, la couverture solide ou non, tandis qu'un baladeur numérique hors-service n'a plus de valeur malgré son design.\n\n\n=== Organisation industrielle ===\n\nL'organisation industrielle ou économie industrielle est le champ de l'économie qui étudie le comportement stratégique des entreprises et plus généralement des organisations en fonction des structures des marchés.\nL'étude de l'organisation industrielle ajoute au modèle de concurrence pure et parfaite des « frictions » du monde réel tels que : une information limitée, les coûts de transaction, les coûts nécessaires aux changements de prix, l'action du gouvernement et les barrières à l'entrée mises à l'arrivée de nouveaux concurrents.\nL'émergence de l'organisation industrielle comme champ séparé doit beaucoup à Edward Chamberlin, Edward Mason et Joe Bain.\n\n\n=== Économie de la santé ===\nL'économie de la santé est l'application de la science économique au domaine de la santé. Cette discipline a connu un fort essor à la suite du développement des dépenses de santé dans les pays développés et aux problèmes qu'elles posent aux comptes sociaux (comptes nationaux de la santé).\n\n\n=== Économie de l'éducation ===\nL'économie de l'éducation est une branche de l'économie qui traite de l'influence de l'éducation sur le développement économique des pays.\n\n\n== Méthodologie et épistémologie ==\n\n\n=== Méthode déductive ou a priori de John Stuart Mill ===\nPour John Stuart Mill, non seulement « la méthode a priori est un mode légitime d'investigation philosophique dans les sciences morales » mais « elle en est le mode unique ». Pour Daniel Hausman la méthode à priori ou inductive comporte trois phases : l'observation des faits, des phénomènes ; la déduction de lois et enfin la vérification des lois en examinant leur pouvoir prédictif. Il ne s'agit pas ici de vérifier la véracité des lois mais leur pouvoir prédictif .\n\n\n=== Méthode hypothético-déductive ===\n\nCette méthode a dominé jusqu'aux années 1940. Elle a été utilisée par David Ricardo et explicitement formulée par John Stuart Mill en 1836 et 1843 et Nassau Senior en 1836. Elle comporte quatre étapes : 1) formuler une hypothèse, 2) en déduire une prédiction, 3) tester la prédiction, 4) évaluer l'hypothèse en fonction de la pertinence de la prédiction. Selon Mill, les hypothèses de base de l'économie sont constituées par des introspections psychologiques (les individus veulent plus de richesses) soit sur des hypothèses que l'on peut vérifier empiriquement (la loi des rendements décroissants). Pour lui, la science économique est davantage destinée à vérifier les hypothèses de base que de tester la précision des prédictions qui dépend de multiples causes. Pour Mill donc, l'économie est une science inexacte qui ne peut dégager que des tendances et qui doit se confronter aux tests empiriques de façon à progresser. Senior pose les bases des axiomes de la microéconomie classique en dessinant les contours de l'homo œconomicus.\nCette méthode sera reprise par J. E. Cairns en 1875 et par John Neville Keynes en 1891. Si les néo-classiques de tradition autrichienne ou Walrassienne sont d'abord focalisés sur la prise de décision individuelle et sur les effets micro-économiques de court terme, néanmoins, ils adoptent eux aussi la méthode hypothético-déductive comme le montrent les écrits de Frank Knight (1935 et 1940), de Ludwig Von Mises (1949, 1978, 1981) et de Lionel Robbins (1935).\n\n\n=== Méthode abduction-déduction-induction ===\nFormulée par les tenants de l'école de la régulation, elle consiste à d'abord évaluer un phénomène ou une anomalie avant d'en déduire des hypothèses causales probables, qu'il s'agit de tester jusqu'à ce qu'il ne reste plus que des hypothèses encore incontestées ou d'éléments inexpliqués.\n\n\n=== Révolution méthodologique des années 1930 à 1950 ===\nLe livre de Terence W. Hutchison intitulé The Significance and Basic Postulates of Economic Theory est le premier à critiquer la théorie économique du point de vue de l'empirisme logique. Il reproche à la théorie économique de ne pas avoir de contenu testable À sa suite, Paul Samuelson va développer une approche qualifiée d'« opérationaliste » par Daniel Hausman visant à donner une place importante au comportement des individus mais qui entre en conflit avec une volonté d'avoir une théorie économique. Fritz Machlup accuse Hutchinson et Samuelson de vouloir directement atteindre les postulats de la théorie économique au lieu de se focaliser sur leur conséquences observables.\nMilton Friedman dans son livre The Methodology of Positive Economics, l'ouvrage le plus influent de la période insiste sur le fait que la science et la théorie ont exclusivement un but prédictif. Aussi l'important pour juger d'une théorie ne réside pas dans le caractère réaliste ou non des hypothèses mais dans la capacité de la théorie à prédire ce qui va se passer.\n\n\n== Méthodes ==\n\n\n=== Modélisation ===\n\nLa théorie économique a recours à la modélisation pour appréhender le réel.\nCette modélisation s'appuie souvent sur un formalisme mathématique.\nAu XIXe siècle, les économistes français comme Jules Dupuit ou Augustin Cournot ont été pionniers dans l'utilisation des mathématiques.\nAu XXe siècle, Paul Samuelson a grandement contribué à uniformiser le formalisme mathématique des modèles économiques.\nDans la seconde moitié du XXe siècle, le développement de la théorie des jeux a largement renouvelé la modélisation économique.\n\n\n=== Méthodes statistiques ===\n\nLa branche de l'économie consacrée plus spécifiquement à l'étude quantitative des modèles économiques est l'économétrie.\nL'usage des modèles statistiques en économie s'est développé avec la création de la Société d'économétrie en 1930 et de la revue Econometrica en 1933. Depuis lors, l'économétrie n'a cessé de se développer et de prendre une importance croissante au sein de la science économique. Aujourd'hui, l'on distingue l'économétrie théorique de l'économétrie appliquée. L'économétrie théorique se focalise essentiellement sur deux questions, l'identification et l'estimation statistique. Tandis que l'économétrie appliquée utilise les méthodes économétriques pour comprendre des domaines de l'économie comme l'analyse du marché du travail, l'économie de l'éducation ou encore tester la pertinence empirique des modèles de croissance.\nL'économétrie a pris une place de plus en plus importante dans les publications scientifiques depuis la seconde moitié du XXe siècle. On estime en effet qu'en 2011, 80% des articles publiés dans les meilleures revues en économie contiennent au moins un test empirique.\n\n\n=== Méthodes expérimentales ===\n\nDepuis les travaux des psychologues Amos Tversky et Daniel Kahneman et de Vernon Smith, les expériences de laboratoire sont devenues une méthode à part entière en sciences économiques pour valider empiriquement la pertinence des théories économiques. Ainsi, les travaux expérimentaux en théorie de la décision ont montré que les agents ne se comportaient pas selon la théorie de l'utilité espérée (théorie développée par John von Neumann et Oskar Morgenstern dans Theory of Games and Economic Behavior). La théorie des perspectives, développée par Amos Tversky et Daniel Kahneman (Kahneman et Tversky 1979), est plus conforme aux résultats expérimentaux.\nParallèlement aux expériences de laboratoires, les économistes développent également des expériences de terrain à grande échelle pour tester des théories économiques ou encore évaluer l'effet de politiques publiques. Ces méthodes se sont largement développées depuis les années 1990. En économie du développement par exemple, les économistes Esther Duflo et Abhijit Banerjee ont largement diffusé l'usage de ces méthodes, notamment à travers la création d'un institut voué à ces méthodes, le Abdul Latif Jameel Poverty Action Lab.\n\n\n=== Méthode historique ===\n\n\n== Place de l'économie dans la société ==\n\n\n=== Liens avec le pouvoir politique ===\n\nDès le XVIe siècle et l'avènement du mercantilisme, l'économie est à la fois une discipline qui cherche à comprendre le monde et une discipline qui vise à orienter les politiques publiques. Avec le glissement de la notion d'économie politique à la notion de science économique au milieu du XXe siècle, l'économie a partiellement gommé sa nature politique.\n\n\n=== Rôle de l'économie dans le développement durable ===\n\nLe développement durable vise à instaurer des normes qui permettent de satisfaire les besoins humains, avec une triple préoccupation de protection de l'environnement, d'équité sociale, et de performance économique, non seulement pour le présent mais également pour le futur. Le terme a été utilisé par le rapport Brundtland (1987) qui lui a donné sa légitimité et sa signification de « développement qui satisfait les besoins du présent sans compromettre ceux des générations futures ».\nC'est en 1992, au sommet de la Terre de Rio de Janeiro, qu'ont été définis les trois piliers du développement durable, ainsi que l'agenda 21 pour les collectivités territoriales, qui reprend la triple préoccupation économique, sociale, et environnementale. Les Nations unies ont défini en septembre 2015 dix-sept objectifs de développement durable qui, tout étant lié, concernent tous directement ou indirectement l'économie. Par exemple, l'objectif no 12 vise à « établir des modes de consommation et de production durables », étant donné que « La consommation et la production mondiales — véritables moteurs de l’économie mondiale — reposent sur une utilisation de l’environnement et des ressources naturelles d’une manière qui continue à avoir des effets destructeurs sur la planète ».\n\n\n=== Responsabilité sociétale des entreprises ===\n\nLa responsabilité sociétale (ou sociale) des entreprises (corporate social responsibility en anglais) consiste en l'application des principes de développement durable à l'entreprise. Les entreprises sont ainsi invitées à viser des objectifs de triple performance environnementale, sociale et économique dans leur stratégie (triple bottom line en anglais). On a vu ci-dessus le rôle de précurseur qu'a joué l'économiste keynésien Howard Bowen dans l'émergence de cette notion, avec la publication dès 1953 de son ouvrage fondateur Social Responsibilities of the Businessman.\n\n\n== Liens avec d'autres disciplines ==\nL'économie est une science sociale. Elle partage avec d'autres sciences sociales certains sujets d'études et certaines méthodes.\n\n\n=== Liens avec l'anthropologie ===\n\nEn étudiant les formes de l'échange dans les sociétés non occidentales, les anthropologues ont découvert d'autres formes que le marché ou le troc. En particulier, en prenant appui sur les travaux de Bronisław Malinowski, Marcel Mauss a mis en évidence le rôle du don comme système d'échange économique.\n\n\n=== Liens avec la sociologie ===\n\nDepuis les travaux de Gary Becker sur la famille (Becker 1981), la criminalité (Becker 1957) ou l'éducation (Becker 1964), les sciences économiques ont tendance à explorer des sujets d'ordinaire réservés à la sociologie. On désigne par impérialisme économique cette tendance des sciences économiques à analyser des sujets traditionnellement du domaine de la sociologie.\nÀ l'inverse, la sociologie économique analyse l'économie avec les outils et les théories sociologiques. Ainsi les travaux de Mark Granovetter ont montré l'importance des réseaux sur le marché du travail (ce que les économistes avaient ignoré jusque-là). De même, les travaux de sociologie des marchés montrent le caractère socialement construit des marchés à l'opposé de la vision naturaliste des économistes.\n\n\n=== Liens avec la psychologie ===\n\nLes travaux pionniers de Daniel Kahneman et Amos Tversky sur la théorie de la décision en univers risqué (Kahneman et Tversky 1978) ont donné lieu à un domaine de recherche qui relève à la fois de la psychologie et de l'économie.\n\n\n=== Liens avec la psychanalyse ===\n\nEn métapsychologie psychanalytique, le « point de vue économique » est l'un des trois points de vue que distingue Sigmund Freud, les deux autres étant le « point de vue topique » et le « point de vue dynamique ». En tant que processus psychique, il consiste en la circulation et répartition d'une énergie quantifiable d'ordre pulsionnel (dite aussi énergie pulsionnelle).[pertinence contestée]\n\n\n=== Liens avec les sciences politiques ===\n\nJusqu'au milieu du XIXe siècle, les auteurs spécialisés dans les questions sociales ne faisaient pas de distinction entre leurs écrits relevant de la science politique et leurs écrits relevant de l'économie. La révolution marginaliste a conduit à une plus grande séparation entre la science politique et l'économie. L'économie s'est alors recentrée sur les mécanismes des prix sur un marché et adopté des outils mathématiques alors que la science politique est restée dans une tradition plus littéraire.\nPendant la première moitié du XXe siècle, la science politique et l'économie ont eu des agendas séparés. À la fin des années 1950 et au début des années 1960, les économistes[à vérifier] ont utilisé leur corpus théorique pour analyser le comportement de l'électeur et des hommes politiques.\n\n\n=== Liens avec les neurosciences ===\n\nDans la lignée des travaux en économie comportementale, la neuroéconomie utilise les méthodes des neurosciences cognitives pour comprendre la manière dont les agents prennent leurs décision.\n\n\n=== Liens avec la physique ===\n\nL'éconophysique est une discipline qui utilise des modèles de physique statistique pour étudier des phénomènes économiques.\n\n\n=== Liens avec les sciences de gestion ===\nLa gestion est la mise en pratique des théories économiques de l'entreprise par l'utilisation de méthodes et d'indicateurs spécifiques aux différentes fonctions représentées dans l'organisation. Il existe d'étroites relations entre la théorie économique et la gestion. La différence tient dans le caractère plus appliqué et plus explicitement normatif de la gestion. Certains spécialistes des sciences de gestion réduisent l'objet de la gestion à l'entreprise, d'autres l'élargissent à toutes organisations humaines. Dans le sens de sciences et de techniques d'administration, la gestion s'est départementalisée en suivant le découpage en fonction dans les organisations (gestion commerciale : Commerce, gestion financière (analyse financière) : Finance, gestion de production : Production industrielle par exemple…).\n\n\n== Place des femmes en économie ==\n\nL'économie est une discipline qui a longtemps été largement dominée par les hommes. On peut souligner cependant quelques figures de femmes économistes. Harriet Martineau (1802-1876) a joué un rôle important dans la diffusion de la pensée des classiques. Charlotte Perkins Gilman publie également de nombreux ouvrages théoriques en économie et sociologie sur la place des femmes.\nMary Paley Marshall (1850-1944) a été la première lecturer (maîtresse de conférence) dans une faculté d'économie au Royaume-Uni. Elle est l'autrice de The Economics of Industry avec son mari Alfred Marshall. Joan Robinson (1903-1983) a été une économiste importante. Enfin Anna Schwartz (1915-2012) est une spécialiste d'histoire économique qui a notamment écrit A Monetary History of the United States, 1867–1960 (1963) avec Milton Friedman.\nSeules trois femmes ont reçu le prix Nobel d'économie : Elinor Ostrom en 2009, Esther Duflo en 2019 et Claudia Goldin en 2023, et cinq la médaille John-Bates-Clark : Susan Athey en 2007, Esther Duflo en 2010, Amy Finkelstein en 2012, Emi Nakamura en 2019 et Melissa Dell en 2020.\nEn 2017, une étude a fait scandale dans la profession en révélant les commentaires sexistes sur un site de rumeurs sur le marché du travail des postes académiques en économie aux États-Unis. Cette étude a mis en lumière un environnement toxique envers les femmes dans la profession.\nLes femmes sont nettement sous représentées dans la profession (19 % des auteurs dans la base RePEc en 2018) et la proportion de femmes n'augmente pas, quoique la situation soit plus favorable pour les femmes en France qu'aux États-Unis. Christine Lagarde par exemple, a été ministre de l'Économie française (2007-2011), puis directrice générale du Fonds monétaire international (2011-2019) avant de prendre le poste de présidente de la Banque centrale européenne (2019-). « Au niveau du corps enseignant, l’Hexagone est en meilleure position que les États-Unis. « Alors qu'outre-Atlantique, seulement 29 % des “assistants professors” et 14 % des “full professors” en économie sont des femmes, la situation est un plus équilibrée en France : la proportion de femmes maîtres de conférences était de 43 % en 2016, et celle des femmes professeurs des universités, de 24 % » » . \n\n\n== Critiques et controverses ==\n\n\n=== Contestation de la scientificité de l'économie ===\nL'économie ne répond pas aux critères de scientificité établis par Karl Popper. L'économie ne peut, en effet, dégager de lois, comme le ferait la physique. L’appellation d'« économie politique » en filiation d'avec les autres sciences sociales est aujourd'hui revendiquée en opposition d'une scientificité de l'économie proche des sciences naturelles.\n\n\n=== Capacité prédictive ===\nLa capacité de l'économie comme discipline à fournir des prédictions solides sur l'évolution de l'économie a été remise en question à l'occasion de certaines crises économiques, telles que la grande récession de 2008.\n\n\n=== Mathématisation ===\nL'évolution de l'économie vers une mathématisation accrue a été source de critiques.\n\n\n== Prix et distinctions ==\nDepuis 1969, la Banque de Suède décerne annuellement le prix de la Banque de Suède en sciences économiques en mémoire d'Alfred Nobel communément appelé prix Nobel d'économie. Il est le prix le plus prestigieux de la discipline.\nAprès ce prix, la médaille John-Bates-Clark est la récompense la plus prestigieuse en économie.\nIl existe des prix par pays (le cas de médaille John-Bates-Clark pour les Etats-Unis d'Amérique, Prix du meilleur jeune économiste de France pour la France, Prix Nakahara pour le Japon, etc.), pour la promotion du genre (le Prix Elaine-Bennett pour la recherche nommée d'après l'économiste Elaine Bennett), par région ou continent (le cas Prix Yrjö-Jahnsson pour le continent européen, par exemple), pour un domaine spécifique (le cas de Prix Fischer-Black la finance, le cas de Prix IZA de l'économie du travail pour l'économie du travail, par exemple), etc.\nElinor Ostrom est devenue en 2009 la première femme lauréat du Prix Nobel d'économie pour ses recherches sur la gouvernance économique, et en particulier, celle des biens communs »,.\nEn 2019 le prix Nobel d'économie est attribué pour la première fois à une femme française, Esther Duflo, et son équipe pour leurs travaux sur la lutte contre la pauvreté.\n\n\n== Enseignement ==\n\n\n=== France ===\n\n\n== Bibliographie ==\n\n\n=== Ouvrages fondamentaux ===\nXénophon, L'Économique\nAristote, Éthique à Nicomaque 5.5 et Politique I, 8-10\nAristote, Économiques, texte établi par B. A. van Groningen, traduction par André Wartelle, Les Belles Lettres, 2002 (trois livres pas entièrement attribués à Aristote).\nAntoine de Montchrestien, Traité d'économie politique, 1615\nRichard Cantillon, Essai sur la nature du commerce en général, 1755 (lire sur Wikisource)\nFrançois Quesnay, Tableau économique, Versailles, 1759\nAdam Smith (trad. de l'anglais), Recherches sur la nature et les causes de la richesse des nations [« An Inquiry into the Nature and Causes of the Wealth of Nations »], 1776 (lire sur Wikisource)\nThomas Malthus (trad. de l'anglais), Essai sur le principe de population [« An Essay on the Principle of Population »], Londres, 1798 (lire sur Wikisource)\nJean-Baptiste Say, Traité d'économie politique, 1803 (lire sur Wikisource)\nDavid Ricardo (trad. de l'anglais), Des principes de l'économie politique et de l'impôt [« On the Principles of Political Economy and Taxation »], 1817 (lire sur Wikisource)\nJohn Stuart Mill (trad. de l'anglais), Principes d'économie politique [« Principles of Political Economy »], 1848\nKarl Marx (trad. de l'allemand), Le Capital [« Das Kapital »], 1867\nCarl Menger (trad. de l'allemand), Principes d'économie [« Grundsätze der Volkswirtschaftslehre »], 1871\n(en) William Stanley Jevons, Theory of Political Economy, 1871, 278 p.\n(en) Alfred Marshall et Mary Paley Marshall, The Economics of Industry, Bastian Books, 1879, 252 p. (ISBN 978-0-554-52524-2)\nLéon Walras, Éléments d’économie politique pure : ou théorie de la richesse sociale, 1874\nAlfred Marshall (trad. de l'anglais), Principes d'économie politique [« Principles of Economics »], 1890\n(de) Carl Menger, Die Social-Theorien der Classichen National-Oekonomie und die modern Wirschaftspolitie, Vienne, Neue Freie Presse, 1891\nThorstein Veblen, The Theory of the Leisure Class (œuvre écrite), Macmillan Publishers, New York, 1899.\nVilfredo Pareto, Manuel d'économie politique, V. Giard & E. Brière, 1909\nJohn Maynard Keynes, Théorie générale de l'emploi, de l'intérêt et de la monnaie, 1936\n(en) A.C. Pigou, The Economics of Welfare, Londres, Macmillan Co., 1948, 4e éd. (1re éd. 1932)\n(en) Joseph A. Schumpeter, History of Economic Analysis, Oxford, Oxford University Press, 7 mars 1954, 1260 p. (ISBN 978-0-19-510559-9)\n(en) John von Neumann et O. Morgenstern, Theory of Games and Economic Behavior, Princeton, Princeton University Press, 1944\n(en) P. A. Samuelson, Foundations of Economic Analysis, Cambridge, Mass., Harvard University Press, 1947\n(en) Milton Friedman, The Methodology of Positive Economics, Essays in Positive Economics, Chicago, University of Chicago Press, 1953\nKenneth Arrow, Social Choice and Individual Values, New York, Wiley, 1951 (ISBN 0-300-01364-7)\n(en) Gary S. Becker, The Economics of Discrimination, Chicago, University of Chicago Press, 1957, 1re éd., 167 p. (ISBN 0-226-04115-8)\n(en) Anthony Downs, An Economic Theory of Democracy, Prentice Hall, 1957, 1re éd. (ISBN 978-0-06-041750-5)\n(en) Gary S. Becker, Human Capital : A Theoretical and Empirical Analysis, with Special Reference to Education, Chicago, University of Chicago Press, 1964, 1re éd., 390 p. (ISBN 978-0-226-04120-9)\n(en) Gary S. Becker, A Treatise on the Family, Cambridge, MA, Harvard University Press., 1981, 424 p. (ISBN 0-674-90698-5)\n(en) Milton Friedman et Anna Schwartz, A Monetary History of the United States, 1867–1960, Princeton University Press, 1963, 888 p. (ISBN 978-0-691-00354-2)\n(en) Milton Friedman, Capitalism and Freedom : Fortieth Anniversary Edition, The University of Chicago Press Books, 1962, 230 p. (ISBN 978-0-226-26421-9, lire en ligne)\n(en) Friedrich Hayek, The Road to Serfdom : text and documents, Chicago, The University of Chicago Press Books, 1944, 304 p. (ISBN 978-0-226-32054-0)\nJohn Stuart Mill, Sur la définition de l'économie politique; et sur la méthode d'investigation qui lui est propre, Michel Houdiard Éditeur, 20031844\n(en) Steve Keen, Debunking Economics : The Naked Emperor of the Social Sciences, Zed Books, 2001\nJean Tirole, Économie du bien commun, Presses universitaires de France, 2016.\n\n\n=== Articles fondamentaux ===\n(en) Ronald H. Coase, « The Nature of the Firm », Economica, vol. 4, no 16,‎ novembre 1937, p. 386-405 (DOI 10.1111/j.1468-0335.1937.tb00002.x)\nPaul A. Samuelson, « The Pure Theory of Public Expenditure », The Review of Economics and Statistics, The MIT Press, vol. 36, no 4,‎ novembre 1954, p. 387-389 (DOI 10.2307/1925895, JSTOR 1925895)\n(en) Ronald H. Coase, « The Problem of Social Cost », Journal of Law and Economics, vol. 3,‎ octobre 1960, p. 1-44 (JSTOR 724810, lire en ligne)\n(en) Robert Solow, « Technical Change and the Agregate Production Function », The Review of Economics and Statistics, vol. 39, no 3,‎ août 1957, p. 312-320 (JSTOR 1926047)\n(en) Robert Solow, « A Contribution to the Theory of Economic Growth », Quarterly Journal of Economics, vol. 70, no 1,‎ 1956, p. 65-94 (DOI 10.2307/1884513, lire en ligne)\n(en) Alban W. Phillips, « The relation between unemployment and the rate of change of money wage rates in the United Kingdom, 1861–1957 », Economica,‎ 1958, p. 283-299\n(en) Milton Friedman, « The Social Responsibility Of Business Is to Increase Its Profits », The New York Times,‎ 13 septembre 1970 (lire en ligne)\n(en) George Akerlof, « The Market for Lemons : Quality Uncertainty and the Market Mechanism », Quarterly Journal of Economics, vol. 84, no 3,‎ août 1970, p. 488-500 (JSTOR 1879431)\n(en) George J. Stigler, « The Theory of Economic Regulation », The Bell Journal of Economics and Management Science, vol. 2, no 1,‎ printemps 1971, p. 3-21 (JSTOR 3003160)\n(en) Anne Osborne Krueger, « The Political Economy of the Rent-Seeking Society », American Economic Review, vol. 64, no 3,‎ 1974, p. 291–303 (JSTOR 1808883)\n(en) Michael C. Jensen et William H Meckling, « Theory of the Firm : Managerial Behavior, Agency Costs and Ownership Structure », Journal of financial Economics, vol. 3, no 4,‎ octobre 1976, p. 305-360 (DOI 10.1016/0304-405X(76)90026-X, lire en ligne)\n(en) Daniel Kahneman et Amos Tversky, « Prospect Theory: An Analysis of Decision under Risk », Econometrica, vol. 47, no 2,‎ 1979, p. 263-291\n(en) N.G. Mankiw, « Real Business Cycles : A New Keynesian Perspective », Journal of Economic Perspective, vol. 3, no 3,‎ été 1989, p. 79-90 (JSTOR 1942761)\n(en) David Card, « The Impact of the Mariel Boatlift on the Miami Labor Market », Industrial and Labor Relations Review, vol. 43,‎ 1990, p. 245-257\n(en) David Card et Alan B. Krueger, « Minimum Wages and Employment : A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania », American Economic Review, vol. 84,‎ 1994, p. 772-793\n(en) John Maynard Keynes, « La Fin du laissez-faire », La pauvreté dans l'abondance, Gallimard,‎ 2002\n\n\n=== Ouvrages sur l'économie ===\n(en) Daniel M. Hausman, The Inexact and separate science of economics, Cambridge, Cambridge University Press, 1992, 372 p..\nLaville J.L & Cattani A.D (2005) Dictionnaire de l'autre économie (Vol. 123). Desclée de Brouwer.\nLaville J.L (2003) Avec Mauss et Polanyi, vers une théorie de l'économie plurielle (No. 1, p. 237-249). La Découverte.\nMichaël Biziou, Adam Smith et l'origine du libéralisme, Paris, Presses universitaires de France, 2003\nGilles Campagnolo, Critique de l'économie politique classique, Paris, Presses universitaires de France, 2004\n(en) Barry Clark, Political-economy : A comparative approach, Westport, 1998, 376 p. (ISBN 978-0-275-96370-5, lire en ligne)\n(en) Charles F. Horne, The Code of Hammurabi : Introduction, Forgotten Books, Yale University, 13 novembre 1915, 81 p. (ISBN 978-1-60506-051-4)\n(en) Samuel Noah Kramer, History Begins at Sumer, University of Pennsylvania press, 1er avril 1988 (ISBN 978-0812212761)\nDaniel Villey, Petite histoire des grandes doctrines économiques, Paris, Litec, 1985\n(en) Eli Heckscher, Mercantilism, Allen Unwin, 1955\nMax Blaug, Economic Theory in Retrospect, Cambridge University Press, 1997, 5e éd.\n(en) John B. Davis, « Heterodox Economics, the Fragmentation of the Mainstream and Embedded Individual Analysis », Future Directions in Heterodox Economics, University of Michigan Press,‎ 2006\n(en) Mary S. Morgan et Malcolm Rutherford, « American Economics : The Character of the Transformation », dans From Interwar Pluralism to Postwar Neoclassicism, Duke University Press, 1998, 325 p. (ISBN 9780822323358)\n(en) Ibrahim M. Oweiss, Ibn Khaldun, the Father of Economics, Albany, N.Y., New York University Press, 1988, 365 p. (ISBN 0-88706-698-4)\nDavid Throsby, Economics and Culture, Cambridge University Press, 2001, 5e éd.\n\n\n=== Articles sur l'économie ===\n(en) Jean David C. Boulakia, « Ibn Khaldun: A Fourteenth-Century Economist », The Journal of Political Economy, vol. 79, no 5,‎ 1971 (DOI 10.1086/259818)\n(en) L.K. Jha, « Chanakya: the pioneer economist of the world », International Journal of Social Economics, vol. 25, nos 2/3/4,‎ 1998, p. 267-282 (DOI 10.1108/03068299810193443)\n(en) Balbir S. Sihag, « Kautilya on publics goods and taxation », History of Political Economy Economics, vol. 37, no 4,‎ hiver 2005, p. 723-753 (DOI 10.1215/00182702-37-4-723)\n(en) J.R. Hicks, « M. Keynes and the Classics : A Suggested Interpretation », Econometrica, vol. 5, no 2,‎ avril 1937, p. 147-159 (JSTOR 1907242)\n(en) Ng Yew-Kwang, « Business Confidence and Depression Prevention: A mesoeconomic Perspective », American Economic Review, vol. 82, no 2,‎ mai 1992, p. 365-371 (JSTOR 2117429)\nGuillaume Quiquerez, « Essai sur la définition de la philosophie économique », Revue de philosophie économique, vol. 16, no 2,‎ 2015\n\n\n=== Ouvrages de vulgarisation ===\nRenaud Chartoire et Sophie Loiseau, L'Économie, Nathan, coll. « Repères », 2005, 159 p.\n(en) Steven Levitt et Stephen Dubner, Freakonomics : A Rogue Economist Explores the Hidden Side of Everything, William Morrow, 2005, 242 p.\nAlexandre Delaigue et Stéphane Ménia, Sexe, drogue… et économie, Pearson, 2008\nBernard Maris, Antimanuel d'économie, t. 1 : Les Fourmis, Rosny-sous-Bois, Bréal, 2003, 359 p. (ISBN 2-7495-0078-8, lire en ligne)\nBernard Maris, Antimanuel d'économie, t. 2 : Les Cigales, Rosny-sous-Bois, Bréal, 2006, 359 p. (ISBN 2-7495-0078-8, lire en ligne)\nPierre Cahuc, La Nouvelle microéconomie, Paris, Repères La découverte, 1993\nAnne Perrot, Les Nouvelles Théories du Marché du Travail, Paris, Repère La découverte, 1998\nStéphane Foucart, Comment l'économie est devenue religion, Folio, 2020\nMichael Goodwin et Dan E. Burr (trad. de l'anglais), Economix : la première histoire de l'économie en BD, Paris/impr. en Italie, Les Arènes, 2019, 4e éd., 354 p. (ISBN 978-2-7112-0118-1)\n\n\n=== Articles de vulgarisation ===\nDenis Clerc, « Nouveaux keynésiens, les chantres du salaire d'efficience », Alternatives économiques, no 168,‎ mars 1999 (résumé)\n« Robert Solow », Alternatives économiques poche, no 21,‎ 2005 (lire en ligne, consulté le 30 juin 2010)\n\n\n=== Manuels d'économie ===\nEdmund Phelps, Économie politique, Paris, Fayard, 2007\nPaul Krugman et Robin Wells, Microéconomie, De Boeck, 2009\nPaul Krugman et Maurice Obstfeld, Économie internationale, De Boeck, 1995\nPeter H. Lindert et Charles P. Kindleberger, Économie internationale, Économica, 1982\nPaul A. Samuelson et William D. Nordhaus, Economics, 2004\nJoseph E. Stiglitz et Carl E. Walsh (trad. Florence Mayer), Principes d'économie moderne, Bruxelles, De Boeck, 2004, 2e éd., 982 p. (ISBN 978-2-8041-4474-6)\n\n\n=== Encyclopédies ===\nThe New Palgrave Dictionary of Economics\nBernard Guerrien, Dictionnaire de l'analyse économique, La Découverte, 2002\n\n\n=== Articles encyclopédiques ===\n(en) Craufurd D. Goodwin, « History of economic thought », The New Palgrave,‎ 2008\n(en) R.J Aumann, « Game Theory », The New Palgrave, vol. 3,‎ 1987\n(en) Olivier Blanchard, « Neoclassical Synthesis », The New Palgrave, vol. 3,‎ 1987\n(en) Mark Blaug, « Classical Economics », The New Palgrave, vol. 1,‎ 1987 \n(en) Antonietta Campos, « Marginalist Economics », The New Palgrave, vol. 3,‎ 1987\n(en) G.C. Harcourt, « Post-Keynesian Economics », The New Palgrave, vol. 3,‎ 1987\n(en) L. Tarshis, « Keynesian Revolution », The New Palgrave, vol. 3,‎ 1987\n(en) Hal Varian, « Microeconomics », The New Palgrave, vol. 3,‎ 1987\n(en) CF Bastable et Smith Vernon, « Experimental Methods in Economics », The New Palgrave, vol. 3,‎ 1987\n(en) Ernest Mandel, « Marx, Karl Heinrich », The New Palgrave, vol. 3,‎ 1987\n(en) J.E Roemer, « Marxian Value Analysis », The New Palgrave, vol. 3,‎ 1987\n(en) Andrew Glyn, « Marxist economics », The New Palgrave, vol. 3,‎ 1987\n(en) Allan M. Feldman, « Welfare Economics », The New Palgrave, vol. 3,‎ 1987\n(en) David Friedman, « Law and Economics », The New Palgrave, vol. 3,‎ 1987\n(en) Hashem M Pesaren, « Econometrics », The New Palgrave, vol. 2,‎ 1987\n(en) Peter Groenewegen, « Division of Labour », The New Palgrave, vol. 3,‎ 1987\n(en) Gérard Debreu, « Mathematical EconomicsEconometrics », The New Palgrave, vol. 2,‎ 1987\n(en) New Palgrave édition 2008\n(en) Roger E. Backhouse, « Marginal Revolution », The New Palgrave,‎ 2008 (lire en ligne)\n(en) Mark Blaug, « The Social Sciences: Economics », The New Encyclopedia Britannica, vol. 27,‎ 2007\n(en) N.A, « Mercantilism », The New Encyclopedia Britannica, vol. 8,‎ 2007\n\n\n== Notes et références ==\n(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Economics » (voir la liste des auteurs).\nCet article est partiellement ou en totalité issu de l'article intitulé « Sciences économiques » (voir la liste des auteurs).\n\n\n=== Notes ===\n\n\n=== Références ===\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nChronologie des faits économiques\nChronologie de la pensée économique\nHistoire de la pensée économique\nThéories économiques\nOrthodoxie et hétérodoxie en économie\nListe des revues académiques en économie\nÉconomiste\nÉconomie en France\nÉconomisme\nRecherche en économie\n\n\n=== Liens externes ===\n\nRessource relative à la littérature : The Encyclopedia of Science Fiction \nRessource relative à la santé : Medical Subject Headings \nRessource relative à l'audiovisuel : France 24  \n\n« L'économie politique selon W. Stanley Jevons », 1878\nDossier « L’économie, une science sociale » sur Balises, magazine de la Bibliothèque publique d'information.\n\n Portail de l’économie   Portail des sciences humaines et sociales"
        },
        {
            "pageid": 16610768,
            "ns": 0,
            "title": "10 000 Women",
            "content": "10 000 Women est un programme organisé par Goldman Sachs et la Fondation Goldman Sachs (en) dans le but de contribuer à la croissance des économies locales en fournissant une formation commerciale, du mentorat et du réseautage, ainsi qu'un accès au capital aux femmes entrepreneures mal desservies à l'échelle mondiale. Le programme a été annoncé le 5 mars 2008 à l'université Columbia. Cette initiative est l’un des plus grands projets philanthropiques dans lesquels la banque a été impliquée. Au cours de ses premières années, le programme était dirigé par Dina Habib Powell, directrice générale de Goldman Sachs.\nLe programme se poursuit en 2022. Goldman Sachs a publié un rapport sur l'impact de la pandémie de COVID-19 sur les femmes entrepreneures du point de vue du programme 10 000 Women.\n\n\n== Processus d'attribution des prix ==\nDans le cadre de ce programme, Goldman Sachs a engagé un financement de 100 millions de dollars et a établi des partenariats avec des universités d'Europe et des États-Unis et des écoles de commerce dans des économies en développement et émergentes.\nL'organisation internationale américaine Vital Voices (en) décerne le prix 10 000 Women Entrepreneurial Achievement Award lors de son événement annuel Global Leadership Awards de 2009 à 2011. Le prix a été décerné à une diplômée du programme 10 000 Women, parrainé par Goldman Sachs. Parmi les lauréats précédents figurent Temituokpe Esisi du Nigeria (2009), Andeisha Farid d'Afghanistan (2010) et Fatema Akbari, également d'Afghanistan (2011).\n\n\n== Facilité d'opportunités pour les femmes entrepreneures ==\nEn mars 2014, la Société financière internationale (SFI) de la Banque mondiale et le programme 10 000 Women de Goldman Sachs ont lancé un programme de financement de 600 millions de dollars appelé Women Entrepreneurs Opportunity Facility pour permettre à 100 000 femmes entrepreneures des marchés émergents d'avoir accès au financement. La SFI a investi initialement 100 millions de dollars dans le programme, et la Fondation Goldman Sachs a fourni 32 millions de dollars, avec 486 millions de dollars supplémentaires attendus de la part d'investisseurs publics et privés.\n\n\n== Partenaires académiques ==\n\n\n== Notes et références ==\n\n\n== Liens externes ==\n\n10 000 Women\n Portail du commerce   Portail des femmes et du féminisme"
        },
        {
            "pageid": 15579658,
            "ns": 0,
            "title": "Absence de jalousie",
            "content": "L'absence de jalousie, également connue sous le nom de sans jalousie, est un critère de partage équitable. Il dit que, lorsque les ressources sont réparties entre des personnes ayant des droits égaux, chaque personne doit recevoir une part qui est, à ses yeux, au moins aussi bonne que la part reçue par tout autre agent. En d'autres termes, personne ne devrait ressentir d'envie .\n\n\n== Définitions générales ==\nSupposons qu'une certaine ressource est à partager entre plusieurs participants, de sorte que chaque participant \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n reçoit une part \n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle X_{i}}\n  \n. Chaque participant \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n a une relation de préférence personnelle \n  \n    \n      \n        \n          ⪰\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\succeq _{i}}\n  \n sur différentes parts possibles. La division est dite sans jalousie si pour tout \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n et \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n on a :\n\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n        \n          ⪰\n          \n            i\n          \n        \n        \n          X\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle X_{i}\\succeq _{i}X_{j}}\n  \n.\nUn autre terme pour l'absence de jalousie est l'absence d'envie.\nSi la valeur de préférence des participants est représentée par une fonction numérique \n  \n    \n      \n        \n          V\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle V_{i}}\n  \n, alors cette définition équivaut à :\n\n  \n    \n      \n        \n          V\n          \n            i\n          \n        \n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        ≥\n        \n          V\n          \n            i\n          \n        \n        (\n        \n          X\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle V_{i}(X_{i})\\geq V_{i}(X_{j})}\n  \n.\nUn autre façon de dire est  qu'un participant \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n envie le participant \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n ou est jaloux de \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n si \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n préfère le morceau de \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n à sa propre pièce, c'est-à-dire  si :\n\n  \n    \n      \n        \n          X\n          \n            i\n          \n        \n        \n          ≺\n          \n            i\n          \n        \n        \n          X\n          \n            j\n          \n        \n         \n      \n    \n    {\\displaystyle X_{i}\\prec _{i}X_{j}\\ }\n  \n et \n  \n    \n      \n         \n        \n          V\n          \n            i\n          \n        \n        (\n        \n          X\n          \n            i\n          \n        \n        )\n        <\n        \n          V\n          \n            i\n          \n        \n        (\n        \n          X\n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle \\ V_{i}(X_{i})<V_{i}(X_{j})}\n  \n.\nUn partage est dit sans envie ou sans jalousie si aucun participant ne jalouse un autre.\n\n\n== Cas particuliers ==\nLa notion d'absence d'envie a été introduite par George Gamow et Marvin Stern en 1958. Ils posent la question de savoir s'il est toujours possible de partager un gâteau (une ressource hétérogène) entre n enfants aux goûts différents, de sorte qu'aucun enfant n'en envie un autre. Pour n = 2 enfants, cela peut être fait par l'algorithme diviser et choisir (en), mais pour n > 2, le problème est beaucoup plus difficile. Voir découpe d'un  gâteau sans envie (en).\nDans le découpage du gâteau, l'absence de jalousie signifie que chaque enfant croit que sa part est au moins aussi importante que les autres parts ; dans la division des corvées (en), l'absence d'envie signifie que chaque agent estime que sa part est au moins aussi petite que toute autre part (le critère essentiel dans les deux cas est qu'aucun enfant ne souhaite échanger sa part avec un autre enfant).\nL'absence d'envie a été introduite en économique da le cadre de l'allocation des ressources par Duncan Foley en 1967. Dans ce cas, il y a plusieurs ressources homogènes plutôt qu'une seule. L'absence d'envie par elle-même est facile à atteindre en donnant simplement à chaque personne 1/ n de chaque ressource. Le défi, d'un point de vue économique, est de combiner ce partage avec l'efficacité au sens de Pareto. Le défi a d'abord été défini par David Schmeidler et Menahem Yaari. Voir division efficace sans envie (en).\nLorsque les ressources à diviser sont discrètes, l'absence d'envie peut ne pas être réalisable dès qu'il y a une seule ressource et deux personnes. Il existe différentes manières de traiter à ce problème :\n\nTransférer de l'argent entre les participants afin de compenser ceux qui obtiennent des objets les moins précieux. Cette solution est utilisée, par exemple, dans le problème de l'harmonie locative, et dans la tarification sans envie.\nPartager un petit nombre d'éléments. Cela se fait, par exemple, dans la procédure de gagnant ajustée.\nTrouver des allocations à peu près équitables ; comme décrit dans l'allocation d'articles sans envie .\nTrouver des allocations partielles sans envie aussi grandes que possible ; voir correspondance sans envie .\nUtiliser la randomisation pour trouver des allocations sans envie dans les attentes (\"ex-ante\"); voir répartition aléatoire équitable.\n\n\n== Variantes ==\nL'absence d'envie forte demande que chaque agent préfère strictement sa part aux autres parts\n.\nLa super-absence d'envie exige que chaque agent préfère strictement sa part  à 1/ n de la valeur totale, et préfère strictement 1/ n à chacune des autres parts,. La super-absence d'envie implique une forte absence d'envie qui, elle-même, implique une absence d'envie.\nL'absence d'envie en groupe (aussi appelée absence d'envie en coalition) est un renforcement de l'absence d'envie, exigeant que chaque groupe de participants ait le sentiment que la part qui lui est allouée est au moins aussi bonne que la part de tout autre groupe de même taille. Une exigence plus faible est que chaque agent individuel n'envie aucune coalition d'autres agents ; on l'appelle parfois absence d'envie stricte.\nL'absence d'envie à dominance stochastique (aussi appelée absence d'envie nécessaire) est un renforcement de l'absence d'envie dans un contexte où les agents rapportent des classements ordinaux sur les éléments. Il nécessite une absence d'envie à l'égard de toutes les évaluations additives compatibles avec le classement ordinal. En d'autres termes, chaque agent doit croire que sa part est au moins aussi bonne que la part de tout autre agent, selon l'extension d'ensemble responsive (en) de son classement ordinal des items. Une variante approximative peut être obtenue par la procédure d'allocation d'éléments à tour de rôle.\nL'absence d'envie justifiée est un affaiblissement de l'absence d'envie pour les marchés bifaces, dans lesquels les agents et les « articles » ont des préférences concernant le côté opposé, par exemple, le marché de l'appariement des élèves aux écoles. L'élève A ressent une envie justifiée envers l'élève B, si A préfère l'école attribuée à B, et en même temps, l'école attribuée à B préfère A.\nL'absence d'envie ex ante est un affaiblissement de l'absence d'envie utilisé dans le cadre de l'assignation aléatoire équitable. Dans ce cadre, chaque agent reçoit un billet de loterie sur les articles ; une allocation de loteries est dite ex-ante sans envie si aucun agent ne préfère le billet d'un autre agent, c'est-à-dire qu'aucun agent n'attribue une utilité espérée supérieure au billet d'un autre agent. Une allocation est dite ex-post sans envie si chaque résultat est sans envie. L'absence d'envie ex-post implique l'absence d'envie ex-ante, mais l'inverse n'est peut-être pas vrai.\nL'absence d'envie locale, (également appelée : l'absence d'envie en réseau ou l'absence sociale d'envie) est un affaiblissement de l'absence d'envie basé sur un réseau social. Il suppose que les gens ne connaissent que les allocations de leurs voisins dans le réseau et qu'ils ne peuvent donc envier que leurs voisins. L'absence d'envie standard est un cas particulier d'absence d'envie sociale dans lequel le réseau est le graphe complet.\nLa méta-envie-absence demande que les agents ne s'envient pas, non seulement en ce qui concerne l'allocation finale, mais aussi en ce qui concerne leurs objectifs dans le protocole. ; voir aussi découpe de gâteau équitable symétrique (en).\nLa minimisation de l'envie est un problème d'optimisation dans lequel l'objectif est de minimiser la quantité d'envie (qui peut être définie de différentes manières), même dans les cas où l'absence d'envie est impossible. Pour les variantes approximatives de l'absence d'envie utilisées lors de l'allocation d'objets indivisibles, voir allocation d'éléments sans envie (en).\n\n\n== Relations avec d'autres critères d'équité ==\n\n\n=== Implications entre la proportionnalité et l'absence d'envie ===\nLa proportionnalité (PR) et l'absence d'envie (EF) sont deux propriétés indépendantes, mais dans certains cas, l'une d'entre elles peut impliquer l'autre.\nLorsque toutes les évaluations sont des fonctions d'ensemble additives et que l'ensemble du gâteau est divisé, les implications suivantes s'appliquent :\n\nAvec deux partenaires, PR et EF sont équivalents ;\nAvec trois partenaires ou plus, EF implique PR mais pas l'inverse. Par exemple, il est possible que chacun des trois partenaires reçoive 1/3 selon son opinion subjective, mais que, selon l'opinion d'Alice, la part de Bob vaille 2/3.\nLorsque les évaluations ne sont que des fonctions additives, EF implique toujours PR, mais PR n'implique plus EF, même avec deux partenaires : il est possible que la part d'Alice vaille 1/2 à ses yeux, mais que la part de Bob vaille encore plus. Au contraire, lorsque les valorisations ne sont que superadditive, PR implique toujours EF avec deux partenaires, mais EF n'implique plus PR même avec deux partenaires : il est possible que la part d'Alice vaille 1/4 à ses yeux, mais que celle de Bob vaille encore moins. De même, lorsque le gâteau n'est pas entièrement partagé, EF n'implique plus PR. Les implications sont résumées dans le tableau suivant :\n\n\n== Voir aussi ==\nAversion à l'iniquité\nExpériences de division équitable (en), qui étudie l'importance relative de l'absence d'envie par rapport à d'autres critères d'équité.\n\n\n== Références ==\n(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Envy-Freeness » (voir la liste des auteurs).\n\n Portail des mathématiques   Portail de l’économie"
        },
        {
            "pageid": 6533746,
            "ns": 0,
            "title": "Agent judiciaire de l'État (France)",
            "content": "En France, l'agent judiciaire de l’État (AJE), dénommé agent judiciaire du Trésor public, agence judiciaire du Trésor public ou agent judiciaire du Trésor entre 1790 et 2012, détient le monopole de la défense des intérêts pécuniaires de l’État devant les juridictions judiciaires. Il agit aussi bien en demande, pour recouvrer des créances de l’État, qu’en défense, concernant des dettes réclamées à l’État.\nDepuis 1998, cette fonction est dévolue aux ministères économiques et financiers.\nDepuis le 13 Mai 2024, Clémence Olsina est agent judiciaire de l’État en tant que directrice des affaires juridiques des ministères économiques et financiers.\n\n\n== Histoire ==\n\n\n=== Agent judiciaire du Trésor public (1790) ===\nLa fonction d’agent judiciaire du Trésor public est créée à la Révolution française par l’article 3 du décret du 21 juillet – 15 août 1790, complété par le décret des 27 – 31 août 1791, avec une double fonction : d’une part, assurer la représentation unique et cohérente de l’État devant l’ordre judiciaire (« demandes et répétitions formées judiciairement contre la nation ») et, d’autre part, assurer le recouvrement de l’impôt (« créances actives du Trésor public qui donnent ou donneront lieu à des actions judiciaires »).\n\n\n=== Agence judiciaire du Trésor public (1844) ===\nDes missions d’expertise juridique au profit de l’ensemble des administrations lui sont confiées en 1844, faisant de lui une « agence judiciaire du Trésor public ».\nEn 1992, la mission de recouvrement des créances est transférée aux comptables publics et l’agent judiciaire du Trésor public perd cette mission.\n\n\n=== Intégration à la direction des affaires juridiques des ministères financiers (1998) ===\nEn 1998, l’agence judiciaire du Trésor public est incorporée à la nouvelle direction des affaires juridiques des ministères économiques et financiers et les fonctions d’agent judiciaire du Trésor sont alors dévolues au directeur des affaires juridiques.\n\n\n=== Agent judiciaire de l’État (2012) ===\nEn 2012, l’« agent judiciaire du Trésor » prend le nom d’« agent judiciaire de l’État ».\nCette nouvelle dénomination affirme le caractère interministériel et général de l’agent judiciaire qui este en justice au nom de l’État. Elle tend également à supprimer la confusion, y compris pour les juridictions, entre les missions de l’agent judiciaire et de l’administration fiscale en matière de recouvrement,.\nCe changement de dénomination ne modifie pas ses compétences.\n\n\n== Monopole de représentation judiciaire de l’État ==\n\nL’agent judiciaire de l’État tire de l’article 38 de la loi du 3 avril 1955 un monopole légal dans la représentation de l’État devant les tribunaux judiciaires pour assurer la défense de ses intérêts pécuniaires : \n« Toute action portée devant les tribunaux de l’ordre judiciaire et tendant à faire déclarer l’État créancier ou débiteur pour des causes étrangères à l’impôt et au domaine doit, sauf exception prévue par la loi, être intentée à peine de nullité par ou contre l’agent judiciaire de l’État. »\n\n— Article 38 de la loi no 55-366 du 3 avril 1955 relative au développement des crédits affectés aux dépenses du ministère des finances et des affaires économiques pour l’exercice 1955 (modifiée).\nL’agent judiciaire de l’État a compétence uniquement pour les condamnations pécuniaires à titre principal. À peine de violation de la loi, les condamnations pécuniaires accessoires ne relèvent pas de sa compétence, telles les demandes d’astreinte et de liquidation d’astreinte sans prétention financière ou les condamnations aux dépens. Ainsi, la Cour de cassation reconnaît que la représentation de l’agent judiciaire de l’État doit être exclue lorsque l’action devant le juge judiciaire n’a pas pour objet principal de faire déclarer l’État débiteur, même si une demande de condamnation à indemniser les frais exposés non compris dans les dépens est formée contre l’État. Toutefois, l’agent judiciaire de l’État est toujours recevable à former un pourvoi contre toute décision judiciaire condamnant l’État pour faute et lui assignant une charge de réparation s’il n’a pu être légalement représenté.\nSon monopole de compétence est d’ordre public, c’est-à-dire que ce moyen doit être soulevé d’office par le juge et peut l’être par toute partie à l’instance, sauf à encourir la nullité. Dès lors, l’État ne peut pas renoncer à s’en prévaloir et il ne peut pas être représenté par un autre fonctionnaire (tel le représentant de l’État dans le département) que l’agent judiciaire de l’État devant les tribunaux judiciaires, en ce qui concerne les intérêts pécuniaires de l’administration. En matière pécuniaire, aucune administration ne peut ester directement en justice, sans être représentée par l’agent judiciaire de l’État.\n\n\n=== Exceptions au monopole ===\nL’article 38 de la loi du 3 avril 1955, complété par d’autres dispositions législatives, limite le mandat légal de l’agent judiciaire de l’État, :\n\nen matière domaniale ;\nen matière fiscale ;\nen matière douanière, ;\nen matière d’expropriation pour cause d’utilité publique, sauf pour l’allocation de dommages-intérêts ou d’une indemnité, en vertu du code de l’expropriation pour cause d’utilité publique ;\nen matière d’enseignement ;  et,\nen matière de réquisition de biens ou de services.\n\n\n=== Types d’interventions ===\nL’agent judiciaire de l’État intervient, sur saisine de l’administration, citation, assignation ou avis du ministère public, dans les dossiers intéressant l’État devant les juridictions judiciaires, soit parce que son activité est contestée, soit parce que l’État demande réparation de son préjudice, soit parce qu’un agent de l’État est poursuivi en réparation pécuniaire.\n\n\n==== Actions en demande ====\nL’agent judiciaire de l’État peut se constituer partie civile :\n\npour toute infraction ayant entraîné un préjudice pour l’administration (dégradation de bien public, détournement, extorsion, favoritisme, corruption, remboursement des frais engagés par l’État à la suite d’une pollution, nuisance à l’image de l’État de la part d’un agent,) ;\npour toute de remboursement de frais occasionnés (dit « débours ») par un accident ou une agression sur un fonctionnaire (l’appel de l’agent judiciaire de l’État est alors obligatoire pour représenter l’État en tant qu’organisme social de l’agent, à peine de nullité sur le fond).\nL’agent judiciaire de l’État peut également émettre et rendre exécutoires les titres de perception nécessaires pour recouvrir les créances certaines de leur montant, liquides et exigibles de l’État, en-dehors des matières domaniale et fiscale. En 2021, l’agent judiciaire de l’État a émis 385 titres exécutoires, dont 30 pour un montant supérieur à 100 000 €.\nLe recouvrement des créances constituées à l’issue d’une procédure judiciaire en demande est opéré par le comptable public. En matière pénale, le recouvrement est assuré par le directeur départemental ou régional des finances publiques.Toutefois, en matière civile, l’agent judiciaire de l’État peut émettre un titre de perception à l’encontre du débiteur pour prise en charge par le comptable public territorialement compétent.\nEn 2020, l’État a obtenu 17 850 934 € en demande.\n\n\n==== Actions en défense ====\nSous réserve de la compétence de la juridiction administrative et de la dualité des ordres de juridiction, l’agent judiciaire de l’État peut être assigné, au nom de l’État, devant un tribunal judiciaire par un tiers :\n\nen matière d’accidents causés par les agents de l’État (la responsabilité de l’État est alors substituée à la responsabilité personnelle de l’agent) ;\nen matière de libertés publiques (tout dysfonctionnement du service public de la justice entraînant une durée excessive de jugement et susceptible de faire naître un préjudice, sur le fondement de l’article L141-1 du code de l’organisation judiciaire ; tout dommage causé par les activités de police judiciaire ; toute contestation d’une mesure d’hospitalisation sans consentement ; tout dommage résultant d’une voie de fait de l’administration, c’est-à-dire d’une exécution forcée, dans des conditions irrégulières ou insusceptibles de se rattacher à un pouvoir de l’administration, portant une atteinte grave au droit de propriété ou à une liberté fondamentale ; toute emprise irrégulière ; tout dysfonctionnement du service public de l’état civil comme le refus par l’état civil de délivrer un certificat de nationalité française ; tout dysfonctionnement du service public des tutelles ; et, toute demande en indemnisation de personnes détenues provisoirement dans le cadre d’une affaire ayant donné lieu à un non-lieu, une relaxe ou un acquittement) ;\nen matière de droit social, devant le conseil des prud’hommes et le pôle social des tribunaux judiciaires (anciennement tribunal des affaires de sécurité sociale), en ce qui concerne les agents contractuels de l’État, le paiement des cotisations sociales pour les collaborateurs occasionnels du service public de droit privé, le contentieux des maladies professionnelles et accidents du travail ainsi que la mise en cause de la responsabilité de l’État en tant qu’employeur pour faute inexcusable ;\nen matière de droit économique et financier, en ce qui concerne les contentieux nés de contrats de droit privé conclus par l’État, les contentieux de la construction lorsque l’État est maître d’œuvre, les contentieux de la propriété intellectuelle de l’État ainsi que les contentieux du droit des sociétés lorsque l’État est administrateur d’une entreprise commerciale ; et,\nen matière de contestation d’un titre de perception exécutoire, sauf si l’opposition concerne les modalités de recouvrement ou si l’opposition est portée devant le tribunal administratif.\nLa dépense consécutive à une condamnation de l’État est imputée au budget de l’ordonnateur du département ministériel concerné. En 2020, le montant total des condamnations de l’État en défense s’élève à 20 337 552 €. L’agent judiciaire de l’État n’assure, sur son propre budget, que le règlement des frais de justice, d’actes et d’instances et honoraires d’avocats et d’huissiers de justice. En 2020, le coût global des contentieux de l’agent judiciaire de l’État s’élève à 4 356 621,03 € et les dépenses auxquels l’État a été condamné s’élèvent à 32 398,07 €.\n\n\n==== Transiger au nom de l’État ====\nEn 2011, le Premier ministre a encouragé l’administration à recourir aux règlements amiables des conflits, pour éviter les procédures contentieuses devant les tribunaux. L’agent judiciaire de l’État a donc recours aux transactions, pour éviter des procédures longues, préserver l’image de l’État, favoriser une indemnité rapide des parties, assurer une gestion économe des deniers publics et décharger les juridictions.\nLorsqu’une procédure judiciaire est en cours, l’agent judiciaire de l’État est seul compétent pour transiger au nom de l’État.\nEn 2021, l’agent judiciaire de l’État a conclu 17 transactions.\n\n\n== Représentation ==\n\n\n=== Directeur des affaires juridiques, agent judiciaire de l’État ===\nLes activités de l’agent judiciaire de l’État sont gérées par la direction des affaires juridiques des ministères économiques et financiers. Depuis 1998, les fonctions d’agent judiciaire de l’État sont dévolues au directeur des affaires juridiques.\nOutre l’adjoint du directeur des affaires juridiques également agent judiciaire adjoint de l’État, d’autres agents judiciaires adjoints de l’État nommés parmi les sous-directeurs et les chefs de bureau de la direction des affaires juridiques. Les services de la direction des affaires juridiques, notamment les bureaux de la sous-direction du droit privé et pénal (droit privé général ; droit pénal et de la protection juridique ; droit de la réparation civile), assistent l’agent judiciaire de l’État.\n\n\n=== Avocats ===\nEn pratique, l’agent judiciaire de l’État est représenté par des avocats près les tribunaux judiciaires,. Nommés par le ministre chargé du budget, ils sont désignés à l’issue d’une procédure de marché public depuis 2007. Ces avocats sont soumis à des obligations particulières comme celles de ne pas plaider contre l’État et de faire l’objet d’évaluations régulières.\nEn 2021, 112 cabinets d’avocats et 200 avocats assurent la représentation de l’agent judiciaire de l’État.\n\n\n== Notes et références ==\n\n\n=== Bibliographie ===\nJocelyne Amouroux, Jean-Paul Besson et Jean-Paul Besson, Agent judiciaire de l'État, Dalloz, coll. « Dalloz corpus », 4 novembre 2020, 110 p. (ISBN 978-2-247-20404-5)\n\n\n== Lien externe ==\nSite officiel\n\n Portail du droit français   Portail de l’économie"
        },
        {
            "pageid": 12343849,
            "ns": 0,
            "title": "Agent représentatif (économie)",
            "content": "Un agent représentatif est, en science économique, un agent économique considéré comme moyen.  \n\n\n== Concept ==\nD'un point de vue plus technique, on dit d'un modèle économique qu'il a un agent représentatif si tous les agents du même type (individu, firme, etc.) sont identiques. Les économistes disent également qu'un modèle économique a un agent représentatif si les agents sont différents mais que la somme de leurs choix est mathématiquement équivalente à la décision d'un agent ou de plusieurs agents identiques. Un modèle avec plusieurs agents différents dont les choix ne peuvent pas être agrégés est appelé modèle d'agents hétérogènes.\nLa notion d'agent représentatif apparaît au 19e siècle. Francis Edgeworth (1881) utilisa le terme \"particulier représentatif\" et Alfred Marshall introduisit la notion de \"firme représentative\" dans Principles of Economics (1890). Toutefois, après la critique de Lucas à propos de la prévision économétrique des effets d'une politique économique, le développement de la micro fondation a rendu la notion d'agent représentatif à la fois plus importante et plus controversée. La plupart des modèles macroéconomiques sont aujourd'hui caractérisés par un problème d'optimisation utilisant l'hypothèse d'agent représentatif pour le consommateur ou le producteur (même si les deux sont fréquemment présents dans ces modèles). Les courbes d'offre et de demande de ces agents sont ensuite interprétées comme étant l'offre et la demande agrégées de ces deux types d'agents. \n\n\n== Utilisation de l'hypothèse ==\nL'agent représentatif est utilisé par souci de simplicité. Un modèle qui considère que tous les agents de l'économie sont identiques est plus facilement calibrable qu'un modèle qui prenne en compte une infinité d'agents différents. Une étude qui viserait à étudier la diversité des comportements des agents, ou une étude qui se concentrerait sur l'hétérogénéité radicale des agents, ne doit donc pas recourir à l'hypothèse de l'agent représentatif au risque de trouver des résultats faux.\nAinsi, une modélisation qui viserait à déterminer les effets moyens d'une hausse du prix du pétrole peuvent légitimement recourir à des agents représentatifs, car ce n'est pas la réaction des agents dans leur individualité qui est testée. En revanche, la théorie des enchères nécessite de la relâcher puisque le point important est que les acheteurs peuvent accorder une valeur différente à un même bien en fonction de leur propre subjectivité.\nRobert E. Lucas montre en 1976 que les recommandations politiques basées sur les relations macroéconomiques passées peuvent négliger des changements de comportement individuel importants qui, une fois rassemblés, pourraient modifier ces mêmes relations. On pourrait alors régler ce problème en détaillant de manière explicite la situation dans laquelle l'agent prend sa décision. Dans un modèle incluant cette précision, un économiste pourrait alors analyser les effets d'une certaine politique économique en calculant le problème de décision de chaque agent lorsque cette politique est mise en place puis en regroupant les solutions de ces problèmes afin d'avoir une vision d'ensemble des changements macroéconomiques.\nCet argument a poussé de nombreux économistes à construire des modèles micro-fondés (c'est-à-dire basés sur la microéconomie). Cette méthode est cependant plus complexe que les méthodes de modélisation précédentes. Ainsi, la plupart des  modèles macroéconomiques d'équilibre général furent simplifiés en faisant l'hypothèse d'agent représentatif. Les modèles d'équilibre général avec plusieurs agents hétérogènes sont bien plus compliqués et restent donc une branche de la recherche économique encore peu explorée.\n\n\n== Critique ==\nL'utilisation de l'agent représentatif est critiquée par divers économistes. Dans un papier publié en 1992, Kirman se montre critique et souligne que les modèles qui utilisent cette hypothèse rejettent trop simplement de vrais problèmes liés à l'agrégation des décisions, et commettent donc un sophisme de composition. Il décrit par exemple une situation dans laquelle l'agent représentatif est en désaccord avec tous les individus dans l'économie. Les recommandations politiques visant à augmenter le bien-être de l'agent représentatif seraient alors illégitimes. Kirman en conclut que la réduction d'un groupe d'agents hétérogènes à un agent représentatif ne serait pas seulement une facilité analytique mais serait « injustifiée et mènerait à des conclusions qui sont trompeuses et souvent fausses ». Selon lui, l'agent représentatif « mérite d'être enterré dignement, en tant qu'approche économique primitive, en plus d'être fausse ».\nPlusieurs solutions sont avancées afin de ne plus utiliser l'agent représentatif dans les modèles économiques. Une solution de rechange serait le cours à des systèmes dynamiques basés sur les interactions d'agents, qui sont capables de gérer l'hétérogénéité de ces derniers. Une autre possibilité pourrait être les modèles macroéconomiques d'équilibre général avec des agents hétérogènes, ce qui est compliqué mais de plus en plus utilisé (Ríos-Rull, 1995; Heathcote, Storesletten, et Violante 2009; Canova 2007 section 2.1.2).\nUn papier de recherche publié par Jackson et Yariv en 2017 montre que l'hypothèse d'agent représentatif n'est pas compatible avec les fonctions d'utilité usuellement utilisées et que les modèles macroéconomiques \"standards\" ne sont donc pas micro-fondés. \n\n\n== Voir aussi ==\nHomo œconomicus\nDemande agrégée\nIndividualisme méthodologique\n\n\n== Bibliographie ==\nEdgeworth, Francis (1881), Mathematical Psychics: An Essay on the Application of Mathematics to the Moral Sciences, C.K. Paul & Co\nMarshall, Alfred (1890), Principles of economics\nLucas, Robert E. (1976), 'Econometric policy evaluation: A critique' dans K. Brunner et A. H. Meltzer, The Phillips Curve and Labor Markets, Vol. 1 of Carnegie-Rochester Conference Series on Public Policy, pp. 19–46, Amsterdam: North-Holland\nHartley, James E. (1997), The Representative Agent in Macroeconomics, London, New York: Routledge,  (ISBN 0-415-14669-0)\nKirman, Alan P. (1992): Whom or what does the representative individual represent? Journal of Economic Perspectives 6: 117–136\nRíos-Rull, José-Víctor (1995): 'Models with heterogeneous agents', Chapter 4 in T. Cooley (ed.) Frontiers of Business Cycle Theory, Princeton University Press\nJonathan Heathcote, Kjetil Storesletten, et Giovanni L. Violante (2009), 'Quantitative Macroeconomics with Heterogeneous Households', Annual Review of Economics 1, 319–354\nFabio Canova (2007), Methods for Applied Macroeconomic Research. Princeton University Press\nJackson, Matthew O. et Yariv, Leeat (2017), The Non-Existence of Representative Agents, DOI 10.2139/ssrn.2684776\n\n\n== Références ==\n(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Representative agent » (voir la liste des auteurs).\n Portail de l’économie"
        },
        {
            "pageid": 12822473,
            "ns": 0,
            "title": "Agriculture numérique",
            "content": "L’agriculture numérique se définit comme la convergence de l’agriculture et des technologies de l’information (capteurs, réseaux intelligents, outils de la science de la données, application, voire automatisme et robotique) pour, tout au long de la chaîne de valeurs, améliorer la productivité et répondre aux attentes environnementales et sociétales. Ainsi, le numérique en agriculture mobilise de nombreuses technologies mises en œuvre seules ou en combinaison pour proposer des solutions à destination des agriculteurs.\nL’agriculture numérique est donc le fruit d’une convergence entre une succession de révolutions technologiques et une profonde nécessité de changement face aux nouveaux enjeux de l’agriculture moderne : remise en question de l’usage des produits phytosanitaires, enjeux alimentaires locaux - avec l’alimentation de proximité - et mondiaux, sécurité alimentaire tout au long des filières, transition agroécologique, etc.\n\n\n== Histoire ==\nL’émergence du numérique en agriculture date des années 1970-1980. Au début des années 1970, les programmes de satellites d’observation de la Terre (ERTS puis LANDSAT aux États-Unis) utilisaient des satellites conçus notamment pour répondre aux besoins du département de l’agriculture. Le début des années 1980 a été marqué par l’accès aux systèmes informatiques pour le grand public et le développement de la « télématique » (avec le Minitel et la météo en application-phare pour l’agriculture).\nEn 1983, apparaissent les premiers logiciels conçus pour l’agriculture (comptabilité agricole et gestion de parcelles) qui initient la transition vers le numérique.\nLes images satellites sont d’abord utilisées pour des objectifs d’étude des sols, de cartographie de l’usage des terres, grâce à la création de l’indice NDVI, élaboré en 1973. L’imagerie satellitaire est rapidement associée aux Systèmes d’information géographique (SIG) développés à la fin des années 1960 et qui permettent de superposer plusieurs couches de données sur une carte, données collectées par d’autres moyens (collecte manuelle ou par des capteurs embarqués) et géo-référencées grâce au système GPS, ouvert aux applications civiles dans les années 1980.[réf. nécessaire]\n\n\n=== L’apparition de l’agriculture de précision ===\nCes différents dispositifs, souvent combinés, donnent naissance à l’agriculture de précision (également appelée « gestion intra-parcellaire »), qui voit le jour dans les années 85-90, avec les premières cartes de rendement (1980). À la différence de l’agriculture conventionnelle, dont l’unité de gestion est la parcelle, l’agriculture de précision consiste à répondre aux besoins des plantes à une échelle sub-parcellaire, en déterminant un zonage, chaque zone ayant des besoins spécifiques.\nEn production animale, l’équivalent existe (élevage de précision : la gestion des animaux s’individualise par rapport à une gestion plus uniforme faite au niveau du troupeau).\nAinsi, l’agriculture de précision se construit sur un cycle « observation – diagnostic – préconisation – application ».\nL’agriculture de précision est rendue possible grâce à l'essor des capteurs numériques, leur coût étant par ailleurs devenu plus abordable (c’est la première phase, l’observation). Le rendement est la première unité mesurée. L’une des premières applications de l’agriculture de précision en productions végétales, a été l’intégration de capteurs de quantité récoltée (pesée, volume) dans une moissonneuse-batteuse, pour en déduire un rendement géo-référencé. Côté élevage, les robots de traite apparaissent en 1995 - après la présentation en 1985 du premier robot de traite français, proposé par le Cemagref (aujourd’hui Irstea) - et enregistrent les données de production (volumes produits, puis qualité).\n\n\n=== De l’agriculture de précision à l’agriculture numérique ===\nAu-delà de l’agriculture de précision, l’agriculture numérique est un concept qui apparaît au milieu des années 2010 (rapport agriculture innovation 2025).\nL’agriculture numérique dépasse l’agriculture de précision, concentrée sur la phase opérationnelle de production et la gestion intra-parcellaire ou intra-troupeau et ceci de 2 manières. D’une part, du fait de la multiplication des technologies d’acquisition et d’échange de données, d’autre part du fait des nouveaux services numériques de mise en relation, dont s’emparent les agriculteurs pour s’affranchir des intermédiaires du secteur (commercialisation, formation, échange de savoir et de matériel, locations entre pairs…).[réf. nécessaire]\nAinsi, en agriculture numérique, les TIC sont mises en œuvre à toutes les échelles de la production agricole et de son écosystème :\n\nau niveau de l’exploitation (optimisation des opérations culturales, de la conduite de troupeaux…)\ndans les services d’accompagnement (nouveaux services de conseil agricole basés sur des données collectées automatiquement),\nà des échelles plus larges comme dans un territoire (gestion de l’eau) ou dans une chaîne de valeur (traçabilité, amélioration des intrants comme les semences, meilleure adéquation entre la production et le marché…)\n\n\n== Les enjeux ==\n\n\n=== Traçabilité agricole ===\nLes technologies développées dans le cadre de l'agriculture numérique peuvent former des systèmes de traçabilité agricole numérique, qui permettent aux parties prenantes de suivre les produits agroalimentaires en temps réel. La traçabilité numérique offre un certain nombre d'avantages :\n\nRéduction du gaspillage alimentaire : en 2011 sur toutes les calories alimentaires produites en Europe, 11 % sont gaspillées entre la production à la ferme et la réception auprès des consommateurs. Les systèmes de traçabilité facilitent une meilleure identification des dysfonctionnements du côté de l'offre, où la nourriture est perdue en aval de l'exploitation et gaspillée. Les innovations numériques émergentes, telles que les cartons de lait qui suivent le lait de « la ferme au réfrigérateur », peuvent répondre au gaspillage du côté de la demande en fournissant aux consommateurs des dates de péremption plus précises.\nConfiance des consommateurs : garantir la sécurité, la qualité et l'authenticité des aliments est devenu une exigence réglementaire importante dans les pays à revenu élevé. L'utilisation d'étiquettes RFID pour certifier les caractéristiques des produits agroalimentaires pourrait fournir des signaux de qualité en temps réel aux consommateurs.\nAmélioration du bien-être des producteurs : les producteurs qui peuvent tirer parti de la certification environnementale pourraient vendre leurs produits à un prix supérieur, car les technologies de la chaîne de production pourraient permettre une plus grande confiance dans les labels tels que « durable », « biologique » ou « commerce équitable ».\n\n\n=== Enjeux alimentaires ===\nLa FAO estime que le monde devra produire 56 % de nourriture en plus (par rapport à 2010, dans le cadre d'une croissance \"business as usual\") pour nourrir plus de 9 milliards de personnes en 2050. En outre, le monde est confronté à des défis tels que la malnutrition, le changement climatique, le gaspillage alimentaire et l'évolution des régimes alimentaires. Pour produire un « avenir alimentaire durable », le monde doit augmenter la production alimentaire tout en réduisant les émissions de gaz à effet de serre et en maintenant (ou en réduisant) les terres utilisées pour l'agriculture. L'agriculture numérique pourrait relever ces défis en rendant la chaîne de valeur agricole plus efficace, équitable et durable sur le plan environnemental.\n\n\n=== Enjeux agronomiques ===\nLe numérique, au sens large, impacte le secteur de la production agricole[Interprétation personnelle ?]. Au travers des différentes technologies mises en œuvre, il permet à l’agriculture de gagner en efficience et de la transformer en profondeur.[Interprétation personnelle ?]\nLes masses de données produites par les équipements d’observation, les capteurs installés dans les parcelles agricoles, au sein des troupeaux ou dans les bâtiments permettent une meilleure caractérisation des systèmes agricoles. Leur exploitation à travers des systèmes d’aide à la décision doit permettre de mieux optimiser le pilotage des systèmes de production actuels.\nCes masses de données contribuent également au développement de nouveaux systèmes de production, nécessaire à la transition agroécologique, notamment la diversification des cultures, la transformation des pratiques culturales et l’apparition de nouvelles organisations de travail, tant au niveau des agriculteurs que des décideurs.[réf. nécessaire]\nElles permettent d’améliorer les connaissances du fonctionnement de la plante et des animaux et par conséquent l’amélioration des modèles pour faire des prévisions, anticiper les risques, améliorer la politique de gestion agricole des décideurs ou encore de renforcer les systèmes d’alerte précoce à la sécurité alimentaire.[réf. nécessaire]\nEn particulier, une meilleure caractérisation de l’état sanitaire des cultures peut permettre de réduire l’utilisation des produits phytosanitaires, en optimisant les dates et les doses de traitement et répondre ainsi aux enjeux du plan ECOPHYTO.\nÀ travers la caractérisation comportementale et physiologique des animaux, il est possible[Interprétation personnelle ?] de caractériser de manière plus objective leur état de santé et leur bien-être et ainsi de proposer des systèmes de production pour prendre en compte ce critère. Cela permettrait aussi d’offrir une véritable transparence sur ces aspects à destination des consommateurs.\nPar ailleurs, l’utilisation du potentiel de la télédétection dans la cartographie des zones de pâturages offre[Interprétation personnelle ?] un moyen de médiation pour limiter pour les conflits entre éleveurs et agriculteurs, souvent fréquents dans les pays africains.\n\n\n=== Enjeux économiques ===\nL’agriculture numérique offre la possibilité d’intégrer les avancées technologiques dans l’agriculture, dans le but de diminuer les coûts de production et la quantité d’intrants (eau, énergie, engrais, pesticides, etc.), participant à l’amélioration de la compétitivité de toute la chaîne de production.\nLes outils décisionnels construits à partir des données collectées permettent de contrôler et d’optimiser la quantité des intrants, en accord avec les récentes orientations législatives et environnementales de réduction des produits phytosanitaires.[réf. nécessaire]\nPar exemple, des outils de suivi des opérations culturales permettent ainsi déjà de mesurer précisément la consommation de carburant et ainsi mieux prendre en compte ces éléments dans le raisonnement de cette consommation. En mobilisant les technologies d’autoguidage basées sur le GPS RTK, l’agriculteur peut aussi nettement réduire sa consommation en optimisant son parcours au centimètre près.\nDans le domaine de l’économie collaborative, des plates-formes proposant des places de « marchés numériques » permettent aux producteurs de se rapprocher des consommateurs finaux. Des nouvelles formes de circuits courts pourraient ainsi se développer. Différents acteurs se positionnent déjà sur ce marché. Cela peut également concerner l’échange ou la location de matériel entre agriculteurs.\n\n\n=== Enjeux environnementaux ===\nL’un des principaux enjeux de l’agriculture numérique est d’optimiser le rendement de différents processus agricoles en utilisant le moins possible d'énergie et d'intrants, réduisant ainsi leur empreinte écologique. Dans cet objectif, l’agriculture numérique rejoint l’approche de l'agriculture de précision. L’utilisation de technologies (e.g. drones, capteurs, etc.) permet d’obtenir des indicateurs précis et réguliers sur une exploitation, tels que la vitesse du vent, la pluviométrie ou la nature des sols. Cette connaissance approfondie de l’environnement vise à adapter les pratiques agricoles à la spécificité du milieu de l’exploitation, de l’échelle parcellaire à intra-parcellaire.\nCela peut par exemple faciliter le contrôle de l’utilisation de produits phytosanitaires, la meilleure évaluation du besoin en engrais et en irrigation ou encore la maîtrise du régime alimentaire des animaux, afin de réduire la quantité d’azote rejetée dans les sols.\nL’agriculture numérique est un outil permettant de promouvoir la durabilité et le recyclage du matériel utilisé dans les exploitations par la réduction de l’obsolescence programmée.[réf. nécessaire]\n\n\n=== Renforcer le lien entre agriculteurs et citoyens ===\nLes outils numériques ont un rôle important à jouer dans le renforcement du lien de confiance entre consommateur et agriculteur. Ils accompagnent déjà l’essor massif des circuits courts (utilisés par un agriculteur sur 5 en 2010) via des plateformes qui mettent en relation directe producteur et consommateur. Au-delà de l’acte de vente, ces supports permettent à l’agriculteur de communiquer sur ses méthodes et ses produits, et au consommateur d’accéder aux informations sur la provenance de son alimentation.\nLes réseaux sociaux, sites et blogs, sont de formidables outils de diffusion des connaissances agricoles. En 2016, 82% des agriculteurs visionnaient des vidéos agricoles, dont 22% au moins une fois par jour. Créé en 2014, le site Agriculteurs d’aujourd’hui rassemble et trie les vidéos du web traitant de sujets agricoles, à destination des professionnels comme du grand public. L’offre de sites Internet destinés aux agriculteurs est aujourd’hui grandissante et propose un large éventail de services: de l’achat de fournitures agricoles à la location de matériel entre agriculteurs.[réf. nécessaire]\nL’espace numérique a également permis l’émergence de plateformes de financement participatif destinées à l’agriculture. Ces plateformes sont des alternatives au financement bancaire traditionnel. Elles offrent l’opportunité aux internautes d’investir dans des projets locaux et qui ont du sens.[réf. nécessaire]\nLa plus connue de ces plateformes, Miimosa, a ainsi collecté 12 millions d’euros et financé plus de 2000 projets depuis sa création.\n\n\n=== Enjeux sociaux ===\nL’intégration d’outils numériques dans un système agricole permet à l’agriculteur de diminuer la pénibilité et le temps demandé pour certaines tâches : utilisation de scans pour la création d’inventaires phytosanitaires automatiques, gestion automatisée des tâches de semis et de récolte grâce à la robotique, système de déclenchement d’arrosage à distance, etc.\nDes outils numériques permettent par ailleurs une réduction de la charge cognitive de l’agriculteur, grâce à des applications de gestion automatisée de la logistique et de systèmes d’alerte de risques climatiques, de dysfonctionnements techniques ou d’erreurs humaines : des applications résument les informations importantes pour le suivi des cultures, des capteurs météorologiques peuvent prévenir d’un futur gel, des capteurs intégrés sur le matériel détectent des pannes à venir, l’analyse de données issues de thermomètres connectés à des animaux aident à prédire leur futur vêlage ou maladie, etc.\nCes éléments vont permettre à l’agriculteur une amélioration de ses conditions physiques et mentales, une diminution de son stress et une augmentation de son temps libre, contribuant à son bien-être et à la qualité de ses relations familiales et sociales.\n\n\n== Acteurs et usages ==\nL'agriculture numérique s'adresse à tout type d'agriculture. Les usages sont spécifiques aux besoins aux filières et aux utilisateurs.\nIls varient selon les filières :\n\n\n=== En cultures végétales ===\nen grandes cultures : guidage, télédétection, modulation intra-parcellaire, etc\nen viticulture : modèles de prédiction des maladies, pilotage de la pulvérisation, etc\nen arboriculture et maraichage (fruits et légumes) : logiciels de gestions technico-économiques, robots de désherbage\n\n\n=== En productions animales ===\nen élevage bovin (laitier et viande) : robots de traite, capteurs de chaleur, etc\nen élevage monogastrique (lapins, porcins, volailles...) : capteurs de CO2 et de température, etc\nen élevage caprin et ovin : distributeurs automatiques de concentrés, systèmes de géolocalisation, etc\nd'autres filières comme l'aquaculture ou l'apiculture sont également concernées\nAu sein de chaque filière, différents utilisateurs sont amenés à utiliser des outils et services de l'agriculture numérique :\n\nChefs d'exploitation pour le pilotage des cultures, des élevages et de l'exploitation\nOuvriers agricoles pour l'observation et l'application au champ\nConseillers agricoles pour l'observation et la réalisation des préconisations\nCUMA / ETA pour la gestion du matériel agricole et du personnel\nStructures collectives (Coopératives, chambres d'agriculture, négoce, etc) pour la gestion de la production à l'Échelle d'un territoire\nStructures d'essais, laboratoires de recherche (voir Ecosystèmes de l’innovation) pour la validation de modèles ou expérimentations variétales\n\n\n=== Impacts sur les exploitations et les compétences de l'utilisateur ===\nL'arrivée des outils numériques engendre des aspects positifs et négatifs pour les exploitations.\nLes utilisateurs finaux confirment un gain en précision et en efficacité[réf. nécessaire], ainsi qu’une meilleure anticipation des différentes tâches à accomplir, en fonction des données collectées et analysées par les outils.\nDe plus, une fois maîtrisés, ces outils permettent une augmentation de la traçabilité, de la qualité des produits, du confort de travail, du respect des réglementations et de la simplification globale de la gestion de l'exploitation.[réf. nécessaire]\nIls permettent ainsi de moderniser l'image d'une exploitation et parfois de lui redonner un nouveau souffle.[réf. nécessaire]\nSi le prix des équipements et services est trop élevé[Interprétation personnelle ?] pour certains agriculteurs, ces équipements apportent néanmoins un gain significatif sur les coûts des charges (produits, intrants et alimentation) et de main-d’œuvre.\nLa connaissance de l'animal, la prévention des risques sanitaires et le respect de l'environnement sont, par la même occasion, améliorés.\nCependant, le manque de communication, l'éloignement entre les entreprises, les acteurs de l'innovation et les utilisateurs finaux conduisent à un manque d'informations sur le marché disponible et sur les possibilités offertes par les outils numériques.[réf. nécessaire]\nPar ailleurs, les utilisateurs se sentent submergés par la vague d'offres en outils et services.[Interprétation personnelle ?] Des craintes sur la complexité (manque d'interopérabilité entre les outils), la fiabilité et la dépendance aux outils numériques sont également apparues.[réf. nécessaire]\nDe plus, avec l'apparition de ces nouveaux outils, les utilisateurs ont besoin d'acquérir de nouvelles compétences pour leur déploiement et leur utilisation, compétences pour lesquelles ils pourront être formés auprès des entreprises (et/ou des collectivités) qui leur fournissent le matériel.[réf. nécessaire]\nMais plus que d'une simple formation, les utilisateurs font remonter un besoin de conseils personnalisés et d'un accompagnement pour le déploiement de ces outils sur leur exploitation.[réf. nécessaire]\n\n\n=== Services et outils numériques pour l’agriculture ===\nLe développement de services et outils numériques a considérablement favorisé l’essor de nouvelles sources d’informations en agriculture[réf. nécessaire]. L’ensemble des services/outils numériques aident à recueillir les données pour apporter de l’information qui permettent par la suite de conseiller ou d’orienter l’action ou le traitement agricole (irrigation, traitement pesticide, etc.). L’acquisition de ces informations est possible grâce à l’utilisation d’objets connectés (stations météo, smartphones) et de capteurs (imagerie, Lidar, etc.). Ces technologies permettent d’obtenir des informations sur le suivi de cultures (maladies, demande évaporative) ou de cheptel.\nParallèlement, les vecteurs mobiles (drone, tracteur, moissonneuse, robot…) sont des technologies qui s’améliorent permettant d’embarquer de plus en plus de capteurs (imagerie, Lidar, GPS RTK). Ce développement rend accessibles des informations à la fois précises et géolocalisées.\nLes services de type imagerie satellitaire (Sentinel), généralement développés pour d’autres secteurs d’activités, permettent le suivi de cultures à l’échelle d’un champ. L’utilisation de données mutualisées à travers les réseaux sociaux ou des applications smartphones permettent d’étudier l’information de manière régionalisée.\n\n\n=== La réglementation des données ===\nCompte tenu des enjeux économiques importants autour de la donnée agricole, et en l’absence de réglementation sur la maîtrise de l’usage des données en France, le contrat est la seule alternative permettant de rétablir les rapports de force dans les relations commerciales entre les acteurs de l’agriculture numérique.\nPlusieurs initiatives proposent des recommandations visant à favoriser les échanges de données, tout en les encadrant. Certains sont également allés plus loin en proposant de faire exister un cadre autour de la donnée, il s’agit de soft law. Par conséquent, elle ne lie juridiquement que ceux qui s’y soumettent, la contrainte est d’ordre social et économique. On a différents exemples de chartes établies dans le secteur agricole aux États-Unis, en France et en Europe.\nLes chartes sur les données agricoles ont plusieurs points communs :\n\nElles s’inscrivent dans une démarche volontaire d’auto-réglementation ;\nElles sont fondées sur des principes (résultats des pratiques en matière de données agricoles plutôt que sur le processus par lesquels cela doit être réalisé) ;\nElles ont été préparées par une combinaison d’acteurs (associations ou syndicats d’agriculteurs, de fournisseurs de technologies agricoles, de machines et d’intrants) ;\nElles s’articulent autour de trois points communs fondamentaux: le consentement, la divulgation et la transparence.\nL’adoption des chartes relatives aux données agricoles n’est pas encore assez large pour évaluer leur succès jusqu’à présent. On peut néanmoins souligner certains aspects positifs clés des chartes : (1) Elles renforcent la confiance. (2) Elles comblent des lacunes juridiques. (3) Elles simplifient l’évaluation des comportements (surtout quand elles sont accompagnées d’une certaine forme de certification). (4) Elles sensibilisent (tant les fournisseurs de technologies que les agriculteurs). (5) Elles favorisent la participation et l’inclusion (élaborées conjointement par différentes organisations représentant les parties prenantes concernées ; cela à son tour favorise la confiance et accroît la crédibilité).\n\n\n== Les technologies numériques en agriculture et leurs fonctions ==\nLes technologies mobilisées en agriculture numérique sont nombreuses et diverses. Elles regroupent les technologies de l’information et de la communication (TICs) au sens large avec notamment : les objets connectés et l’Internet des objets (IoT), les capteurs embarqués, les technologies satellitaires ou aéroportées (drones, ULM), les applications smartphones, les réseaux sociaux, ou encore les robots. Le numérique – ou ‘digital’ – renvoie au traitement de l’information via des opérations logiques. Ainsi, les technologies numériques permettent l’acquisition, le stockage, l’échange, la gestion des données de l’agriculture ainsi que leur traitement, analyse et modélisation. Par l’intermédiaire de ces processus, les TICs rendent possible la production et la mise en circulation d’informations entre acteurs du monde agricole : agriculteurs, conseillers, formateurs, entreprises, consommateurs etc.\n\n\n=== Outils et méthodes utilisés en agriculture numérique ===\n\n\n==== Acquisition des données ====\nLes données sont principalement acquises par deux moyens, soit de manière automatique, soit de manière plus ou moins manuelle. L’automatisation de l’acquisition des données met en œuvre des capteurs, qu’ils soient portatifs, statiques (station météo connectée, radar de pluie, piège connecté, quantité/ qualité du lait, etc.), implantés ou portés pour le suivi des animaux (capteurs de chaleurs, de monitoring santé etc des ovins-bovins)  ou embarqués sur matériel agricole (détection des mauvaises herbes, estimation du besoin d’azote…), sur des vecteurs aériens de type avion et drone ou satellite. Ils délivrent un signal ou une image qui sont spécifiques du phénomène observé ou au contraire qui sont très génériques (température, humidité, pression etc.), et qui – pour atteindre la grandeur recherchée - nourrissent un modèle spécifique. L’alternative à l’automatisation de la collecte est la saisie manuelle. Le smartphone est un dispositif très intéressant et de plus en plus utilisé car bien fourni en capteurs (appareil photo – voir application PixFruit pour déterminer le rendement des manguiers, géolocalisation, accéléromètre…) et facile à utiliser en tant que terminal de saisie par les agriculteurs (observation des maladies ou des ravageurs au champ, suivi de maturité, suivi de contrainte hydrique comme l’application Apex Vigne pour déterminer la contrainte hydrique de la vigne etc.).\nIl faut noter que les données acquises pour l’agriculture et en agriculture sont de plus en plus nombreuses. Le déploiement des objets connectés - équipés de capteurs et géo-localisés qui communiquent les données collectées via internet - et de l’internet des objets, ou IoT pour « Internet of things » émerge au début des années 2010. C’est - avec les images satellitaires et le phénotypage haut débit - un levier fort de cette massification. Le développement des objets connectés touche tous les secteurs avec selon le Digiworld institute - une forte attractivité de 3 domaines (utilities, automobile, et électronique grand public) – et 36 milliards d’objets connectés dans le monde en 2030 (Digiworld Yearbook 2017). En agriculture, des objets comme un smartphone, tracteur, un outil agricole tracté, une station météo, un piège à insectes etc., délivreront en temps réel les informations utiles à la gestion. Cette massification permettra rapidement de constituer des méga-données – ou Big Data – pouvant être analysées pour fournir à l’agriculteur des outils d’aide à la décision.\n\n\n==== Stockage et transfert ====\nLes données et informations acquises ou observées peuvent être reportées et échangées via des plateformes (réseaux sociaux, logiciel, etc.) disponibles sur smartphone ou tablette. Les échanges, stockages et transferts des données sont de plus en plus complexes du fait du volume croissant des données, de leur hétérogénéité (différentes sources), de leur complexité (différents référentiels) et des questions liées à la confidentialité. Cependant, la donnée seule n’est pas intéressante en soi mais c’est sa compilation avec un grand nombre de données et son traitement qui peut en faire ressortir de l’information. D’où le besoin de créer des plateformes pour faciliter ces échanges et rendre ces données accessibles. Un tel portail ouvrant les données agricoles favoriserait l’innovation ouverte (rapport Bournigal, 2017). En France comme ailleurs, des infrastructures et des entreprises émergent pour faciliter les échanges de données agricoles et encourager l’innovation ouverte à partir de ces données: en Australie avec AgReFed, aux Pays-Bas avec JoinData, en France avec API-Agro.\n\n\n==== Traitement, analyse des données et aide à la décision ====\nLes données peuvent être à la fois utilisées pour l’étalonnage, l’ajustement et la validation des modèles existants et pour la création des modèles représentant les phénomènes agronomiques mais aussi économiques, climatiques etc. La nouveauté est le caractère plus en plus massif des données, desquelles on infère des informations et des modèles en utilisant des méthodes mathématiques classiques (régression, classification, etc.), l’intelligence artificielle, et si les données sont trop abondantes, les méthodes de traitement du Big data. Dans ces approches d’inférence de modèles, qui s’opposent à la construction mécaniste des modèles, l’enjeu est alors de trouver des règles qui ont un sens du point de vue de l’homme de l’art. Ceci n’est pas trivial car avec ces méthodes, de type « boite noire », les relations entre paramètres ne sont pas explicites. L’analyse des données peut servir plusieurs fonctions.\nEn 2017, le think tank Villa numeris a organisé une Vision Camp #ImagineAgri, consacrée à une réflexion prospective sur les enjeux et besoins de l'« agriculture connectée », et à proposer des solutions. Cela a notamment débouché sur la création d'un portail de données agricoles (intégré dans le programme « Agriculture-innovation 2025 ») et préparé par un rapport remis par Jean-Marc Bournigal (Irstea) en janvier 2017 à Stéphane Le Foll (ministre de l'Agriculture), et à Axelle Lemaire) (secrétaire d'État au numérique et à l'innovation),.\nDes applications comme PlantVillage et Virtual Agronomist permettent à des agriculteurs, avec peu de moyens, de faire des diagnostics de maladies ou de besoins en intrants.\n\n\n==== Échanges et communications ====\nLe dernier maillon de la chaîne de traitement des données est la communication et l’échange des informations, connaissances, recommandations ou décisions tirées de l’analyse. Les modèles inférés, par les statistiques ou l’intelligence artificielle, ne sont pas appropriables directement par les acteurs (agriculteurs, commerçants, etc.), les 2 écueils étant trop de variables à renseigner et des sorties incompréhensibles. Les informations peuvent être transmises sous forme de recommandations (ex : date de traitement), d’indicateurs bruts (ex : température moyenne) ou de distribution (ex : cartographie). La transmission de ces informations peut se faire au travers d’outils d’aide à la décision (logiciels, application, alerte sms, etc..) qui alimentent des systèmes automatiques (ex : pilotage automatique de l’irrigation) ou qui sont soumis à l’agriculteur pour qu’il prenne lui-même la décision par rapport à la recommandation. L’aide à la décision est la fonction la plus régulièrement citée et la plus mise en œuvre par les entreprises quand il s’agit de l’analyse de données en agriculture. Cependant le résultat de cette analyse peut être autre chose qu’une aide à la décision. Cela peut servir à de l’évaluation a posteriori, à de la réflexion, à une prise de recul. Cela pourrait être utilisé pour de la capitalisation de connaissances, à de nouvelles formes d’expérimentations agronomiques.\nPar ailleurs les transferts de données ne sont pas forcément associés à de l’analyse. Des données ‘brutes’ peuvent être transmises, on pense notamment à tout ce qui est traçabilité.\nOn notera toutefois que la variété des formes d’échanges d’informations en agriculture ne saurait se réduire aux complexes processus d’acquisition, de traitement et de transmission de données numériques décrits jusqu’à présent. En effet, ces échanges peuvent aussi s’opérer suivant des circuits de communication plus directs. C’est le cas notamment des échanges entre agriculteurs, qui au moyen de leurs smartphones produisent eux-mêmes des informations sur leurs activités et les partagent à un public plus ou moins ouvert sur les réseaux sociaux. Les technologies utilisées sont alors beaucoup plus ordinaires (smartphones équipés de caméra) et les données transférées sous forme de captations audiovisuelles sont le plus souvent montées et accompagnées par un discours (texte et/ou prise de parole) qui participe de les rendre intelligibles. Ainsi, les technologies numériques génériques peuvent servir à échanger des connaissances mais aussi de biens et de services.\n\n\n=== Domaines d’application des technologies ===\nDepuis le milieu des années 2010, de nombreuses start-up ont été créées dans le domaine de l’agriculture numérique. Il s’agit principalement de sociétés de service, qui produisent un conseil à partir de données collectées dans les champs, qui connectent les agriculteurs et des tiers (agriculteurs, consommateurs…) pour vendre/ acheter des produits et services ou qui proposent des services et des technologies disruptives (robots). Ils sont regroupés dans une association, La Ferme Digitale. Au-delà des technologies proposées par ces start-up, des technologies numériques, allant du simple site Internet au tracteur connecté, sont aussi mises sur le marché par des entreprises d’agrofournitures ainsi que par des organisations professionnelles agricoles, des associations, des organisations publiques ou parapubliques. Le numérique en agriculture comprend aussi le numérique non spécifique à l’agriculture mais utilisé dans ce domaine, avec des technologies proposées alors par les acteurs classiques du numérique.\n\n\n==== Production végétale ou animale ====\nDe nombreuses technologies numériques sont développées pour la production végétale et animale en tant que telle. Certaines technologies permettent de collecter des données pour connaitre l’état de l’environnement et des productions (plantes, animaux) : sont concernés les sols (hygrométrie, texture, teneur en azote etc.), les plantes (stress hydrique, maladies, croissance), l’environnement (météo, qualité de l’eau, qualité de l’air), les animaux (détection des chaleurs, santé, alimentation etc.). Par exemple, plusieurs entreprises proposent des stations météo connectées équipées de pluviomètre, d’anémomètre, de thermomètre, d’hygromètre etc.\nLa donnée collectée peut être traitée pour fournir des indicateurs, des préconisations, ou pour automatiquement régler le fonctionnement de l’outil. Ainsi, une des utilisations est l’agriculture de précision où l’apport des intrants (produits phytosanitaires, engrais, irrigation, semences, alimentation animale, traitements pharmaceutiques) est piloté - en quantité, en qualité - selon des caractéristiques spatiales et temporelles mesurées.\nPour ce qui concerne la production au sens strict, il y a également tous les outils de robotique et d’automatisation : robot de traite, distributeur automatique d’aliment, robot de désherbage etc., les outils de géolocalisation, etc.\n\n\n==== Recherche et développement ====\nLe numérique en agriculture peut être utilisé pour la recherche et le développement, apportant de nouvelles connaissances mais aussi sûrement de nouvelles manières de faire et d’organiser la recherche et le développement. Les données collectées sur les parcelles, en conditions réelles, peuvent servir à alimenter des thématiques de recherches. Par ailleurs, on constate le développement des sciences participatives, notamment sur tout ce qui concerne la biodiversité et l’environnement. Un domaine de recherche particulièrement touché par le développement du numérique est la génétique.\nSélection variétale accélérée par le phénotypage haut-débit\nLe phénotypage haut-débit est un verrou technologique à lever pour accélérer la sélection de variétés plus adaptées aux nouvelles conditions de climat et de marché et moins consommatrices en intrants. L’objectif est de caractériser des collections de génotypes de plantes en fonction de leur réponse à divers scénarios environnementaux associés aux changements climatiques. Tout l’enjeu est de faciliter l’identification de gènes d’intérêt agronomique afin de sélectionner les plantes pour des systèmes de culture innovants, à bas niveau d’intrants, ou de mieux bénéficier de leur diversité génétique. Les technologies haut-débit ont ici un potentiel important et viennent bousculer les manières de faire de la sélection variétale.\n\n\n==== Outils d’information et de formation ====\nNe l’oublions pas, le numérique est avant tout un outil qui permet de transférer de l’information. Et avoir accès à de l’information en agriculture est déjà très utile et parfois compliqué. Le numérique peut faciliter cet accès-là, que ce soit pour des données météo, des données techniques, réglementaires ou économiques. Le développement du secteur de la formation digitale impacte aussi le secteur agricole. Grâce à Internet, à la vidéo et aux simulateurs 3D, de nouvelles offres de formations se développent. On peut citer Icosystème, une plateforme numérique de formation en ligne en agro-écologie, Ver de Terre Production ou encore Le Mas Numérique et sa visite virtuelle.\n\n\n==== Outils de communication et de collaboration entre acteurs du monde agricole ====\nAvec Internet et les NTIC, le numérique permet de créer facilement la mise en relation entre producteurs, acteurs du monde agricole, ou encore consommateurs.\nParmi les outils de mise en relation entre agriculteurs et acteurs du monde agricole (prestataires, fournisseurs), citons des exemples comme LinkinFarm (plateforme numérique de mise en relation entre agriculteurs et prestataires de travaux agricoles), Wefarmup.com ou VotreMachine.com (sites de location de matériel agricole), ou encore Agriconomie, un site internet qui permet (i) aux agriculteurs de trouver tout ce dont ils ont besoin pour leur exploitation au même endroit et au meilleur prix, afin de leur faire économiser du temps et de l’argent ; (ii) aux distributeurs et fournisseurs d’étendre leur périmètre géographique de ventes et de proposer leurs offres à un plus grand nombre d’agriculteurs en Europe, à moindre coût. Au-delà de ces services marchands, les NTIC peuvent servir à communiquer et coopérer pour la gouvernance des structures collectives, à co-construire et diffuser des connaissances, à se coordonner entre acteurs, à mettre en œuvre des projets etc. Une liste exhaustive ne peut être réalisée, les potentiels de coopération via les NTIC étant multiples. Le numérique peut permettre de mettre en relations les agriculteurs entre eux et notamment permettre le partage de connaissances, d’expériences et de services entre agriculteurs.\n\n\n==== La valorisation des productions ====\nLes technologies numériques en agriculture servent aussi à la valorisation de la production.\nLes NTIC se développent pour des échanges entre les agriculteurs et le reste de la société. Elles sont également utilisées pour vendre ou mieux vendre sa production. Plusieurs exemples : Connecting Food, un site Internet qui permet une transparence pour les consommateurs des processus de production de leur alimentation, Les Grappes qui mettent en relation vignerons et consommateurs, Miimosa, une plateforme de financement participatif de projets agricoles, Panier Local, un outil qui propose la gestion de l'ensemble des tâches liées à la commercialisation (prise de commande, stock, préparation, livraison, facturation, etc), ou encore Poiscailles, un site qui permet d’acheter en circuit court des produits de la mer. Sur un autre registre, le site Comparateur Agricole est une place de marché en ligne pour la vente de céréales à la tonne ou l’application Captain Farmer fournit une aide pour vendre au meilleur prix sur les marchés.\nPour valoriser la production, les outils de traçabilité numérique se sont développés et se complexifient avec notamment la technologie de la blockchain.\n\n\n==== Traçabilité - Gestion globale- Logistique ====\nEnfin, le numérique est et sera de plus en plus utilisé pour la gestion globale de l’exploitation agricole. Notamment, les exigences réglementaires de traçabilité sont fortes – concernant les apports d’intrant par exemple – et le temps passé par les agriculteurs pour les respecter est important, de 5 à 10 h par semaine, pour l’ensemble des tâches administratives. Le numérique permet d’automatiser certaines entrées de données de traçabilité, de les rassembler, les organiser, les sauvegarder. Cependant, les nouveaux outils numériques requièrent souvent de la saisie manuelle et de la manutention, ce qui peut amener de nouvelles tâches à effectuer (et donc du temps passé en plus). Ainsi, numérique et gain de temps ne sont pas toujours associés.\nAu-delà du respect des exigences réglementaires, le numérique peut servir à rassembler de la donnée afin de gérer son parcellaire et ses rotations, de gérer son parc matériel, ses stocks, d’organiser la main-d’œuvre, la logistique et les différentes tâches (ex du système Keyfield qui facilite la saisie d’opérations agricoles avec un scanner RFID ou le logiciel Agreo de Smag, cahier de culture électronique). Avoir une traçabilité des tâches effectuées permet de constituer un capital informationnel, de calculer des coûts de production et d’apprendre à partir des années passées.\nLe numérique est aussi là pour diminuer la charge mentale des agriculteurs, leur niveau de stress : le succès du GPS a largement été dû au fait que les agriculteurs pouvaient relâcher leur concentration lors des labours ou autres tâches répétitives. Les systèmes d’alerte (chaleurs ou de vêlage pour les vaches, fuites d’eau, portes ouvertes…) facilitent la gestion et permettent de maîtriser les gros risques. C’est la promesse de sociétés comme Ekylibre. Cependant, les effets induits par l’utilisation de ces technologies ne correspondent pas toujours aux effets attendus. Par exemple, l’utilisation de ces technologies numériques en élevage amène parfois à un stress et une charge mentale plus importants pour les éleveurs.\nL’anticipation et la gestion des risques est l’un est des domaines avec lesquels le numérique est attendu.\nIl ne faut pas oublier non plus le numérique qui sert pour les services bancaires, la comptabilité, les déclarations administratives, les demandes d’aide.\nGlobalement, les outils numériques peuvent intervenir dans différentes fonctions et à différentes échelles, allant de la plante au territoire. Ils peuvent être utilisés directement par les exploitants, mais également par d’autres acteurs du secteur, que ce soient les conseillers, les fournisseurs, le secteur aval, etc. Un des points de vigilance est que les effets promis par ceux qui proposent ces technologies peuvent être différents des effets constatés lors de l’usage en agriculture. Où peuvent avoir des effets différents selon le contexte dans lequel la technologie est utilisée. Ainsi, l’utilisation du numérique doit être raisonnée en fonction d’un ensemble de paramètres tels que les objectifs recherchés, le contexte socio-économique et environnemental, les compétences mobilisables, etc. De plus, le numérique en agriculture a de nombreuses potentialités, mais ne peut pas être la solution à tout type de problème et en toutes circonstances. Le numérique apporte un ensemble d’outils, mobilisables parmi d’autres types d’outils. L’articulation entre technologies, numériques ou non, est un élément important à considérer pour un raisonnement global et cohérent des systèmes agricoles.\n\n\n== Écosystème de l'innovation ==\n\n\n=== Recherche & Enseignement ===\n\n\n==== En France ====\nAu niveau français, tous les grands Instituts agronomiques sont impliqués dans l’agriculture numérique : l'Inra, le Cirad, Acta - les instituts techniques, ou encore Irstea. Afin de réunir différents acteurs de la recherche en agriculture numérique, l’Institut Convergences Agriculture Numérique #DigitAg a été créé en 2017. Il réunit 17 partenaires publics et privés et a pour vocation de créer un socle de connaissance pour permettre le déploiement de l’agriculture numérique en France et dans le monde. La chaire AgroTIC est l’une chaire d’entreprises permettant l’association d’établissements d’enseignement et de recherche (Montpellier SupAgro, Bordeaux Sciences Agro et Irstea) avec des acteurs socio-économiques dans le domaine de l’agriculture numérique. Montpellier SupAgro, l’ESA d’Angers et Bordeaux Sciences Agro proposent des formations en agriculture numérique\n\n\n==== A l’international ====\nEn Hollande, l’Université de Wageningen s’engage également dans la recherche en agriculture de précision et numérique, à l’instar de l’Université de Californie Davis aux États-Unis. En Australie, de nombreux acteurs sont impliqués dans l’agriculture numérique comme les universités du Queensland, Griffith, Curtin, le CSIRO ou encore Food Agility.\n\n\n=== Les entreprises et startups ===\nDepuis quelques années, on assiste à l’émergence d’un nouvel écosystème de startups œuvrant dans l’AgTech, marqué par des levées de fonds dépassant la centaine de millions d’euros, comme avec l’entreprise Ynsect (125 millions d’euros en 2019). Selon le media AgFunder news, ces levées ont augmenté de 550 % sur les six dernières années, ce qui est un indicateur important du dynamisme du secteur.\nCes startups sont accompagnées par des incubateurs spécialisés dans les thématiques agricoles. Aux États-Unis, les plus gros d’entre eux se situent en Californie (Terra, PNP) ou plus proche des centres traditionnels de production comme the Yield Lab (St Louis, Missouri). En France, Euratechnologies a une antenne consacrée à l’AgTech, et des réseaux d’accélérateurs comme Le Village by CA permettent à des startups de se développer à proximité des agriculteurs. En France, certaines de celles-ci s’organisent en association comme la Ferme Digitale afin de promouvoir l’innovation et le numérique dans l’agriculture, par exemple à travers des événements comme les LFDays.\nToujours en France, la plupart des acteurs privés de l’agriculture numérique sont regroupés au sein de la Chaire AgroTIC.L'agriculture numérique a suscité l'intérêt d'entreprises historiques de différents domaines tels que l'électronique, le développement de logiciels et les producteurs de produits agricoles. Par exemple, les grands acteurs du marché de la sélection et de la gestion des cultures, tels que Bayer et Syngenta, ou bien des agroéquipementiers comme John Deere  ont commencé à fournir des services numériques aux agriculteurs. Leur rôle est important par leur capacité à intégrer des entreprises innovantes dans leur cœur de métier (rachat de Climate corporation par Monsanto/Bayer, rachat de Blue river Technology par John Deere).\nCertaines entreprises du numérique commencent également à s’investir dans l’agriculture numérique, identifiée comme un levier de croissance potentiel. Microsoft a lancé l’initiative “AI for Earth” ainsi que Farm Beats avec pour but de développer l’utilisation de l’intelligence artificielle pour le développement durable et l’agriculture. Enfin, Bosch développe aussi des capteurs connectés pour l’agriculture.\n\n\n=== Living Lab ===\nPlusieurs Living labs se sont développés ces dernières années pour promouvoir le développement de l'agriculture numérique.\nIl y a par exemple l'Institut BioSense en Serbie, le Yeesal Agrihub à Thiès (Sénégal) ou encore le projet OccitaNum qui vise à faire de l’Occitanie le leader de l’agriculture et de l’alimentation de demain en mobilisant les technologies numériques dans une approche d’innovation ouverte.\n\n\n== Risques et controverses ==\nL’agriculture numérique est un domaine qui évolue très rapidement, tout comme les multiples controverses qui y sont liées. Pour illustration, les 24es Controverses européennes de Bergerac, organisées en juillet 2018 se sont interrogées sur “L’agriculture augmentée : quels impacts sociaux et économiques ?”.\nSont énumérées ci-dessous les principales controverses :\n\n\n=== Automatisation du travail sur l’exploitation agricole ===\n\n\n=== Individuation des traitements sur les plantes et les animaux ===\n\n\n=== Acquisition des connaissances sur l’exploitation ===\n\n\n=== Gestion de données ===\n\n\n=== Un secteur en constante évolution ===\n\n\n=== Impact environnemental ===\n\n\n=== Agriculture numérique dans les pays en développement ===\n\n\n== Références ==\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nAgriculture\nAgriculture de précision\nAgriculture durable\nTransition écologique\nAgroécologie\nTechnologies de l'information et de la communication\nTransformation numérique\n Portail de l’agriculture et l’agronomie"
        },
        {
            "pageid": 9831690,
            "ns": 0,
            "title": "Ancrage territorial des entreprises",
            "content": "L’ancrage territorial des entreprises désigne l'utilisation par une entreprise de son lien avec le territoire sur lequel elle opère. L'expression renvoie aux interactions entre le milieu et la performance économique. Il s'agit d'un concept d'économie territoriale, de géographie économique et de sciences de gestion.\n\n\n== Concept ==\nL'ancrage territorial est « le processus et le résultat d’interactions entre entreprise et territoire, fondés sur la création collective de ressources communes, spécifiques et localisées, permettant une longue période de sédentarité d’une entreprise ». Cette définition, issue des sciences de gestion, est inspirée des approches développées en économie et en géographie,,.\nLes constituants essentiels de l'ancrage territorial sont :\n\nl'existence d'une action coopérative, permettant notamment des phénomènes d'apprentissage,\nl'absence de droit de propriété sur une partie des ressources issue de cette action collective,\nle rattachement de ces ressources à un territoire, qui fait que l'entreprise en sera privée si elle se délocalise,\nle temps long dans lequel s'inscrit cette coopération; l'ancrage demeurant toutefois réversible.\nL'ancrage territorial des entreprises est un enjeu politique et économique fort. Il apparaît comme un objectif majeur de la gouvernance des territoires et des politiques publiques locales. L'ancrage d'entreprises dans un territoire est en effet une manne d'emplois via des créations endogènes d'emplois. \nDes exemples d'entreprises ancrées sont constitués par des entreprises implantées dans des pôles de compétitivité, ou ayant une production située dans une appellation d'origine protégée,. Il est toutefois possible que des entreprises soient ancrées sans être rattachées à des institutions de ce type. \nLa seule présence d'une entreprise dans un territoire ne permet pas de l'ancrer à proprement parler. Il est nécessaire pour ce faire que l'entreprise bénéficie de ressources locales, entre dans une dynamique de coopération avec les institutions publiques locales et le réseau entrepreneurial local, etc.. À l'inverse, le fait qu'une entreprise garde des relations socialisées fortes en dehors du territoire n'interdit pas que celle-ci puisse être ancrée.\n\n\n== Fondements ==\nLes fondements de l'ancrage territorial des entreprises sont au nombre de cinq. \nLe premier est la proximité. L'ancrage territorial est un objet de l'économie locale. À ce titre, la proximité géographique entre l'entreprise et les acteurs du territoire (politiques, institutions, autres entreprises) est essentielle. Toutefois la seule proximité géographique ne suffit pas à permettre un ancrage à proprement parler. Il est nécessaire pour les entreprises d'entretenir une proximité institutionnelle et organisationnelle afin de la favoriser.\nLe deuxième est l'utilisation de réseaux par l'entreprise. Les travaux sur les réseaux contribuent à expliquer la possibilité de l'émergence de l'ancrage dès lors qu'ils sont localisés. Le concept d’embeddedness est lié à celui d'ancrage, dans le sens où l'entreprise n'est plus conçue comme étant au-dessus du territoire, mais comme en étant une partie intégrante.\nLe troisième fondement est constitué des externalités. Les ressources collectivement créées par ancrage, et démunies de droits de propriété, peuvent être considérées comme des externalités. Tous les travaux en économie sur les externalités contribuent à comprendre la nature des ressources issues de l'ancrage, leur diffusion et leur non-transférabilité,. Ces externalités sont généralement considérées comme positives mais peuvent aussi scléroser certaines entreprises ancrées.\nLe quatrième est le territoire. Bien que ce concept soit multiforme, il est très souvent mobilisé dans l'étude de l'ancrage. Les travaux sur les districts, sur les clusters, sur les milieux innovateurs (travaux du GREMI, sur le développement des territoires) permettent en partie de comprendre la dynamique de l'ancrage dans sa dimension exogène.\nLe cinquième fondement de cet ancrage territorial est la stratégie générale d'entreprise. En sciences de gestion, l'ancrage est souvent abordé comme constituant une stratégie de territorialisation,.\n\n\n== Ancrage et RSE ==\nLes recherches récentes sur la responsabilité sociale des entreprises ont mobilisé le concept d'ancrage territorial des entreprises afin de déployer l'arsenal conceptuel de la RSE dans ce cadre. La norme ISO 26000, relative à la responsabilité sociétale des organisations, précise que « l’ancrage territorial est le travail de proximité proactif d’une organisation vis-à-vis de la communauté. Il vise à prévenir et à résoudre les problèmes, à favoriser les partenariats avec des organisations et des parties prenantes locales et à avoir un comportement citoyen vis-à-vis de la communauté ». \nEn juillet 2018, la Plateforme RSE a publié un avis intitulé « Vers une responsabilité territoriale des entreprises », dans lequel elle analyse les stratégies d’ancrage territorial des entreprises, les formes de coopération, les instruments de mesure ainsi que les freins existants.\n\n\n== Notes et références ==\n\n Portail des entreprises   Portail de l’économie"
        },
        {
            "pageid": 6024426,
            "ns": 0,
            "title": "Baromètre mondial de l'espoir économique",
            "content": "Le Baromètre mondial de l'espoir économique est établi et publié par l'Institut Gallup International. \n\n\n== Modalités d'établissement ==\nIl résulte d'enquêtes menées auprès d'échantillons représentatifs de la population de chaque pays (entre 500 et 2700 personnes par échantillon) et réalisées en face-à-face, par téléphone ou via internet. \n\n\n== Résultats 2011 ==\n(publiés dans les Journaux «Le Parisien»  et «la Croix» )\nL'Enquête définit un « indice d'espoir économique » mesurant le solde entre optimistes et pessimistes;\nD'une manière générale, les pays émergents affichent un réel optimisme alors que l'Europe a une vision sombre de l'avenir.\n\n\n=== Synthèse par continents ===\nDu point de vue économique, l'Asie et l'Afrique se rangent parmi les optimistes tandis que l'Europe et l'Amérique du Nord sont pessimistes.\n\n\n=== Les dix pays les plus optimistes ===\nLe Nigéria, Le Vietnam, le Ghana, l'Ouzbékistan, le Soudan du Sud, la Tunisie, le Cameroun, l'Azerbaïdjan, la Colombie, l'Irak\n\n\n=== Les dix pays les plus pessimistes ===\nLa France, L'Irlande, L'Autriche, la Belgique, la Bosnie-Herzégovine, la Serbie, l'Ukraine, l'Espagne, l'Allemagne, Hong Kong.\n\n\n== Notes et références ==\n\n Portail de l’économie   Portail de la société"
        },
        {
            "pageid": 14309457,
            "ns": 0,
            "title": "Capitalisme partenaire",
            "content": "Les partisans du capitalisme partenaire affirment que c'est une approche holistique du capitalisme qui cherche à répondre aux besoins de toutes les parties prenantes : les actionnaires tout comme les clients, les employés, les partenaires de distribution, les fournisseurs, les vendeurs, les communautés locales et l'environnement. Au-delà de cela, le terme manque d'une définition convenue. Les deux définitions existantes ont une différence subtile mais significative qui a un impact sur le débat croissant sur le sujet (Voir Étymologie.) Investopedia définit le capitalisme partenaire comme « un système dans lequel les entreprises sont orientées pour servir les intérêts de toutes leurs parties prenantes. Parmi les principales parties prenantes figurent les clients, les fournisseurs, les employés, les actionnaires et les communautés locales. » Selon l'Enterprise Engagement Alliance, une organisation commerciale, « le capitalisme partenaire cherche à créer des rendements pour les actionnaires uniquement en créant de la valeur pour la société - clients, employés, fournisseurs, communautés et environnement ». Le sujet du capitalisme partenaire a reçu une couverture à la fois favorable et défavorable dans des publications telles que le New York Times, le magazine Fortune, Barrons, le Wall Street Journal et bien d'autres publications depuis 2019.\n\n\n== Étymologie ==\nLes partisans du capitalisme partenaire s'accordent sur le fait qu'il appelle à une société capitaliste plus humaniste qui met l'accent sur l'importance de répondre aux besoins de toutes les parties prenantes plutôt que de se concentrer uniquement sur les actionnaires. Selon l'organisation, les parties prenantes comprennent les employés, les clients, les partenaires de distribution, les fournisseurs et les communautés, toute personne dont les intérêts sont affectés par l'organisation. Parce que le concept a effectivement jailli des activités des hommes d'affaires plutôt que des établissements d'enseignement et n'est pas enseigné dans les écoles, l'absence d'une définition claire a conduit à des controverses précoces reflétées dans les débats décrits ci-dessous.\nL'une des premières références au concept est faite dans le Manifeste de Davos publié en 1973 par Klaus Schwab et le Forum économique mondial: «Le but de la gestion professionnelle est de servir les clients, les actionnaires, les travailleurs et les employés, ainsi que les sociétés, et d'harmoniser les différents intérêts des parties prenantes.» Le concept a essuyé le feu à la fois du côté gauche et du côté droit du spectre politique, comme indiqué ci-dessous.\nLa plus grande partie de l’opposition au concept est fondée sur la définition du Manifeste de Davos et d’Investopedia, parce qu’il semble que cela change la responsabilité d’une organisation qui ne répond plus aux besoins des actionnaires — contrairement aux principes attribués à l’économiste Milton Friedman — en essayant de répondre et d’équilibrer également les besoins de tous les intervenants, c.-à-d. les employés, les clients, les partenaires de distribution, les fournisseurs et les collectivités. Certains croient que les opinions de Milton Friedman sont plus nuancées que ce que l’on reconnaît souvent. Le professeur Alex Edmans soutient que:\n«Friedman n’a jamais préconisé que les entreprises exploitent les parties prenantes. Il a soutenu qu’il est légitime pour une entreprise de se concentrer sur l’augmentation des profits parce que la seule façon de le faire, du moins à long terme, est de traiter les intervenants au sérieux.» Il a écrit: «Il se peut fort bien qu’il soit dans l’intérêt à long terme d’une entreprise qui est un employeur important dans une petite collectivité locale de consacrer des ressources à fournir des commodités à cette collectivité ou à améliorer son gouvernement. Cela pourrait faciliter l’attraction d’employés désirables.».\nD’autres opposants, comme la Heritage Foundation, soutiennent que le capitalisme partenaire détourne les profits des actionnaires. Andrew Olivastro écrit:\n«Les actionnaires se font voler. Vous possédez littéralement l’entreprise. C’est votre propriété, sous forme de capital, qui a rendu possible la production de produits et services utiles et maintenant disponible sur le marché pour les clients. Et ce capital est maintenant détourné vers des causes et des fins que vous n’avez jamais eu l’intention de soutenir.».\nLes partisans affirment que, selon les principes du capitalisme des parties prenantes, les organisations divulguent de manière transparente leur but, leurs valeurs et leurs objectifs, afin que les actionnaires puissent décider si ceux-ci correspondent à leurs propres intérêts. D'autres soutiennent qu'il est de la seule obligation des personnes qui mobilisent des capitaux auprès des actionnaires de se concentrer sur leurs intérêts et que, de toute façon, il est impossible de gérer une organisation basée sur l'équilibre des besoins de toutes les parties prenantes,.\nCertains pensent que c'est déjà le cas avec le capitalisme actionnarial. Dans une interview de 2021 avec le New York Times, McDonalds Corp. Le PDG Chris Kempczinski, a déclaré à propos du système de capitalisme des actionnaires: «Cela finit toujours par être un exercice d'équilibre. Comment satisfaire de nombreux constituants différents, et le faire d'une manière qui améliore en définitive la marque?» \nDu côté gauche de l'échiquier politique américain, l'opposition au capitalisme partenaire est basée sur une opposition globale à la propriété privée. Max B. Sawicky dans le magazine Jacobin écrit: «Le rêve d'un 'capitalisme progressiste' ne restera qu'un rêve, ses horizons toujours strictement limités par la propriété privée capitaliste. \" Notez que les défenseurs du capitalisme partenaire considèrent l'actionnariat salarié comme un moyen important d'engager les travailleurs.».\nLa définition alternative du capitalisme partenaire à celle posée par Investopedia ne répond pas aux préoccupations des organisations contre la propriété privée. Cette définition ne remet pas non plus en cause la responsabilité de l'organisation envers ses actionnaires, mais se concentre plutôt sur ce qui est considéré comme le meilleur moyen de répondre aux intérêts des actionnaires sur le long terme. Basée sur des recherches considérables ainsi que sur le bon sens, ses partisans affirment que la définition alternative est fondée sur la prémisse que les actionnaires obtiendront un meilleur retour sur investissement dans la durée dans les organisations avec une approche stratégique et systématique pour répondre aux besoins de toutes les parties prenantes. Ils soutiennent que ces organisations ont une méthode plus durable pour réaliser des profits plus élevés d'une manière plus harmonieuse avec les besoins de la société. L'objectif de l'organisation basée sur le capitalisme partenaire reste le même que sous le capitalisme des actionnaires ; ce qui change, c'est la manière dont l'objectif est atteint et les avantages qui en résultent pour toutes les parties prenantes, et pas seulement pour les actionnaires. Voir: Stakeholder Capitalism: A Primer.\n\n\n== Histoire ==\nLa première référence formelle au terme « Stakeholder Engagement » que l'on trouve sur Internet remonte au Manifeste de Davos de 1973. Le terme n'a pas gagné beaucoup de terrain dans les médias d'affaires jusqu'en 2019, lorsque l'organisation Business Roundtable des grandes entreprises a changé sa définition de la gouvernance d'entreprise pour se concentrer non seulement sur les besoins des actionnaires mais sur toutes les parties prenantes. L'utilisation du terme a été popularisée par l'[Quoi ?] partir de 2019 et par B-Lab qui certifie les organisations dans des pratiques durables[réf. nécessaire]. Un autre partisan des principes généraux est une organisation fondée par John Mackey de Whole Foods connue sous le nom de Capitalisme Conscient.\nLe concept d'obtenir des résultats meilleurs et plus durables grâce à une approche stratégique et systématique pour répondre aux besoins de toutes les parties prenantes est le produit de nombreux penseurs organisationnels remontant aux années 1980. Cela inclut le travail d' Edward Deming, qui a écrit sur l'importance de répondre aux besoins de tous les clients, qu'il a définis comme des parties prenantes internes et externes. Des principes similaires ont été avancés au cours de cette période par l'auteur et consultant Peter Drucker, les professeurs de Harvard Leonard Schlesinger et James Heskett, la société de conseil Peppers & Rogers, et plus tard des consultants de la Gallup Organization. Le Forum for People Performance Management and Measurement de la Medill School of Journalism, Media, and Integrated Marketing Communications, a mené des recherches considérables sur le lien entre l'engagement des employés et des clients et les résultats financiers au début des années 2000. Sur la base de cette recherche et d'autres, l'Enterprise Engagement Alliance a été fondée en 2008 pour promouvoir les avantages d'avoir une approche stratégique et systématique pour impliquer toutes les parties prenantes, y compris la création d'un indice boursier expérimental des « entreprises engagées » ainsi qu'un processus pratique pour la mise en œuvre des principes du capitalisme des parties prenantes connus sous le nom d' Engagement d'Entreprise.\nEn 2015, l' Organisation internationale de normalisation (ISO) a publié de nouveaux principes de gestion de la qualité mis à jour pour inclure la nécessité de prendre en compte les intérêts de toutes les parties prenantes. Il a également publié de nouvelles exigences de l'annexe SL applicables à 60 normes de processus métier ISO qui exigent des organisations certifiées ISO qu'elles démontrent une approche stratégique et systématique dirigée par le PDG pour répondre aux besoins de toutes les « parties intéressées ». En 2015, le comité technique ISO/TC 176 a créé la première certification pour la gestion de la qualité des personnes, connue sous le nom de lignes directrices ISO 10018 pour l'engagement des personnes. DTE Energy, une société d'énergie publique basée à Detroit, MI, a obtenu la première certification ISO 10018. La même année, un article du Huffington Post intitulé « Stakeholder Capitalist Model » plaide pour « un modèle de capitalisme - le capitalisme partenaire - qui prend en compte les besoins, les valeurs et les exigences culturelles de chacune des parties prenantes est essentiel au succès, traction et maximisation des rendements et de l'impact planétaire. \".\nAu fil des ans, un certain nombre de chefs d'entreprise ont adopté les principes du capitalisme partenaire sans utiliser le terme, notamment Herb Kelleher, cofondateur de Southwest Airlines, John Mackay de Whole Foods, Hubert Joly de Best Buy, Kenneth Frazier de Merck, Paul Tudor Jones, l'investisseur de fonds spéculatifs dont l'organisation JUST Capital utilise le terme, avec Laurence Fink de Blackrock, et bien d'autres.\nEn 2019, l'Enterprise Engagement Alliance a publié « Enterprise Engagement for CE0s: The Little Blue Book for People Centric Capitalists », qui donne un aperçu du processus de mise en œuvre des principes du capitalisme des parties prenantes. En août 2020, Forbes a publié un article de Bruce Bolger et Alex Edmans, en janvier 2021, intitulé « Can Stakeholder Capitalism Change the World : First We Must Define it, donnant une définition formelle du capitalisme des parties prenantes.». Le Forum économique mondial a publié « Stakeholder Capitalism Metrics », un ensemble de mesures volontaires que les entreprises peuvent utiliser dans les rapports de développement durable. En 2021, Wiley a publié un livre de Klaus Schwab du Forum économique mondial intitulé \"Stakeholder Capitalism\".\nLe concept de capitalisme partenaire a gagné du terrain ces dernières années, de concert avec le nombre croissant d'investisseurs intéressés par les questions environnementales, sociales et de gouvernance, y compris les coalitions de fonds de pension telles que la Human Capital Management Coalition. À l'automne 2020, la Securities & Exchange Commission des États-Unis a publié de nouvelles règles exigeant que les sociétés cotées en bourse rendent compte des méthodologies et des mesures du capital humain dans la mesure où elles sont importantes pour les activités d'une organisation. En 2021, la Securities & Exchange Commission a annoncé qu'elle entamerait le processus de révision des exigences de divulgation du capital humain. L'actuel président de la SEC, Gary Gensler, a déclaré qu'il croyait que les règles de divulgation mises à jour devraient inclure des « mesures spécifiques, telles que le roulement de la main-d'œuvre, les compétences et la formation au développement, la rémunération, les avantages sociaux, la démographie de la main-d'œuvre, y compris la diversité, et la santé et la sécurité, y compris la diversité de planches.\".\nLes mouvements de gestion du capital humain et de diversité, d'équité et d'inclusion et les exigences de divulgation à venir incitent de plus en plus d'organisations à considérer toutes leurs parties prenantes comme une occasion de faire grossir le gâteau, et qu'il y a des avantages économiques concrets à engager la communauté de talents la plus large possible, clients, chaîne d'approvisionnement et partenaires de distribution,.\n\n\n== Justification économique ==\nLe concept de capitalisme des parties prenantes n'est pas né de théoriciens mais de chefs d'entreprise à la recherche d'un moyen plus efficace et durable d'obtenir des résultats organisationnels. Alors que les investisseurs et le marché boursier récompensent souvent, mais pas toujours, les entreprises pour les comportements qui génèrent des rendements à court terme, comme la réduction des coûts pour atteindre des objectifs de profit à court terme ou le rachat d'actions en l'absence d'autres stratégies de création de valeur, un certain nombre de les chefs d'entreprise et les investisseurs sont arrivés à la conclusion qu'une approche stratégique et systématique pour répondre aux besoins de toutes les parties prenantes donne des résultats plus durables. Voir ces études et articles pour des exemples de retour sur investissement des principes du capitalisme des parties prenantes.\n\nHeskett, Jones, Loveman et Sasser, Jr., « Putting the Service Value Profit Chain to Work », Harvard Business Review,‎ juillet 2008 (lire en ligne, consulté le 2 juillet 2021)\nEntre 2009 et 2014, l'indice iShares MSCI USA ESG Select (symbole KLD), un fonds négocié en bourse qui suit les sociétés qui, selon lui, respectent des normes « environnementales, sociales et de gouvernance » élevées, a généré un rendement annualisé de 2,3 %, contre 1,7 % pour Indice Standard & Poor's 500 actions.\nLe Calvert Equity Fund (CSIEX), considéré comme l'un des plus grands et des plus anciens fonds d'investissement socialement responsable, ou ISR, a gagné 6,9 % en rythme annualisé sur 15 ans de 1999 à 2014, contre 5,5 % pour le S&P au cours de cette période.\nBarber et Strack, « The Surprising Economics of a \"People Business\" », Harvard Business Review, vol. 83, no 6,‎ juin 2005, p. 80–90, 149 (PMID 15938440, lire en ligne, consulté le 2 juillet 2021)\nEdmans, « 28 Years of Stock Market Data Shows a Link Between Employee Satisfaction and Long-Term Value », Harvard Business Review,‎ 24 mars 2016 (lire en ligne, consulté le 2 juillet 2021)\nAlex Edmans, Grow the Pie: How Great Companies Deliver Both Purpose and Profit, Cambridge University Press, 2020 (lire en ligne) This contains a review of the academic evidence for stakeholder capitalism.Indice des actions des entreprises engagées, créé par l'Enterprise Engagement Alliance sur la base du Good Company Index créé par McBassi, une société d'analyse du capital humain. L'étude consistait en un faux ETF (Exchange-Traded Fund) d'environ 40 entreprises avec des notes élevées en termes d'engagement des clients, des employés et de la communauté, surperformant le S&P 500 de plus de 37 points de pourcentage sur six ans.Tonti, « Our Top Survey Insights on the State of Stakeholder Capitalism from 2020 », justcapital.com, 8 janvier 2021 (consulté le 2 juillet 2021)== Article connexe ==Capitalisme compassionnel== Notes et références == Portail du capitalisme   Portail de l’économie"
        },
        {
            "pageid": 9044971,
            "ns": 0,
            "title": "Chaîne de fabrication",
            "content": "En macroéconomie, une chaîne de fabrication est constituée par les différents intervenants dans la fabrication d'un produit ou d'un bien.La chaîne de fabrication peut impliquer plusieurs établissements travaillant en cascade voire plusieurs secteurs économiques: dans l'industrie agroalimentaire, elle voit l'intervention du secteur primaire (productions agricoles) puis du secteur secondaire (production industrielle de produits alimentaires tels que les conserves ou les surgelés); dans l'industrie automobile, de la sidérurgie puis des chaînes de montage. Dans le langage courant, le terme peut être utilisé pour désigner aussi une chaîne de production ou, en terme microéconomique, une chaîne de montage ou d'assemblage dans une usine.== Historique ===== Évolution de l'industrie ===À partir du milieu du XIXe siècle, l'industrialisation apparaît massivement principalement en Europe. Ce processus est né dès le milieu du XIXe siècle en Angleterre. C'est la mécanisation qui va permettre cette effervescence principalement dans le domaine du textile et de la métallurgie. === Outils ===Quand on s'intéresse aux chaînes de fabrications il peut être intéressant de regarder l'évolution des différents outils que l'on utilise aujourd'hui. En effet certains outils que l'on utilise encore aujourd'hui existent depuis des millénaires on peut prendre l'exemple de l'araire qui sert à retourner le sol dans l'agriculture ou encore de la faucille. Cependant on se doute que ces derniers ont connu énormément de changement au cours du temps et ont beaucoup évolué. Ces évolutions ont été cherchées pour pouvoir aller plus vite, être plus efficace, ou encore faciliter le travail. Il est impossible de s'intéresser à l'évolution de chaque outil on va donc rester dans le secteur du jardinage pour notre exemple. L'entretien des jardins n'a été réellement mécanisé qu'en 1920. Mais en 1830 Edward Beard Budding crée la première tondeuse à cylindre. Et c'est en 1931 que l'anglais David Cockburn crée la tondeuse à système rotatif (celle aujourd'hui utilisée). On voit donc que l'outillage a évolué dans le temps tout en restant dans les mêmes fonctionnalités de base.[1]== Description ===== Technologies ===Après le taylorisme et le fordisme, le progrès technique a permis d'améliorer les chaines de fabrication. En effet la technologie a fait évoluer les machines et techniques de fabrication, ce qui a augmenté leurs productivités. Les chaines de fabrication sont en grande partie semi-automatisées. L'aspect humain est toujours nécessaire à fournir des informations et à la maintenance des machines. Ces avancées technologiques permettent de réduire le travail des humains ou de le soulager.  Les usines connectées sont aussi des progrès techniques importants pour les chaines de fabrication. Elles ont pour but de permettre la visualisation et la simulation d'usine entièrement connectées par des machines intelligentes et automatisées. Cette technologie permet de détecter des anomalies et de les modifier directement sur la chaîne de fabrication mais aussi de fabriquer des pièces à partir d'image numérique...=== Domaines ===Ces chaines sont utilisées dans plusieurs domaines, secteurs ou industries. Plusieurs étapes interviennent dans la chaîne de fabrication, ces étapes peuvent faire intervenir plusieurs domaines et ce dans plusieurs pays. Toutes fabrications ou productions nécessitent une chaîne de fabrication. L'utilisation de celle-ci se fait dans l'industrie de l'automobile, de la technologie, de l'agroalimentaire...=== Ressources ===Les ressources de la chaîne de fabrication sont principalement économiques, humaines et technologiques. Les ressources économiques et technologiques sont essentielles à la fabrication de produit. Les ressources humaines sont utilisées pour la maintenance ou pour des procédés demandant une certaine minutie malgré la fabrication de grande série.== Économie ===== Industrialisation ===L'industrialisation peut se définir comme étant un processus permettant d'appliquer à un secteur des techniques et des procédés industriels qui apportent une automatisation et une hausse de productivité. Au niveau économique, l'industrialisation permet le développement d'échanges de produits manufacturés entre pays industrialisés et en cours d'industrialisation. === Mondialisation ===Le terme de mondialisation correspond à la libre circulation des marchandises, des capitaux, des services, des personnes, des techniques et de l’information. La mondialisation permet donc à une société d'obtenir des ressources dont elles ne disposent pas dans son pays d’origine ou profiter de coûts de production plus faibles (main d’œuvre et ou matière première moins chère, etc.). La mondialisation du XXIe siècle repose sur deux facteurs principaux : la faiblesse des coûts de transport au regard des écarts des coûts de production ;la baisse des coûts de communication au niveau mondial.Concernant la chaîne de fabrication, un bien est fabriqué en plusieurs étapes qui peuvent correspondre à autant de pays différents. Il peut être plus rentable de fabriquer un produit dans un pays, la transporter et la vendre dans un autre pays.== Procédé ===== Définition ===Un procédé de fabrication est une méthode, une technique, par lequel on fabrique un bien ou un objet. Ce type de procédé implique généralement une production à grande échelle dite en série et il est composé d'une ou plusieurs étapes. === Techniques ===Trois grandes catégories existent dans la chaîne de fabrication : les procédés d'usinage, de découpage et de formage. Ces procédés sont en permanence optimisés afin d'augmenter leurs précisions et obtenir des produits finaux de meilleure qualité.Les procédés d'usinage et de découpage sont relativement proches, ils sont tous les deux basés sur l'enlèvement de matière par façonnage. Cependant ils diffèrent par la formation de copeaux. Les procédés de formage quant à eux, consistent à déformer la matière jusqu'à obtention de la pièce voulue. - Les principaux procédés d'usinage par enlèvement de matière :Le tournage : c'est un procédé menant à l'obtention de diverses pièces de forme cylindrique ou/et conique par enlèvement de matière appelés copeaux. L'enlèvement de matière est effectué par le biais d'outils coupants couplés à des machines appelées tours. La pièce que l'on désire usiner peut être fixée de différentes manières : soit dans une pince, dans un mandrin ou encore entre des pointes. Bien que ce ne soit pas sa fonction première, il est possible à l'aide d'une tour d'effectuer un perçage.Le fraisage : c'est un procédé mécanique par coupe se traduisant par deux mouvements en coordination. L'un est un mouvement de rotation d'un outil appelé fraise composé d'une multitude d'arêtes (mouvement de coupe). Le second mouvement accompagné de la pièce à découper est rectiligne (appelé mouvement d'avance). Le fraisage présente divers avantages comme un rendement et une précision élevés, une bonne finition. Il faut également savoir que le fraisage est une méthode d'usinage de plus en plus utilisée en raison d'une gamme large de machines, de systèmes de commande ou encore d'outils de coupe.Le perçage : c'est un procédé ayant pour objectif de faire un trou dans une pièce (de part en part ou non). Ce trou peut être réalisé par différents outils à savoir : un foret sur une perceuse, une mèche sur vilebrequin, laser, etc. L'opération de perçage avec un foret est une des plus utilisées dans la fabrication de pièces mécaniques. En effet, environ 25 % des usinages sont des perçages dans la mécanique générale. Différentes types de perçages existent tels que le perçage cylindrique, lamée, tronconique, etc.- Les principaux procédés de découpage par enlèvement de matière :Le découpage laser : c'est une technique de fabrication utilisant un laser afin de découper et graver de la matière. Ce procédé de découpage peut être réalisé sur différents matériaux tels que les matériaux ferreux, les matériaux non ferreux ou encore les alliages organiques dans des formes complexes. Cette technique de découpage consiste à concentrer un laser à la fois puissant et très précis sur une surface restreinte du matériau afin d'élever sa température pour le découper. De manière générale, un ordinateur est nécessaire pour diriger le laser et définir le chemin de découpe. Le découpage au laser démontre un haut niveau de précision, permet une économie de matériaux, une haute vitesse de production, une très faible déformation du matériau et une sécurité durant le découpage. Cependant, le principal inconvénient que présente ce procédé est sa grande consommation d'énergie pour engendrer la puissance du laser industriel,.Le découpage jet d'eau : c'est un procédé de fabrication qui utilise un jet d'eau très puissant pour découper différentes matières comme de la mousse, le cuir, des matériaux métalliques  et bien d'autres encore. Seul le verre trempé ne peut pas être découpé au jet d'eau car il ne supporte pas les fortes contraintes présentes dans le matériau. C'est une pompe à ultra haute pression qui génère la puissance du jet d'eau pouvant atteindre la pression de 6480 bars. Cette pression est convertie en vélocité par le biais d'une buse créant un jet d'eau extrêmement fin, comparable à un cheveu humain. Il faut savoir que du sable de grenat (abrasif) peut être couplé au jet d'eau pour multiplier par 1000 la puissance de découpe. Le jet d'eau pure est destiné à la découpe des matériaux tendres tandis que le jet d'eau chargé d'abrasif est capable de découper des matériaux solides tels que le métal, la céramique, la pierre ou encore les composites. Le découpage au jet d'eau est un procédé écologique et présente de nombreux avantages comme la précision de découpe, l'absence de poussière et de détérioration du produit par la chaleur ou encore la récupération de déchets de découpe dans l'eau,.Le découpage plasma : c'est un procédé thermique de découpage par fusion principalement utilisé pour la découpe de métaux conducteurs tels que le laiton, le cuivre, l'aluminium ou encore l'acier. Pour obtenir le plasma, il est nécessaire de procéder à une convection forcée du gaz choisit pour le découpage. Les gaz les plus souvent utilisés sont l'oxygène, l'air, l'argon, l'hydrogène ou des mélanges. La fonte du matériau pour découper la pièce a lieu à la suite de la création d'un arc électrique entre une électrode et la pièce à découper. Durant l'opération, une température de 20 000 °C peut être atteinte. La découpe plasma présente divers avantages comme des coûts faibles d'énergie (utilisation d'air), une large gamme de matériaux pouvant être découpés ou encore des vitesses de découpe rapides. Cependant, ce procédé possède également quelques inconvénients. En effet, les matériaux non conducteurs tels que le bois ou le plastique ne peuvent pas être découpés par le plasma. De plus, certaines limites sur l'épaisseur des plaques à découper sont à prendre en compte (160 mm pour la coupe sèche et 120 mm pour la coupe sous l'eau). - Les principaux procédés de formage (ou déformation)Le tréfilage : c'est une technique de formage à froid des métaux se traduisant par la réduction de la section d'un fil par déformation plastique. Sous l'action d'une force de traction et avec l'aide d'un lubrifiant, le fil est étiré à travers d'un orifice calibré. Cette technique a pour but d'obtenir des fils de diamètres inférieurs à 5 mm. De plus, la mise en forme à froid permet de s'affranchir de certaines limites technologiques et économiques comparé à la déformation à chaud. Les matériaux les plus utilisés pour ce procédé sont l'acier, le cuivre, l'aluminium et le tungstène. Les produits créés à la suite de l'utilisation de cette technique sont nombreux tels que : les clôtures, armatures pour béton, câbles, agrafes etc.L'extrusion (plastique) : l'extrusion est un procédé de transformation dit continu. Le granulé entre dans un tube chauffé (composé d'une vis sans fin) afin de l'homogénéiser et de le rendre mou. La matière est ensuite poussée et comprimée à travers une filière qui jouera le rôle de moule pour obtenir la forme souhaitée. Les produits fabriqués à l'aide de cette technique sont par exemple des profilés pour les portes et les fenêtres ou encore des canalisations,des câbles, des feuilles plastiques, etc. Le produit est ensuite refroidit pour être découpé à la taille voulue. Deux types d'extrusions principales existent : l'extrusion soufflage et l'extrusion gonflage.Le forgeage : c'est un procédé de fabrication où l'on façonne une pièce métallique en phase solide grâce à l'application de forces de pression (utilisation d'une presse). Il y a différents types de forgeage : forgeage à \"chaud\" (jusqu'à 1 150 °C mais varie selon le matériau), \"mi-chaud\" ou \"froid\" (température ambiante).L'emboutissage est le procédé de forge à chaud le plus courant, dans ce cas, le matériau est compressé dans une presse entre un poinçon et une matrice.La procédé de fabrication de forge à froid utilise principalement des techniques telles que le roulage, l'étirage, l'emboutissage, l'essorage, l'extrusion et le refoulement en bout.La forge à froid présente l'avantage de réaliser une économie de matériaux car elle permet de fabriquer des pièces de formes proches de leurs dimensions finales tout comme leurs poids. Il faut noter que les pièces issues du forgeage à froid présente un bon niveau de précision réalisable et une qualité de surface excellente.La forge à chaud donne lieu à des composants se prêtant à diverses configurations grâce à leur ductilité accrue. De plus, la forge à chaud permet la réalisation de pièces sur mesure étant donné de la souplesse du procédé comparé à la forge à froid. L'assemblage peut être considéré comme un procédé. Cela permet d'assembler deux pièces obtenues par un même procédé ou par des procédés différent, afin d'obtenir une nouvelle pièce ou produit.== Développement durable ===== Politique ===Selon le ministère de la transition écologique et solidaire, toutes entreprises et organismes publics devraient adopter l'écoconception des produits. Cette méthode préventive de l'environnement consiste à prendre en compte la protection de l'environnement au début de la conception de biens ou services. Son objectif final : réduire l'impact environnemental des produits ou services tout au long de leur cycle de vie. Cette approche apporte un certain nombre de bénéfices : des gains environnementaux, un nouveau regard sur les produits ou services, un effet positif sur la synergie dans l'entreprise et vis-à-vis des partenaires ainsi qu'un impact sur le coût du produit. En France, cette méthode est essentiellement promu par l'Ademe.=== Économie ===En France, \"Industrie du futur\", programme créé en 2015, a pour but d'accompagner chaque entreprise à moderniser ses outils industriels et de procéder à une transition d'un modèle économique à un modèle numérique. Ce raisonnement amène donc certaines questions sur le point de vue économique et social de la chose : Comment cela va-t-il transformer le marché et quels seront les impacts sur les emplois ? Quelle place aura l'Homme au sein de l'usine du futur ? Mais aussi sur le plan environnemental : quels bénéfices pourront-nous en tirer ? Sur quels indicateurs ? Et dans quelles conditions ?=== Normes ===Les normes ISO donnent lieu à une mise en place d'un management plus responsable. Ces dernières nous permettent d'avoir un suivi continuel sur chacune des machines et autres produits utilisés au sein d'une chaine de fabrication. Les normes ISO permettent de palier et d'anticiper toutes formes de problèmes (techniques, humains, environnementaux...) qui pourraient engendrer l'arrêt complet d'une chaine de production. Ces normes environnementales améliorent donc la productivité.== Contrôle qualité ==Le contrôle de la qualité était il y a quelques années encore l'activité principale de nombreuses entreprises en matière de qualité. Le contrôle qualité est devenu un outil fondamental. Si on se réfère aux normes AFNOR et ISO sur le vocabulaire de la qualité, le contrôle est en effet \"l'action de mesurer, examiner, essayer, passer au calibre une ou plusieurs caractéristiques d'un produit ou service et de les comparer aux exigences spécifiées en vue d'établir leur conformité\".== Articles connexes ==Chaîne d'approvisionnementChaîne de productionChaîne logistiqueIndustrialisation== Notes et références == Portail de l’économie   Portail de la production industrielle"
        }
    ],
    "Environnement": [
        {
            "pageid": 1032,
            "ns": 0,
            "title": "Environnement",
            "content": "L'environnement est « l'ensemble des éléments (biotiques et abiotiques) qui entourent un individu ou une espèce et dont certains contribuent directement à subvenir à ses besoins », ou encore « l'ensemble des conditions naturelles (physiques, chimiques, biologiques) et culturelles (sociologiques) susceptibles d’agir sur les organismes vivants et les activités humaines ».La notion d'environnement naturel, souvent désignée par le seul mot « environnement », a beaucoup évolué au cours des derniers siècles et tout particulièrement des dernières décennies. L'environnement est compris comme l'ensemble des composants naturels de la planète Terre, comme l'air, l'eau, l'atmosphère, les roches, les végétaux, les animaux, et l'ensemble des phénomènes et interactions qui s'y déploient, c'est-à-dire tout ce qui entoure l'Homme et ses activités ; bien que cette position centrale de l'être humain soit précisément un objet de controverse dans le champ de l'écologie.Au XXIe siècle, la protection de l'environnement est devenue un enjeu majeur, en même temps que s'imposait l'idée de sa dégradation à la fois globale et locale, à cause des activités humaines polluantes. La préservation de l'environnement est un des trois piliers du développement durable. C'est aussi le 7e des huit objectifs du millénaire pour le développement, considéré par l'ONU comme « crucial pour la réussite des autres objectifs énoncés dans la Déclaration du Sommet du Millénaire ».== Linguistique ===== Origine ===Le mot environement apparait en français dès 1265 dans le sens de « circuit, contour » puis à partir de 1487 dans le sens « action d'environner ». Le mot provient du verbe environner, qui signifie action d'entourer. Lui-même est un dénominatif d'environ, qui signifie alentours,.Deux dictionnaires au XIXe siècle attestent un emprunt à l'anglais environment mais pour traduire le mot milieu. Bertrand Lévy précise que le mot, au sens d'« environnement naturel qui entoure l'homme », apparaît pour la première fois en 1964, il est dérivé de l’américain environment. Avant, les géographes qui s'intéressaient au sujet et notamment Élisée Reclus utilisaient le terme milieu.=== Sens ===Le mot environnement est polysémique, c'est-à-dire qu'il a plusieurs sens . Ayant le sens de base de ce qui entoure, il peut prendre le sens de cadre de vie, de voisinage, d'ambiance, ou encore de contexte (en linguistique). L'environnement au sens d'environnement naturel qui entoure l'homme est plus récent et s'est développé dans la seconde moitié du XXe siècle.Le mot environnement est à différencier du mot nature qui désigne les éléments naturels, biotiques et abiotiques, considérés seuls, alors que la notion d'environnement s'intéresse à la nature au regard des activités humaines, et aux interactions entre l'homme et la nature. Il faut également le différencier de l'écologie, qui est la science ayant pour objet les relations des êtres vivants avec leur environnement, ainsi qu'avec les autres êtres vivants, c'est-à-dire l'étude des écosystèmes. La notion d'environnement englobe aujourd'hui l'étude des milieux naturels, les impacts de l'homme sur l'environnement et les actions engagées pour les réduire.L'environnement a acquis une valeur de bien commun, et a été compris comme étant aussi le support de vie nécessaire à toutes les autres espèces que l'Homme. En tant que patrimoine à raisonnablement exploiter pour pouvoir le léguer aux générations futures, il est le support de nombreux enjeux esthétiques, écologiques, économiques et socio-culturels, ainsi que spéculatifs (comme puits de carbone par exemple) et éthiques.L'ONU rappelle dans son rapport GEO-4 que sa dégradation « compromet le développement et menace les progrès futurs en matière de développement » (…) et « menace également tous les aspects du bien-être humain. Il a été démontré que la dégradation de l'environnement est liée à des problèmes de santé humaine, comprenant certains types de cancers, des maladies à transmission vectorielle, de plus en plus de zoonoses, des carences nutritionnelles et des affectations respiratoires ».Ce même rapport rappelle que l'environnement fournit l'essentiel des ressources naturelles vitales de chacun (eau, air, sol, aliments, fibres, médicaments, etc.) et de l'Économie ; « Presque la moitié des emplois mondiaux dépendent de la pêche, des forêts, ou de l'agriculture. L'utilisation non-durable des ressources naturelles, englobant les terres, les eaux, les forêts et la pêche, peut menacer les moyens d'existence individuels ainsi que les économies locales, nationales et internationales. L'environnement peut grandement contribuer au développement et au bien-être humains, mais peut tout aussi bien accroître la vulnérabilité de l'homme, en engendrant de l'insécurité et des migrations humaines lors de tempêtes, de sécheresses, ou d'une gestion écologique déficiente. Les contraintes écologiques encouragent la coopération, mais elles contribuent aussi à la création de tensions ou de conflits ».== Histoire ==L'histoire de l'environnement est une sous-division de l'histoire qui intéresse de plus en plus de chercheurs. Son but est d'étudier rétrospectivement l'état de l'environnement à différentes époques et ses interactions avec les activités humaines.=== Avant le XIXe siècle ===La prise de conscience de l'existence d'un environnement s'est développée par vague et de manière différente selon les époques, les régions et les cultures humaines. Certaines interprétations animistes ou religieuses, comme le bouddhisme, ont favorisé un certain respect de la vie, des ressources naturelles, et des paysages. Ce respect était motivé avant tout par des croyances religieuses, bien plus que par un réel désir de protection des milieux naturels. En effet, les concepts d'environnement économique, urbain ou civique tels que nous les définissons aujourd'hui ne semblent pas avoir été relevés par les ethnologues ni par les historiens.=== Au XIXe siècle ===Au XIXe siècle, en Occident, le romantisme a mis en avant la beauté des paysages sauvages, parfois en les opposant aux paysages et à la misère des mondes ouvriers, et industriels. En vantant les beautés de la nature, les romantiques ont fait prendre conscience que ce bien était précieux et devait être préservé. C'est par cet intérêt porté au paysage que les sociétés humaines vont commencer à prendre en compte l'environnement.À partir de 1825, les peintres de l'École de Barbizon sortent de leurs ateliers, ils peignent directement la nature dans la forêt de Fontainebleau et souhaitent en préserver sa beauté. Contre les forestiers qui souhaitent planter des résineux au risque d'altérer le paysage, ils inventent l'écoterrorisme en s'opposant aux coupes et en arrachant les jeunes plants potentiellement disgracieux. En 1853, ils obtiennent que cette forêt soit classée sur plus d'un millier d’hectares pour un motif esthétique. En 1861, un décret impérial officialise ces « réserves artistiques ». Ainsi la forêt de Fontainebleau devient le premier site naturel protégé au monde.Le géographe Élisée Reclus décrit avec émerveillement et poésie le milieu dans lequel vivent les hommes tout en constatant les effets du capitalisme sur l’agriculture et l’environnement. Précurseur de l'écologie, il sensibilise et incite ses lecteurs à endosser la responsabilité de la beauté de la nature, condition pour l’épanouissement de la nature et de l’humanité.En 1872, sous la menace que le gouvernement d'Adolphe Thiers fait peser sur la forêt de Fontainebleau George Sand se révèle pionnière de la future écologie. Soucieuse de rigueur et de curiosité scientifique elle convoque toutes les sciences naturelles : la biologie, la géologie, l'entomologie mais aussi les sciences de l'ingénieur pour rédiger un plaidoyer de douze pages, où elle écrit : « Si on n’y prend garde, l’arbre disparaîtra et la fin de la planète viendra par dessèchement, sans cataclysme nécessaire, par la faute de l’homme », elle initie ainsi les règles d'une exploitation forestière respectueuse et sauve la première réserve naturelle.Les États-Unis créent le statut de parc national, avec le président Abraham Lincoln le 30 juin 1864 et la Yosemite Valley devient le second site naturel protégé au monde. Le parc national de Yellowstone deviendra en 1872 le premier parc national. La France, en 1906, vote sa première loi sur la protection du paysage. À cette époque, c'est plutôt le paysage, et non l'écosystème qui guide les choix des élus pour les sites à protéger, comme le montre par exemple le classement des boucles de la Seine peints par les impressionnistes.En 1896, Arrhenius développe l'embryon de la première théorie environnementaliste, en étudiant l'effet de l'augmentation de la teneur en dioxyde de carbone (CO2) dans l'atmosphère ; dans son article De l'influence de l'acide carbonique dans l'air sur la température du sol, il cite la vapeur d'eau et le CO2 comme gaz à effet de serre, et emploie même le terme. Il propose certains calculs mettant en évidence l'élévation de la température en fonction de l'élévation de la concentration en CO2 ; il formule l'hypothèse du lien entre des variations de concentration au cours des âges géologiques, expliquant les variations de températures correspondantes.=== Au XXe siècle ===Dès la fin du XIXe siècle et pendant la majeure partie du XXe siècle, le développement mondial est très fort. La révolution industrielle et la forte croissance économique favorisent une industrie lourde et fortement consommatrice en ressources naturelles. Les nombreux conflits font prendre conscience de la rareté de certaines ressources, voire localement de leur épuisement.Les premières catastrophes industrielles et écologiques visibles (marées noires, pollution de l'air et des cours d'eau) sensibilisent l'opinion publique et certains décideurs à la protection des écosystèmes.La perception de l'environnement a également fortement progressé avec une meilleure diffusion des connaissances scientifiques et une meilleure compréhension des phénomènes naturels. La découverte et l'exploration de nouveaux milieux (Arctique, Antarctique, monde sous-marin) ont mis en évidence la fragilité de certains écosystèmes et la manière dont les activités humaines les affectent. Ils ont été respectivement et notamment vulgarisés par de nombreux auteurs, dont Paul-Émile Victor et le commandant Cousteau.Dans le même temps, la connaissance rétrospective de l'histoire de la planète et des espèces progressait avec la paléoécologie, et la mise à jour de preuves scientifiques de catastrophes écologiques majeures qui ont fait disparaître successivement des espèces durant des millions d'années. Ces sciences du passé ont montré les liens forts qui lient la pérennité des espèces à leur environnement et au climat.De nombreux outils scientifiques et techniques ont également contribué à une meilleure connaissance de l'environnement et donc à sa perception. Parmi les principaux, citons l'observation, puis l'analyse et la synthèse, photographie aérienne, puis satellitaire, et plus récemment, la modélisation prospective.Vers la fin du XXe siècle, la prise de conscience de la nécessité de protéger l'environnement devient mondiale, avec la première conférence des Nations unies sur l'environnement à Stockholm en juin 1972. En juin 1992, lors du sommet de la Terre de Rio de Janeiro, l'environnement est défini comme un bien commun et un bien public. Depuis les années 1990, les mentalités évoluent très rapidement pour se rapprocher de la perception que nous[Qui ?] avons aujourd'hui[Quand ?] de l'environnement.Cependant, la prise en compte de l'environnement dans les décisions et les pratiques environnementales diffère énormément d'un pays à l'autre. Dans les pays en voie de développement, où les préoccupations de la population sont très différentes de celles des pays développés, la protection de l'environnement occupe une place beaucoup plus marginale dans la société.=== Au XXIe siècle ===La Charte de l'environnement a été annoncée le 3 mai 2001 à Orléans par le président de la République française Jacques Chirac. Elle a été adossée à la Constitution française par la loi constitutionnelle no 2005-205 du 1er mars 2005. Par principe de précaution, elle dispose que : « Chacun a le droit de vivre dans un environnement équilibré et respectueux de la santé ». Avec la Charte de l’environnement, le droit à l’environnement devient une liberté fondamentale de valeur constitutionnelle. La Charte place en effet, désormais, les principes de sauvegarde de notre environnement au même niveau que les Droits de l’Homme et du Citoyen de 1789 et les droits économiques et sociaux du préambule de la constitution de 1946.== Art et environnement ==Depuis quasiment les débuts de l'art, l'environnement a été une source d'inspiration inépuisable pour l'homme. Les représentations d'animaux ou de paysages jalonnent l'histoire de l'art, et il n'est pas une époque qui fasse exception à la règle.Les paysages occupent une part primordiale dans l'art en Extrême-Orient, notamment en Chine et au Japon, mais il faudra attendre la Renaissance en Europe pour voir les paysages prendre de l'importance dans la peinture. De nombreux peintres seront qualifiés de paysagistes, tant parmi les romantiques que parmi les impressionnistes.Plus tard, les éléments environnementaux seront toujours très présents dans les nouvelles formes d'art, comme la photo, et plus tard, le cinéma. Plus récemment, des artistes ou des personnalités utilisent l'art pour sensibiliser la population à la défense de l'environnement : c'est le cas par exemple d'Al Gore, qui réalisa un film An inconvenient truth, ou le photographe Yann-Arthus Bertrand.== Sciences de l'environnement ==La science a connu un développement considérable au cours du dernier siècle. Les connaissances scientifiques ont beaucoup progressé, en particulier dans le domaine de l'environnement. Certaines disciplines spécialement dédiées à l'environnement, qui n'existaient pas jusque-là sont même apparues récemment, comme l'écologie devenue seulement prééminente dans la seconde moitié du XXe siècle.La mise au point de nouveaux moyens techniques, d'instruments de mesure et d'observation, a fait considérablement avancer la connaissance que nous avions de l'environnement, que ce soit au niveau du fonctionnement des êtres vivants et des interactions avec leur milieu, des écosystèmes. Les avancées de la physique et de la chimie nous ont permis de comprendre le fonctionnement des végétaux et plus globalement des corps vivants. L'avancée de la science a entraîné une plus grande mesurabilité des impacts humains sur l'environnement, d'où provient également une plus grande prise de conscience.Les problématiques environnementales sont passées de problèmes locaux, comme la protection d'une espèce, à des problèmes mondiaux (trou dans la couche d'ozone, réchauffement de la planète, par exemple). La nécessité d'avoir des données mondiales est donc apparue, entraînant le besoin de mutualiser les données. Par nécessité, le monitorage (programme de surveillance) environnemental se développe aujourd'hui à échelle planétaire, aidée par les avancées techniques, politiques et idéologiques. L'Organisation des Nations unies offre un cadre international de travail : PNUE, ainsi que des conférences internationales, et des sommets mondiaux, comme celui de Rio, permettant ainsi à des chercheurs de divers horizons de rassembler leurs connaissances. Les problématiques environnementales étant récemment devenues mondiales, il est fondamental d'appréhender la recherche scientifique de manière globale, et non plus locale.De nombreux pays ou groupes de pays ont également des communautés d'intervenants, d'indicateurs et de chercheurs spécialisés dans les thématiques environnementales, avec des programmes de mutualisation et d'échange des connaissances.=== Observation (monitoring) de l'environnement ===Des agences ou observatoires de l'environnement se sont constitués dans de nombreux pays. Ils relèvent, mesurent, et suivent des indicateurs environnementaux et produisent des statistiques, éventuellement agrégées au niveau local, régional, national, européen (ex : Eurobaromètre) et planétaire (sous l'égide de l'ONU et du Programme des Nations unies pour l'environnement (PNUE). Ce sont des outils d'aide à la décision.== Impacts de l'Homme sur l'environnement ==L'idée d'une dégradation de l'environnement de la Terre dans laquelle vivent les humains, par l'effet de la pollution, est devenue largement majoritaire à la fin du XXe siècle : cet effet prend la forme d'une crise écologique globale. Plus qu'une idée, les faits démontrent que l'évolution de l'environnement est représentative d'une dégradation de l'habitat, imputable à l'activité humaine.Pour mesurer cette dégradation, on peut se servir de plusieurs indicateurs :les pollutions apparentes, c'est-à-dire les traces de composés synthétisés par l'homme dans les milieux naturels : les sols, l'air et l'eau. Ces indicateurs sont plus couramment désignés sous d'autres noms, comme qualité de l'eau pour la présence de pollution dans l'eau, ou qualité de l'air pour la présence de polluants dans l'air ;la raréfaction des ressources naturelles, renouvelables ou pas ;la perte de biodiversité, qui est même considérée comme un indicateur clé de l'état de l'environnement.En 2001, un rapport de l'OCDE a fait l'état des thématiques environnementales et leur a associé un « niveau d'inquiétude ». Cette étude montre que les impacts de l'homme sur l'environnement sont multiples et variés. Presque tous les éléments constituant l'environnement sont touchés par les activités humaines.Ces impacts sur l'environnement sont liés à plusieurs facteurs, dont ceux évoqués le plus souvent sont la démographie et le développement économique. En effet, le lien entre la population et la pollution est évident : les impacts humains locaux sont proportionnels au nombre d'habitants d'une région, et il en est de même pour le nombre d'habitants sur la Terre,. Mais la démographie n'est pas le seul facteur qui intervient dans cette équation. Le niveau de développement économique, les habitudes de vie, le climat et toute une multitude de facteurs, jouent un rôle très important dans les impacts sur l'environnement, ce qui amène de nombreux spécialistes à relativiser le rôle de la démographie et de la surpopulation dans les problèmes environnementaux,.=== Sols ===Les problèmes liés aux sols sont souvent des problèmes d'ordre local. On parle de régression et dégradation des sols lorsqu'un sol perd en qualité ou que ses propriétés changent. Ils peuvent être divisés en deux catégories :les problèmes liés à l'érosion. L'érosion est un phénomène naturel, mais elle peut s'avérer désastreuse lorsqu'elle est provoquée par l'homme. Pouvant avoir pour cause certaines techniques d'agriculture comme la monoculture, l'agriculture intensive ou l'irrigation sur certains types de sols, des techniques d'élevage comme le surpâturage, ou la déforestation (les racines contribuent souvent à stabiliser le sol et à empêcher l'érosion), elle peut avoir comme effet des glissements de terrain, favoriser la désertification, l'aridification ou des menaces pour la biodiversité ;les problèmes de changement des qualités du sol. Il peut alors s'agir de salinisation, souvent due aux techniques agricoles, ou de pollution directe du sol, d'origine industrielle ou individuelle. Le sol concerné peut alors devenir infertile, et hostile à certaines espèces végétales ou animales et affecter la diversité des organismes peuplant le sol.=== Eau ===Selon un rapport de l'OCDE de 2001, trois points sont particulièrement préoccupants concernant l'eau. Il s'agit de la consommation d'eau et l'épuisement de la ressource, la pollution des eaux de surface et la pollution des eaux souterraines.==== Eau ressource ====La gestion de l'eau en tant que ressource naturelle est une question préoccupante pour de nombreux états. Le rapport de l'OCDE de 2001 qualifie ce problème comme « nécessitant une attention urgente ». Toujours d'après ce rapport, un grand nombre d'humains vivent dans des zones soumises au stress hydrique. En 2030, en l'absence de mesures efficaces pour préserver les ressources en eau potable, il pourrait y avoir 3,9 milliards de personnes concernées par le stress hydrique, dont 80 % de la population du BRIC (Brésil, Russie, Inde, Chine). Cette pénurie sera aggravée par l'augmentation de la population et donc des besoins en eau pour boire ou pour l'agriculture.Le réchauffement de la planète aurait également des incidences fortes sur les ressources en eau. Des régions comme l'Asie centrale, l'Afrique sahélienne ou les grandes plaines des États-Unis pourraient connaître un assèchement dramatique pour les populations, leur approvisionnement en eau, et l'agriculture, comme le rappellent les études de l'UNFCCC.Ce manque d'eau à l'échelle mondiale semble donc inéluctable, et s'annonce lourd de conséquences sur les activités humaines (agriculture, développement, énergie), et sur les relations diplomatiques internationales. En effet, les enjeux se multiplient autour de l'eau ; indispensable à la survie d'une population, elle l'est aussi pour l'agriculture, via l'irrigation, à la production d'énergie hydraulique. Les cours d'eau ne se limitant généralement pas à un seul État, ils sont devenus des enjeux géopolitiques stratégiques déterminants à la source de nombreux conflits. La plupart des états sont conscients de ces enjeux forts, comme l'atteste la tenue régulière du forum alternatif mondial de l'eau.==== Qualité de l'eau ====La pénurie d'eau n'est pas la seule préoccupation à avoir vis-à-vis de la gestion des ressources en eau. En 2001, l'évolution de leur qualité et de leur degré de pollution était également inquiétante.Parce que l'eau douce est une ressource précieuse, la pollution des nappes phréatiques, qui constituent une réserve importante d'eau douce relativement pure, et des lacs et des rivières, est sans doute la plus préoccupante. Ceux-ci étant également liés aux activités humaines, ils sont affectés, et leur état est globalement en cours de dégradation. Les pollutions des eaux douces se retrouvent dans les mers et les océans, de par le cycle de l'eau, et viennent ainsi aggraver la pollution marine.La pollution des eaux peut être d'origine et de nature diverses et variées. Elle peut être :physique : qui elle-même peut être thermique ou radioactive. La pollution thermique est due principalement aux industries qui utilisent l'eau comme liquide de refroidissement. Provoquant un réchauffement significatif des cours d'eau concernés, elle peut avoir pour conséquence la disparition locale de certaines espèces animales ou végétales. La pollution radioactive, pouvant survenir lors d'accidents nucléaires, est extrêmement persistante. Ses effets à long terme sont aujourd'hui méconnus ;chimique : extrêmement diverse, elle est causée par le rejet de différentes substances chimiques issues de l'industrie, l'agriculture ou des effluents domestiques. Les principales pollutions chimiques sont :les pollutions issues de l'agriculture et des certaines industries. Forte consommatrice de produits chimiques, l'agriculture a un impact considérable sur les milieux aquatiques. L'usage de pesticides, produits extrêmement nocifs aux êtres vivants, entraîne une dissémination de ces substances dans des milieux aquatiques, souterrains ou de surface, et provoque la mort de certaines espèces animales. Les nitrates et les phosphates, contenus en fortes quantités dans les engrais, entraînent des problèmes d'eutrophisation. Le fort développement de bactéries ou d'algues de surface, qui trouvent dans les nitrates et les phosphates les éléments nécessaires à leur développement, entraîne un manque d'oxygène dissous dans l'eau, ce qui conduit finalement à la destruction de toute vie animale ou végétale en dessous de la surface,les pollutions aux métaux lourds, comme le plomb, le mercure, le zinc ou l'arsenic. Issus pour la plupart des rejets industriels, ils ne sont pas biodégradables. Présents tout au long de la chaîne alimentaire, ils s'accumulent dans les organismes,les pollutions aux acides, provenant des pluies acides sont également nocifs,les pollutions aux substances médicamenteuses. Un très grand nombre de molécules médicamenteuses ne sont pas entièrement assimilées par le corps humain, et sont donc rejetées à l'égout. En l'absence de traitements spécifiques, elles se retrouvent dans les milieux naturels aquatiques, avec des conséquences pour l'environnement et la santé humaine encore mal connues. Des études sont en cours pour mesurer les impacts de ces substances,les pollutions aux hydrocarbures, comme les marées noires ou les dégazages sauvages. Spectaculaires en mer, elles sont aussi fréquentes en milieu urbain, ou elles peuvent représenter jusqu'à 40 % des pollutions de l'eau,les pollutions aux PCB : utilisées principalement dans les transformateurs électriques, condensateurs, et comme isolants en raison de leurs excellentes caractéristiques diélectriques, ces substances se stockent dans les graisses des êtres vivants, et peuvent avoir des effets toxiques et cancérigènes.Organique : cette pollution est la pollution la plus « naturelle », mais aussi la plus ancienne. En effet, en l'absence de traitement, une ville de 100 000 habitants rejette 18 tonnes de matière organique par jour dans ses égouts. Cette matière, bien que biodégradable, n'en est pas dénuée d'impacts pour autant. De trop forts rejets dans les rivières peuvent conduire à l'asphyxie des écosystèmes aquatiques, les premiers concernés étant les poissons, puis, à plus forte concentration, le reste de la faune et de la flore aquatique ;Microbiologique : on désigne sous ce terme les pollutions par les virus, bactéries et parasites. Principalement contenus dans les excréments, ces germes peuvent provoquer des maladies graves pour ceux qui les ingurgitent.=== Air ===La pollution atmosphérique, ou pollution de l'air, est une pollution d'origine diffuse qui peut avoir des effets locaux ou globaux. Le terme « pollution de l'air » signifie généralement « l'introduction directe ou indirecte dans l'air ambiant (à l'exception des espaces confinés) par l'homme de toute substance susceptible d'avoir des effets nocifs sur la santé humaine et/ou l'environnement dans son ensemble ».Comme pour l'eau, la pollution de l'air peut être de nature et d'origine diverses et variées. On distingue différents types de pollutions :les gaz chimiques toxiques, issus principalement de la combustion (provenant de l'industrie ou des moteurs, par exemple), dont :l'ozone, qui bien qu'étant un composé naturel de certaines couches de l'atmosphère, est considéré comme un polluant avec des effets néfastes sur la santé (asthme, irritations des voies respiratoires supérieures…) lorsqu'il est présent dans la basse atmosphère,,les gaz issus de la combustion, comme le dioxyde de soufre, les oxydes d'azote, le monoxyde de carbone, l'hydrogène sulfuré, et certains autres gaz à effet de serre ;les poussières, ou plus généralement les particules en suspension et les COV, provenant principalement des travaux publics, du nettoyage ou autre ;les gaz à effet de serre, dont les principaux sont le dioxyde de carbone, le méthane, mais aussi certains gaz fluorés, provenant de la combustion, des transports, des élevages, et des industries ;les métaux lourds, issus de différentes industries spécifiques, dont l'arsenic, le plomb, le zinc, le cuivre, le chrome, le mercure et le cadmium sont les principaux.Les effets de cette pollution peuvent être régionaux ou mondiaux. Régionalement, on peut avoir :un effet direct de toxicité sur la flore, la faune ou les hommes, dans le cas de gaz toxiques, notamment. Les métaux lourds, les particules en suspension, et les gaz issus de la combustion ont des effets notoires dangereux sur les organismes. Lors de fortes pollutions, les polluants peuvent obscurcir le ciel, réduisant la photosynthèse, et pouvant influer sur l'intensité des précipitations et la météorologie locale ; c'est le cas par exemple du nuage brun d'Asie ;une modification de la composition de l'air, qui entraîne une accumulation de polluants dans les pluies, pouvant provoquer des pluies acides, aux effets désastreux sur la flore locale et sur les organismes vivants aquatiques.À l'échelle de la planète, les effets de la pollution atmosphérique sont importants, et ont des impacts sur l'atmosphère et le climat de l'ensemble du globe. Les deux principaux effets de cette pollution sont :le trou dans la couche d'ozone. Historiquement, c'est une des premières prises de conscience des effets globaux que peut avoir l'activité humaine sur la planète. Dû aux gaz chlorés et halogénés, et notamment aux CFC et aux halons, le trou n'a été découvert que vers le début des années 1980. Il a des impacts importants sur la santé humaine, la faune et la flore, notamment par le biais des rayons ultraviolets qui ne sont alors plus filtrés par l'ozone stratosphérique. À la suite d'une réduction drastique de ces gaz du fait de leur interdiction progressive, leur utilisation a été divisée par 8 en 20 ans, et le trou dans la couche d'ozone a cessé de s'agrandir et devrait se refermer autour de 2050 ;le réchauffement climatique, défini par le secrétaire général des Nations unies comme un enjeu majeur de notre temps, est très probablement dû à un rejet massif de gaz à effet de serre d'origine humaine. Mettant en jeu des processus très longs, ce réchauffement pourrait avoir des conséquences négatives importantes sur la biodiversité, le niveau des océans, et les courants marins au niveau mondial, et pourrait entraîner ou favoriser des destructions d'écosystèmes, des désertifications ou des bouleversements climatiques graves à une échelle locale (sécheresses, inondations, intensité des cyclones…). Les conséquences affecteraient une majeure partie de la population mondiale et seraient multiples et globalement négatives.=== Biodiversité ===Les activités humaines ont une incidence forte sur la biodiversité, c'est-à-dire sur l'avenir des espèces vivantes, animales et végétales. Le taux d'extinction actuel des espèces est de 100 à 1 000 fois supérieur au taux moyen naturel constaté dans l'histoire de l'évolution de la planète. En 2007, l'UICN a évalué qu'une espèce d'oiseaux sur huit, un mammifère sur quatre, un amphibien sur trois et 70 % de toutes les plantes sont en péril,. Cette extinction massive des temps modernes est souvent désignée par le nom d'extinction de l'Holocène.L'origine de cette extinction massive d'espèces est principalement humaine, et notamment depuis les années 1500, où l'influence de l'homme a considérablement augmenté.La surchasse et la surpêche sont à l'origine de la disparition ou facteurs de menaces sur plusieurs espèces, mais c'est surtout la destruction et la dégradation de l'habitat naturel qui a eu les plus importantes conséquences. L'anthropisation grandissante des milieux naturels, via la déforestation, l'imperméabilisation des sols, l'agriculture et l'élevage extensif, l'urbanisation des littoraux, l'introduction d'espèces invasives, mais aussi la pollution des eaux et des sols, ainsi que le changement climatique, sont autant de facteurs qui réduisent ou détruisent l'habitat de certaines espèces, causant parfois leur disparition.La biodiversité fait l'objet d'études internationales dirigées par les Nations unies, via un groupe d'experts : l'IPBES. Elle est considérée comme un indicateur important, dont la dégradation serait significative pour la santé de la planète, mais aussi pour le bien-être humain. La préservation de la biodiversité est également une cible des objectifs du millénaire pour le développement.=== Ressources naturelles ===Une ressource naturelle est un élément présent dans la nature, exploité ou non par les humains, et pouvant être renouvelable ou non renouvelable. Dans une approche quantitative, on parle de capital naturel.La raréfaction des ressources naturelles est considérée comme inquiétante et représente une menace pour l'environnement et les activités humaines, qu'il s'agisse des ressources naturelles renouvelables, ou des ressources non renouvelables.S'agissant des ressources renouvelables (poissons, forêts, etc.), leur surexploitation peut entraîner une baisse significative de la ressource disponible, diminuant ainsi sa capacité de renouvellement. Ce sont les problèmes de la surpêche et de la déforestation entre autres. Si rien n'est fait pour enrayer cette spirale, cela peut conduire à l'épuisement total de la ressource, comme cela s'est déjà produit localement sur l'île de Pâques, par exemple, où la déforestation a conduit à la disparition des arbres sur l'île et à l'extinction de plusieurs espèces.Pour les ressources non renouvelables telles que les énergies fossiles et les minerais, l'impact de leur extraction sur l'environnement est relativement faible à court terme. C'est leur utilisation, qui produit souvent une pollution significative, et leur raréfaction qui sont une source d'inquiétude socio-économique. En effet, certaines de ces ressources sont une composante importante de l'activité humaine et économique. Leur extraction, continuellement en hausse, conduit à une baisse inquiétante des réserves, ce qui pose des problèmes pour les besoins des générations futures en matières premières.=== Catastrophes écologiques ===L'apparition de certains types d'industrie et de nouvelles techniques au cours du XXe siècle a rendu possible des accidents ou des actions ayant des conséquences très importantes sur les hommes et sur de multiples domaines de l'environnement, tout en touchant des zones géographiques plus ou moins vastes. Certains de ces accidents, dont certaines grandes catastrophes industrielles ou certains accidents nucléaires, peuvent affecter des écosystèmes entiers et engendrer des séquelles graves sur l'environnement. On parle alors de catastrophe environnementale ou écologique,. Le terme est parfois utilisé pour désigner, non pas un événement ponctuel, mais une action ayant des effets négatifs importants et constants sur l'environnement. Le thème a notamment été largement utilisé dans les médias pour parler de l'impact écologique du barrage des Trois-Gorges.=== Effets sur la santé humaine ===Les dégradations de l'environnement ont des effets importants, sur la santé humaine et la qualité de vie des populations,,, comme l'attestent les études sur le sujet et les différents organismes chargés d'étudier la relation entre la santé et l'environnement. La qualité de l'environnement — notamment dans les régions fortement peuplées —, est devenue un véritable problème de santé publique.Le lien entre santé et environnement a pris toute son importance depuis le sommet de la Terre de Rio en 1992 ; la protection de l'environnement est alors apparue comme une étape incontournable des politiques de santé publique mondiales. Ce lien est généralement désigné par le terme santé-environnement, et il est étudié par la médecine environnementale et le domaine des risques sanitaires.Les domaines de l'environnement pour lesquels la pollution peut avoir les conséquences les plus néfastes sur les populations sont l'eau et l'air, ressources indispensables à la vie. La pollution des sols peut aussi générer, à plus long terme, des problématiques sanitaires.L'eau et l'air peuvent être vecteurs de produits toxiques, CMR, non-biodégradables, allergisants ou eutrophisants mais aussi de virus, bactéries et autres agents pathogènes ayant des effets pathologiques directs, à court, moyen ou long terme, sur les organismes vivants.=== Relations de l’humain avec l’environnement ===Il existe un pan de recherche portant spécifiquement sur les relations que l’humain entretient avec l’environnement, soit l’anthropologie de l’environnement. Plusieurs approches marquent cette branche de la recherche : l’écologie culturelle de Steward, l’approche écosystémique de Rappaport, l’ethnoscience et l’ethnoécologie comme chez Haudricourt, l’œuvre d’anthropologie structurale de Claude Lévi-Strauss, les sur rapports à la nature, des vivants et des non-vivants, notamment ceux de Ellen et Katsuyochi, de Descola et de Viveiros de Castro, et ceux sur la perception et sur « l’habiter » menés par Ingold. Ces recherches, selon Doyon, ont quelques points en commun : d’abord de questionner les perceptions et les constructions sociales de la nature. Mais aussi, elles cherchent souvent aussi à montrer que les divisions courantes dans la pensée occidentale entre la nature et la culture, ou entre la société et l’environnement ne sont finalement pas universelles et s’ancrent plutôt dans des constructions modernes, dans la suite des travaux de Latour.Quant aux thèmes de recherche, ceux-ci sont aussi variés. Les conséquences sociales, économiques et politiques des discours globalisés sur l’environnement sont une voie explorée par plusieurs spécialistes. Des enjeux connexes peuvent être discutés et analysés, comme la justice environnementale, les réfugiés climatiques et le racisme environnemental. D’autres sujets peuvent être étudiés dans les rapports entre humain et environnement, recensés par Doyon, parmi lesquels il y a : l’exploitation de la nature par la production mécanisée et industrielle en agriculture (pêche, exploitation minière, foresterie ou carburants fossiles), mais aussi le développement durable, la privatisation et la marchandisation de la nature et du vivant, la création des aires protégées, le développement de l’écotourisme. Le lien qui existe entre l'Homme, les animaux, la biodiversité et l'environnement est représenté par le concept du « One Welfare ».== Techniques de protection de l'environnement ==Dans les dernières années, des moyens techniques ont été développés pour adapter les méthodes industrielles aux impacts de l'activité humaine sur l'environnement. Ces moyens peuvent être techniques, mais aussi législatifs et normatifs. Au niveau international, des accords comme le protocole de Kyoto imposent des quotas maximum d'émissions de gaz à effet de serre. D'autres accords règlent des points plus précis, comme la protection d'un lieu, d'une espèce menacée, ou l'interdiction d'une substance.=== Traitement des effluents ===Dans les pays développés, les effluents, qu'ils soient liquides ou gazeux, sont majoritairement traités. Ces effluents peuvent être d'origine industrielle ou provenir des particuliers.Dans la plupart des pays riches, les effluents sont traités lorsqu'ils sont polluants. Pour l'eau, les particuliers sont équipés de fosses septiques ou sont reliés à l'égout. Les rejets liquides passent alors par une station d'épuration avant d'être rejetés dans la nature. Pour les industries, la législation impose des normes qualitatives pour les rejets. Les industries possèdent leur propre station de traitement, ou sont elles aussi reliées à l'égout.S'agissant de l'air, il existe là-aussi des normes imposant de traiter les rejets polluants. Ces normes sont cependant très dépendantes des techniques existantes, selon le principe de la meilleure technique disponible.La situation est très différente dans les pays en voie de développement. La plupart des effluents ne sont pas du tout traités, par manque de moyens, ou par absence de législation contraignante. Les enjeux environnementaux sont véritablement importants ; des effluents non traités ont un impact fortement négatif, non seulement sur l'environnement, mais aussi sur la santé des habitants.=== Gestion des déchets ===L'homme a un impact fort sur l'environnement via ses déchets. On estime que l'ensemble de l'humanité produit entre 3,4 et 4 milliards de tonnes de déchets par an, soit environ 600 kilos par an et par personne. Et ce chiffre est en constante augmentationComme pour les effluents, l'absence de gestion des déchets dans les pays pauvres ou sortant des circuits légaux dans le monde, entraînent des impacts négatifs sur l'environnement et la santé humaine. On estime qu'environ 75 % des déchets d’équipements électriques et électroniques (50 millions de tonnes par an) disparaissent des circuits officiels de retraitement, exportée en grande partie illégalement vers des décharges clandestines en Afrique (Ghana, Nigeria), en Asie (Chine, Inde, Pakistan, Bangladesh), ou encore en Amérique du Sud.Pour éliminer les déchets, il faut tout d'abord les collecter. Ensuite, il existe différentes techniques pour les éliminer :le stockage, ou l'enfouissement dans des décharges : en général, il est préférable de stocker uniquement les déchets ultimes, comme les résidus d'incinération ;l'incinération : très utilisée, car peu coûteux, il impose notamment de traiter les fumées qui peuvent s'avérer très nocives. Cette technique peut servir à une valorisation énergétique ;la pyrolyse ou la gazéification, qui permettent elles aussi une valorisation énergétique des déchets, et nécessitent également un traitement des fumées ;la méthanisation ou biométhanisation : en enfouissant les déchets organiques et en les privant d'oxygène, la matière organique fermente et dégage du méthane. Ce gaz peut ensuite être brûlé pour produire de l'énergie ou être distribué dans le réseau de gaz de ville ;le recyclage, qui a pour avantage de réduire la consommation en matières premières pour la fabrication de nouveaux biens, et qui permet de minimiser l'impact environnemental des déchets.L'impact environnemental des déchets peut être limité, à la fois par les industriels par l'Écoconception et d'autres dispositifs. Mais aussi par les consommateurs, à travers la démarche zéro déchet et la règle des 5 R, qui sont à appliquer dans cet ordre :Refuser : tous les produits à usage unique. Privilégier les objets réutilisables et les achats sans déchet (comme le vrac)Réduire : la consommation de biens, aux quantités réellement nécessaires. Eviter le gaspillage.réutiliser : tout ce qui peut l'être (réparer, vendre/acheter d'occasion, louer, emprunter…)recycler tout ce qui ne peut pas être réutilisé.composter tous les déchets organiques (rot en anglais)Cette démarche permet d'éviter à la source la création de déchets, de préserver ainsi les ressources naturelles, et de mieux valoriser les déchets qui sont malgré tout générés.=== Gestion des ressources naturelles ===La gestion des ressources naturelles est un enjeu environnemental de premier plan.Dans le but de sauvegarder les ressources non renouvelables, et de préserver les ressources renouvelables, des techniques de gestion se sont mises en place.Dans le cas du papier, certains labels certifient une gestion durable de la forêt, certifiant que l'exploitation respecte les rythmes de croissance des arbres et ne participe pas à la déforestation. Pour de nombreuses autres ressources, des labels existent, certifiant de techniques de gestion durables. Pour la pêche ou la chasse des quotas réglementaires imposent de respecter le rythme de renouvellement des espèces animales. Pour des espèces animales ou végétales menacées ou plus fragiles, il est possible de leur assurer une certaine protection grâce à des parcs naturels.Dans ce domaine, les efforts restant à faire sont grands pour assurer une gestion durable de la majorité des ressources que nous utilisons. C'est pour cette raison que l'OCDE a en fait une de ses priorités.=== Protection des milieux et des espèces ===Dans le but de préserver la biodiversité, de nombreux moyens ont été développés pour protéger les milieux naturels et les espèces qui y vivent.Les réserves naturelles, qui existent dans de nombreux pays au monde, permettent de préserver des écosystèmes rares ou menacés en limitant l'urbanisation et les activités humaines dans les zones concernées. Pour les espèces menacées, l'UICN dresse et actualise une liste rouge répertoriant les espèces menacées d'extinction. Appuyées par des conventions internationales, comme la convention de Washington, des mesures sont prises pour leur préservation.Plus récemment, la meilleure compréhension des espèces animales a permis la création des corridors biologiques, qui permettent de relier des milieux naturels entre eux, favorisant ainsi la migration et la dispersion des espèces.=== Réduction des émissions de gaz à effet de serre ===La réduction des émissions de gaz à effet de serre est devenue un enjeu mondial majeur pour la lutte contre le réchauffement climatique.La sobriété, le choix d'équipements moins gourmands en énergie sont là aussi les méthodes principalement employées. Le recours aux énergies renouvelables contribue, en réduisant les émissions de gaz à effet de serre, à combattre le réchauffement climatique, et représentent un avenir prometteur. Certains pays ont vu l'émergence et la progression de ces énergies ces dernières années, bien qu'elles restent encore marginales dans la plupart des pays. L'adoption par les consommateurs d'un régime végétarien ou végétalien contribue également à réduire l'émission de gaz à effet de serre.Les énergies renouvelables englobent des techniques relativement récentes, comme l'énergie solaire thermique, l'énergie solaire photovoltaïque, mais aussi d'autres formes d'énergies qui sont utilisées depuis longtemps sous d'autres formes, comme la biomasse, l'énergie éolienne, la géothermie et l'énergie hydraulique.== Actions de protection de l'environnement ==En réponse à la croissance des impacts négatifs sur l'environnement, et en partie, par la place grandissante de l'intérêt pour l'environnement dans la société, les gouvernements ont élaboré ou mis en place des lois ou des normes techniques, dans le but de réduire les répercussions néfastes de l'activité humaine sur l'environnement.=== Environnement : un des trois piliers du développement durable ===Le terme développement durable apparaît pour la première fois dans un rapport de l'UICN publié en 1980. La traduction du terme anglais sustainable development devrait être développement soutenable, mais l'expression développement durable lui a été préférée. C'est le rapport Brundtland (1987) qui pose véritablement les bases du développement durable, et qui en donne la définition de référence : « un développement qui répond aux besoins des générations du présent sans compromettre la capacité des générations futures à répondre aux leurs ».Comme le détaille le rapport Bruntland, cela implique un développement qui soit à la fois vivable (écologiquement supportable et socialement juste), viable (économiquement rentable et écologiquement supportable) et équitable (économiquement rentable et socialement juste), s'appuyant en cela sur ce qu'on appelle souvent les trois piliers du développement durable : l'économie, le social et l'environnement.L'idée d'un développement soutenable signifie que l'on ne doit pas prendre à la Terre plus que ce qu'elle peut donner. Cela implique le recours aux énergies renouvelables, au recyclage pour les matières premières dont le stock n'est pas renouvelable (comme les métaux par exemple), mais aussi une bonne connaissance du rythme de renouvellement des espèces animales, des végétaux, de la qualité de l'air, de l'eau, et plus généralement, de toutes les ressources que nous utilisons ou sur lesquelles nous agissons. Le but de cette démarche est d'avoir une empreinte écologique suffisamment faible pour ne pas faire diminuer le capital naturel. Le développement durable a été décliné en programmes pour la préservation de l'environnement par la majorité des gouvernements et des instances internationales ; en effet, il existe aujourd'hui un consensus global autour de la nécessité de se préoccuper de la durabilité du développement.Mais le développement durable est aussi l'objet de nombreuses critiques. Luc Ferry, par exemple, se demande « qui voudrait plaider pour un « développement intenable » ! Évidemment personne ! […] L'expression chante plus qu'elle ne parle ». Le développement durable peut également parfois être instrumentalisé, soit à des fins politiques pour légitimer des idées protectionnistes, par exemple, ou à des fins commerciales, comme argument de vente par des grandes sociétés. Enfin, le développement durable met la croissance économique au cœur de la stratégie de protection de l'environnement, accordant notamment une place importante à l'innovation et aux solutions techniques alors que certains de ses détracteurs estiment que c'est la croissance économique elle-même qui est à l'origine de la dégradation de l'environnement : c'est la théorie de la décroissance.=== Modèles économiques ===Le modèle économique de société, de par la consommation d'énergie, de matières premières, et de par le progrès technique, est très étroitement lié avec les impacts sur l'environnement et sa protection. Pour beaucoup, adopter un modèle économique différent permettrait de réduire nos impacts, : les deux modèles les plus couramment évoqués sont celui du développement durable et celui de la décroissance.==== Décroissance ====La décroissance est un modèle théorique qui prône la décroissance de l'économie dans le but de réduire les impacts humains sur l'environnement.Ce courant de pensée a pris naissance avec les réflexions du club de Rome, qui publia un rapport en 1972, sous le nom de The Limits to Growth, traduit en français par Halte à la croissance ? et aussi connu sous le nom de Rapport Meadows. Ce rapport part du constat que la population humaine ne cesse de croître, ainsi que la consommation de biens matériels, de matières premières, d'énergie, et la pollution engendrée. Il préconise donc de se limiter à une croissance zéro, pour éviter d'épuiser les ressources naturelles.Partant du même constat, les partisans de la décroissance, aussi appelés objecteurs de croissance, concentrent leurs critiques sur le choix du PIB comme indicateur de référence, jugeant ce dernier trop restrictif. En effet, cet indicateur ne prend pas en compte l'état de l'environnement et de ses ressources, pas plus que le bien-être humain. Pour eux, la meilleure solution serait d'entrer en décroissance économique de manière durable et d'abandonner ce qui n'est pas indispensable pour se contenter de satisfaire ses besoins naturels primaires sans entrer dans une société de consommation excessive.Les partisans de la décroissance sont opposés au développement durable, qui accorde une place importante à la croissance et au développement technique.Cette théorie est vivement critiquée, notamment sur le fait qu'elle ne prend pas en compte le fait que les progrès scientifiques et techniques pourraient permettre de moins polluer, remplacer les énergies fossiles par des énergies renouvelables, et qu'il est possible de maintenir une croissance économique sans augmenter les consommations d'énergie et de matières premières. Pour étayer cet argument, ils s'appuient par exemple sur l'évolution de l'intensité énergétique des grandes économies mondiales qui a significativement baissé depuis vingt ans. Cette théorie a fait notamment l'objet des critiques de plusieurs « prix Nobel » d'économie, comme Amartya Sen ou Robert Solow, qui précisent que le progrès permettra de remplacer les matières premières manquantes, notamment par le biais du recyclage. Ils citent en exemple le rapport Meadows qui prédisait la fin du pétrole pour le début du XXIe siècle. Enfin, un autre argument souvent repris est qu'un arrêt de la croissance économique serait préjudiciable aux pays les plus pauvres, dont la survie est très dépendante de la croissance, comme le prouve la crise économique de 2008-2009.==== Changement de mode d'alimentation ====Plusieurs chercheurs et ingénieurs mettent en avant le fait qu'une diminution significative de la consommation de viande permettrait d'agir efficacement pour l'environnement. Le secteur de l'élevage représente environ 15 % des émissions de gaz à effet de serre, principalement sous forme de méthane. L'élevage, intensif ou extensif, conduit à des risques environnementaux diverses tel que la pollution du sol et des eaux, une substitution des forêts au profit des prairies, et une substitution des prairies au profit de cultures dédiées à l'alimentation animale,.=== Politiques de l'environnement ======= L'environnement en politique ====Historiquement, ce n'est véritablement qu'avec l'apparition des ministères de l'Environnement dans les pays développés que l'environnement a occupé une place dans le débat politique. C'est à la fin des années 1970 que les premiers ministères de l'environnement voient le jour, avec la création le 2 décembre 1970 de l'Environmental Protection Agency par le gouvernement Nixon aux États-Unis, suivi en janvier 1971 par la France et en mai de la même année par l'Australie. Petit à petit, l'ensemble des pays développés vont se doter d'un tel ministère, avec plus ou moins d'importance, et souvent à la suite d'une détérioration importante de l'environnement, comme en Allemagne à la suite de la catastrophe de Tchernobyl.Depuis, la défense de l'environnement a pris une part croissante dans le débat politique, avec la création des partis verts. Les performances électorales de ces partis dans les pays développés se sont globalement améliorées des années 1980 à nos jours.Aujourd'hui, certaines élections récentes montrent l'importance des questions environnementales dans les débats politiques. En France en 2007, le pacte écologique de Nicolas Hulot, demandant un engagement fort en matière d'environnement, a été ratifié par tous les candidats à l'élection présidentielle. À l'élection présidentielle américaine de 2008, les questions environnementales ont eu une place importante dans les débats, défendues ardemment par Barack Obama. Enfin, aux élections européennes de 2009, le très bon score du groupe des Verts dans les pays de l'Union européenne vient confirmer cette tendance : l'environnement est véritablement devenu un enjeu politique fort.==== Actions internationales ====Illustrant la globalité du phénomène et sa place croissante dans le monde politique et géopolitique, les actions internationales en lien avec l'environnement se sont multipliées : sommets internationaux, accords et protocoles, journées mondiales, évolution des réglementations, etc..La description de la politique environnementale des États-Unis fait l'objet d'un article spécifique.Le manque de vision stratégique holistique bloque un certain nombre d'avancées pour l’environnement (ex Cf protocole de Kyoto et taxe carbone qui est l'exemple d'un échec majeur)===== Sommets et accords internationaux =====La première réunion internationale autour de l'environnement fut la Conférence internationale sur l’usage et la conservation de la biosphère, qui s'est réunie en 1968 à Paris. Elle permit aux différents acteurs présents d'entamer les discussions en vue du premier Sommet de la Terre, prévu à Stockholm en 1972.Ces sommets de la Terre sont les principaux sommets internationaux consacrés à l'environnement, et se tiennent tous les 10 ans.La conférence des Nations unies sur l'environnement de Stockholm en juin 1972, premier sommet international de grande ampleur consacrée à l'état de l'environnement, marque véritablement la prise de conscience d'un problème environnemental mondial, et de la nécessité d'une action concertée de préservation. Elle débouche sur une déclaration de principes et un plan d'action concrètes.Le 3 mars 1973, la convention de Washington est adoptée par un grand nombre de pays. Elle a pour objectif de veiller à ce qu'aucun commerce ne mette en danger la pérennité d'une espèce animale dans son milieu naturel. Son combat le plus connu est peut-être celui contre le trafic d'ivoire, qui met en danger les éléphants d'Afrique. La même année est adoptée la convention MARPOL, qui réglemente les pratiques en vue de diminuer les pollutions marines.Le sommet de la Terre de Nairobi, qui s'est tenu en 1982, a été un échec, du fait du faible intérêt de Ronald Reagan, alors président des États-Unis, du faible retentissement de ce sommet, et de l'absence de décisions importantes. Ce sommet n'est d'ailleurs pas considéré comme un sommet de la Terre.En 1984, le Programme des Nations unies pour l'environnement (PNUE) organise la Conférence mondiale de l’industrie sur la gestion de l’environnement, à Versailles, puis l'année d'après la Conférence internationale sur l’évaluation du rôle du dioxyde de carbone et autres gaz à effets de serre à Villach, alors que les premières interrogations sur le réchauffement climatique commencent à surgir.Le 16 septembre 1987 est signé le protocole de Montréal, qui vise à stopper les dégâts causés à la couche d'ozone, notamment en interdisant l'usage des chlorofluorocarbures et d'autres gaz nocifs pour la couche d'ozone. En 1989, la convention de Bâle réglemente le commerce des déchets, en interdisant notamment l'exportation de déchets des pays développés vers les pays en voie de développement pour échapper aux réglementations locales.En juin 1992, lors du sommet de la Terre de Rio de Janeiro, l'environnement a été défini comme un « bien commun » ou un « bien public ». Les acteurs internationaux ont montré avoir pris conscience que la problématique environnementale ne pouvait pas être découplée des problèmes économiques, écologiques et sociaux, de sorte que l'environnement a été considéré comme un dénominateur des trois piliers du développement durable. Il a été intégré dans les objectifs des agendas 21 pour les collectivités territoriales.Le 11 décembre 1997 est signé le protocole de Kyoto. Ce texte est d'une importance fondamentale puisque les pays l'ayant signé s'engagent à réduire leurs émissions en gaz à effet de serre, avec des objectifs chiffrés, et ce, pour essayer de limiter le réchauffement climatique. La mise en application du protocole et son suivi donneront lieu à une conférence internationale quasiment tous les ans. Ce protocole n'est entré en vigueur qu'en 2005, puisqu'il devait pour cela être ratifié par des pays dont les émissions en gaz à effet de serre représentent au moins 55 % des émissions mondiales.En 2002, lors du Sommet de la Terre de Johannesburg, sous l'impulsion, entre autres, des grandes ONG environnementales, l'environnement et le développement durable ont touché le monde des entreprises. On a vu émerger le concept de responsabilité sociétale des entreprises, application des principes de développement durable aux entreprises, l'environnement étant un témoin de l'efficacité fonctionnelle des trois piliers (économique, écologique et le social) du développement durable.Les préoccupations environnementales touchent également d'autres domaines, et apparaissent dans de nombreuses autres conférences ou sommets mondiaux (G8, G20, Conférences mondiales sur l'habitat, les villes, entre autres). Le conseil de sécurité des Nations unies s'est réuni en avril 2007 pour agir contre les changements climatiques et les dégradations de l'environnement, témoignant de l'importance de la question.Le dernier sommet mondial important a été le sommet de Copenhague en décembre 2009, dont le bilan est mitigé qui a entamé la préparation de l'après-Kyoto, et essayé de lui donner un nouveau souffle en décidant d'engagements chiffrés en matière de réduction des émissions de gaz à effet de serre.===== Les journées internationales =====Les journées mondiales ou internationales sont souvent officialisées par l'Organisation des Nations unies. Un nombre croissant de journées internationales sont consacrées à des thèmes environnementaux, illustrant la place grandissante des thématiques environnementales dans la société. On y trouve, entre autres :20 ou 21 mars, jour de l'équinoxe : Jour de la Terre ;22 mars : Journée mondiale de l'eau ;22 mai : Journée internationale de la biodiversité ;5 juin : Journée mondiale de l'environnement ;8 juin : Journée mondiale de l'océan ;17 juin : Journée mondiale de la lutte contre la désertification et la sécheresse ;16 septembre : Journée internationale de la protection de la couche d'ozone ;22 septembre : Journée sans voiture ;4 octobre : Journée internationale des animaux.==== Réglementation ====Le droit de l'environnement est une discipline relativement récente qui a pour objet l'étude ou l'élaboration de règles juridiques concernant l'utilisation, la protection, la gestion ou la restauration de l'environnement. C'est un droit technique et complexe, en pleine expansion, et dont les champs tendent à se densifier au fur et à mesure des avancées sociales, scientifiques et techniques. Il est dans un nombre croissant de pays matérialisé par un code de l'environnement, mais sans juridiction spécialisée à ce jour (il n'y a pas de juge de l'environnement, comme il peut y avoir un juge à l'enfance, une spécialité criminelle ou anti-terroriste). Dans certains pays il existe cependant des services de police, douane ou garde-côte ayant une spécialité environnementale.Les textes de références sont généralement nationaux, sauf dans le cas de conventions, d'accords, et de systèmes de management internationaux, comme la norme de management environnemental ISO 14001. La plupart des pays cherchent désormais à harmoniser leurs textes réglementaires pour adopter une réponse plus adaptée aux problèmes mondiaux.Sans que cela soit pour autant réglementé, de nombreuses ONG appellent à une éthique de l'environnement qui soit reconnue par la majorité. De même, certaines organisations demandent que soit développée la notion de crime environnemental, notion diversement définie à travers le monde.=== Associations écologistes ===Il existe de nombreuses associations et organisations non gouvernementales actives sur les questions d'environnement. Parmi les plus en vue au niveau international, on trouve :Avaaz.org ;Friends of the Earth international ;Les Amis de la Nature ;Greenpeace ;Climate Action Network ;Sustainable Building Alliance ou SB Alliance ;Union internationale pour la conservation de la nature ;World Wildlife Fund.En France, les associations peuvent être « agréées au titre de l'environnement » par le ministère de l'Écologie et du Développement durable. Ce sont des associations régies par la loi de 1901 qui contribuent à révéler des problèmes ou à trouver et tester des solutions dans les domaines de la protection de la nature et de l'environnement et de l'amélioration du cadre de vie (leur vigilance s'exerce sur l'ensemble du territoire). Il existe aussi des associations concernant l'éducation à l'environnement et au développement durable (EEDD) ou le lien santé-environnement, comme l'Association Santé Environnement France (ASEF).== Économie de l'environnement ===== Théorie économique ===L'économie de l'environnement est souvent considérée comme une sous-discipline de l'économie, qui s'intéresse aux relations entre l'environnement et l'économie, c'est-à-dire aux coûts des atteintes à l'environnement, de la protection et de la connaissance de l'environnement, ainsi qu'à l'efficacité et à la conception d'instruments économiques pour changer les comportements à l'égard de l'environnement. Toutefois, cette position est critiquée notamment par l'agroéconomiste américain Lester R. Brown, qui considère que l'économie devrait être au contraire une sous-discipline de l'écologie.Le problème qui se pose souvent est celui de la valeur marchande à attribuer à un bien environnemental, à une ressource ou à sa qualité. Par exemple, il est très difficile d'attribuer un montant à un air de bonne qualité ou de chiffrer les impacts d'une pollution sur l'eau. Les outils économiques permettant d'influencer les comportements sont nombreux, allant de la loi de l'offre et de la demande (qui rend moins accessible une ressource rare en augmentant son prix), les amendes, dont le calcul du montant peut s'avérer difficile, les licences, normes, permissions, etc.Cela nécessite une prise en compte des problèmes relatifs aux externalités liées à une activité, qui induisent un coût environnemental non pris en compte par le responsable ; par exemple, un agriculteur ne va pas payer les coûts engendrés par une éventuelle pollution de l'eau par les pesticides, ou un transporteur ne va pas payer pour les gaz rejetés dans l'atmosphère. C'est la prise en compte de ces problèmes qui a fait naître le principe de pollueur-payeur, mais également les droits à polluer, dont l'exemple le plus connu est peut-être la bourse du carbone, prévue par le protocole de KyotoL'économie de l'environnement traite également des marchés associés au domaine de l'environnement, et dont la croissance est forte. Ces marchés répondent à des besoins de non-pollution, d'efficacité énergétique, de traitement de l'air, de l'eau, de propreté ou de dépollution. Cette croissance entraîne une hausse de la demande en personnel formé aux métiers de l'environnement.=== Métiers et formations ===Les métiers de l'environnement se sont fortement développés dans le contexte du développement durable, faisant de l'environnement un secteur économique en plein développement. Le Grenelle de l'Environnement en France, et les objectifs de croissance verte et de réduction des rejets de CO2 dans les pays industrialisés ont donné une nouvelle impulsion au développement des métiers de l'environnement. On peut les séparer en cinq grands domaines :la protection et la gestion des espaces et espèces naturelles, ne représentant qu'un faible pourcentage des emplois du secteur. Assurées par l'État et des organisations spécialisées, ces missions se retrouvent principalement dans le secteur des forêts, des ressources naturelles et des parcs naturels. ;la prévention et le traitement des pollutions et des nuisances, dans les secteurs de l'eau (avec notamment l'ultrafiltration et l'osmose inverse), du bruit (murs anti-bruits), des déchets ménagers ou industriels, mais aussi de la recherche scientifique et technique de nouveaux moyens en vue de réduire les nuisances ;l'aménagement du territoire, principalement dans l'urbanisation, le paysage et la construction d'infrastructures ;la prise en compte des incidences sur l'environnement des différents projets, plans ou programmes à travers l'élaboration des études d'impact ou des évaluations environnementales. Ces documents sont pris en charge, en général, par des bureaux d'étude qui rassemblent les différentes compétences qui couvrent les champs de l'environnement : biodiversité, pollutions, nuisances…) ;la prise en compte des problématiques environnementales dans les entreprises est généralement du ressort du ou des pôles « QHSE » (Qualité Hygiène Sécurité Environnement). Il s'agit de prendre en compte la règlementation sur l'environnement et de réduire les impacts en matière de pollutions au cours de l'activité régulière ou accidentelle d'une entreprise. La majorité des grandes entreprises aujourd'hui ont engagé une démarche environnementale ;la gestion sociétale de l'environnement, qui englobe les métiers de l'éducation à l'environnement, les politiques, les métiers du droit de l'environnement, mais aussi le lobbying, le conseil et l'audit.À cela il faut ajouter tous les métiers qui ne sont pas directement liés à l'environnement, mais qui comportent une fort dimension environnementale, comme les métiers de l'énergie, de la construction et de la thermique du bâtiment.La forte croissance de ces métiers demande des formations adaptées, elles aussi en forte augmentation. Dans les pays développés, il est aujourd'hui possible de trouver de nombreuses formations spécialisées ou ayant un lien avec l'environnement.== Philosophie et éthique de l'environnement ===== Environnement et religion ===La plupart des religions anciennes étaient respectueuses de l'environnement bien que la notion d'environnement à l'époque ne fût pas la même qu'aujourd'hui. Certaines religions animistes et celtiques faisaient des éléments de la nature, comme les sources, certains animaux ou plantes, des divinités. En effet, la non-compréhension de la nature lui conférait un aspect mystique qui aboutissait souvent à une divinisation de ses éléments.Dans l'hindouisme, l'environnement a une grande importance. On traduit hindouisme par sanatana dharma, qui, traduit approximativement, signifie l'« essence éternelle du cosmos » – la qualité qui lie tous les êtres humains, animaux et végétaux à l'univers alentour et éventuellement à Dieu, source de toute existence.Le shintoïsme a également divinisé de nombreux éléments naturels, sous le nom de kami. Un kami peut être toute entité supérieure à l'homme par sa nature.Le monde naturel joue un rôle important dans le judaïsme. Dans la loi juive (halakhah), on trouve des mises en garde pour la protection des arbres fruitiers, ou de tout ce qui relève du bien commun, y compris les éléments naturels constituant l'environnement. La gestion de la création a été confiée par Dieu à l'homme afin de lui assurer une base matérielle et un tremplin pour son développement spirituel. Le rapport du Judaïsme à la nature est donc marqué par le respect de ce qui appartient à Dieu (l'homme est gestionnaire, et non propriétaire) et le fait que tout élément sur terre a son rôle à jouer dans la création, pour le bien être de l'homme et l'harmonie de l'ensemble des créatures.L'Église catholique alerta la communauté internationale dès les années 1970 sur un important manque d'éthique. Notamment le pape Paul VI, inquiet des nouvelles politiques agricoles, a pris position en 1970 lors du 25e anniversaire de la FAO, puis a délivré un message fort en 1972 à l'ouverture de la Conférence des Nations unies sur l'environnement de Stockholm. Puis, en parallèle à l'œcuménisme prôné par Jean-Paul II, divers évènements chrétiens eurent lieu sur la question de l'environnement. De multiples initiatives œcuméniques ont abouti à proposer en 2007 de consacrer un temps pour la sauvegarde de la Création chaque année entre le 1er septembre (journée de prière pour la sauvegarde de la Création chez les orthodoxes, adoptée ensuite par les catholiques) et le 4 octobre (fête de saint François d'Assise chez les catholiques).De même, la plupart des autorités religieuses islamiques se sont positionnées en faveur d'un plus grand respect de l'environnement. 60 responsables religieux musulmans représentant 20 pays différents, se sont réunis les 17 et 18 août 2015 à Istanbul pour le colloque de l’Islamic Climate Change Symposium, et ont signé la déclaration islamique sur le changement climatique.=== Position du Saint-Siège ===En juin 2012, à l'approche de la Conférence des Nations unies sur le développement durable, Rio+20, le Saint-Siège rappelle « que l’on ne peut pas réduire à un problème « technique » ce qui touche la dignité de l’homme et des peuples : on ne peut pas, en effet, confier le processus de développement à la seule technique parce que, de cette manière, il serait privé d’orientation éthique. La recherche de solutions à ces problématiques ne peut pas être séparée de notre compréhension de l'être humain. La personne humaine à laquelle est confiée la bonne gestion de la nature ne peut pas être dominée par la technique et en devenir l'objet ».En juin 2015, quelques mois avant la Conférence de Paris sur le climat (COP 21), le pape François publie l'encyclique Laudato si' (« sur la sauvegarde de la maison commune »). C'est la première encyclique d'un pape entièrement consacrée aux questions d'environnement, d'écologie intégrale, et de développement durable et intégral. Bien conscient des problèmes environnementaux de la planète, notamment de l'origine anthropique du réchauffement climatique, le pape souligne que ce sont les pauvres de la planète qui souffrent le plus de la dégradation de l'environnement, et il montre que la préservation de l'environnement ne peut pas être dissociée de la préoccupation d'aider les plus pauvres, ce qui constitue la dimension sociale de la doctrine de l'Église.== Notes et références ===== Notes ====== Références ===Sauf indication contraire, les sources présentées ici sont exclusivement en français ((fr)).== Annexes ===== Bibliographie ===André Beauchamp, Environnement et Église : le temps de l'engagement, Montréal, Fides, 2008, 167 p. (ISBN 978-2-7621-2926-7)Conférence des évêques de France, La Création au risque de l'environnement, Paris, Bayard-Centurion, Fleurus-Mame, janvier 2009, 64 p. (ISBN 978-2-204-08849-7)Ministère de l'écologie et du développement durable, Réponses environnement. Entreprises et environnement : Rapport à la commission des comptes et de l'économie de l'environnement, Paris, La documentation française, 2004, 217 p. (ISBN 2-11-005695-9)Jean-Claude Fritz, Charalambos Apostolidis et Gérard Fritz (dir.), L'humanité face à la mondialisation. Droit des peuples et environnement, Paris, L'Harmattan, 1997, 230 p. (ISBN 2-7384-5517-4)Jean-Paul Besset, René Dumont, une vie saisie par l'écologie, Paris, Stock, coll. « Au vif », 1992, 375 p. (ISBN 2-234-02467-6)Bernard Kalaora et Chloé Vlassopoulos, Pour une sociologie de l’environnement, société et politique, 2013, Champ-VallonLimoges C & Doray P (1994) Le débat public comme apprentissage social et comme régulation constituante : le cas de l'environnementalisation. Avril 1994 ; In Actes du colloque international de Montréal : Quand la science se fait culture.Naomi Klein : Tout peut changer : capitalisme et changement climatique, Actes Sud, 2015Jean de Kervasdoué: Ils croient que la nature est bonne, Robert Laffont, 2016=== Articles connexes ===Glossaire de l'environnement et de l'écologieBiosphèreÉcosphèreConvention sur la diversité biologiqueÉcologieÉducation à l'environnement et au développement durableNatureListe des ministres de l'EnvironnementOrganisation mondiale de l'environnementPolitique climatiqueAmericana (salon) (forum sur l'environnement et Salon international de la technologie environnementale)Écologie intégrale=== Liens externes ===Ressource relative à l'audiovisuel : France 24  Portail Environnement de la Commission européenneSélection de sites web sur l’écologie, la biodiversité et l’environnement dans le répertoire encyclopédique : Les Signets de la Bibliothèque nationale de France Portail de l’environnement   Portail des sciences de la Terre et de l’Univers"
        },
        {
            "pageid": 16136172,
            "ns": 0,
            "title": "Glossaire de l'environnement et de l'écologie",
            "content": "Ce glossaire a pour objectifs de regrouper et de donner des explications courtes des termes utilisés pour les questions environnementales telles que l'écologie, la biodiversité, les conséquences du réchauffement climatique etc. Il doit être complété et enrichi...== A ==Abiotique (facteur) : adj. purement physico-chimiques, par opposition aux facteurs biotiques qui sont le fait d'êtres vivants.Acclimatée : se dit d'une espèce exotique bien installée dans un milieu mais ne s'y reproduisant pas de façon sexuéeAcide aminé : élément de base de la structure des protéines dans lesquelles ils sont liés les uns aux autres par des liaisons amide (ou liaison peptidique).Acide trifluoroacétique (TFA) : composé organofluoré de très petite dimension parmi les PFAS.Acidification des océans : diminution du pH de l’eau de mer due à l’absorption de dioxyde de carbone anthropique.Adaptation : ajustement fonctionnel de l’être vivant au milieu.ADN : abréviation pour acide désoxyribonucléique. Macromolécule constituée de 4 bases (adénine, cytosine, guanine ou thymine) liée au désoxyribose, lui-même lié à un groupe phosphate. Présent dans toutes les cellules ainsi que chez de nombreux virus, l’ADN contient l’information génétique de tous les êtres vivants.ADN environnemental ou ADNe : traces ADN que les différentes espèces laissent dans leurs habitats, mesuré par séquençage de l’ensemble des fragments d’ADN que l’on trouve dans un environnement donné.Adret : versant le plus ensoleillé d’une montagne (en général orienté vers le sud). Il est le versant le plus favorable à la végétation, aux cultures et à l’habitat. Opposé à l’ubac.Aérobie : organisme vivant ayant besoin de dioxygène pour subsister et se développerAgribashing : le terme, controversé, désigne selon le syndicat FNSEA « un présupposé dénigrement systématique du monde de l'agriculture ».Agriculture alternative : mouvement social qui défend les petits agriculteurs et la protection de l'environnement et qui critique la modernisation de l'agriculture.Agriculture biodynamique : système de production agricole sans intrants de synthèse, issu de l'anthroposophie.Agriculture biologique ou AB : méthode de production agricole visant à respecter les systèmes naturels, à maintenir l’état du sol et la santé des végétaux et des animaux en respectant un cahier des charges précis.Agriculture de précision : pratiques agricoles visant à optimiser le rendement et les investissements au niveau de la parcelle en utilisant de nouvelles technologies telles que l’imagerie satellitaire et l'informatique.Agriculture durable (anciennement soutenable, traduction alternative de l'anglais sustainable) : système de production agricole qui vise à réduire l'impact environnemental de l'agriculture en protégeant la biodiversité, l'eau et les sols qui lui sont nécessaires.Agriculture intégrée : pratique agricole visant à minimiser le recours aux intrants extérieurs à l’exploitation agricole en mettant en œuvre des rotations longues et diversifiées et en restituant les résidus de cultures et des déjections animales au sol.Agriculture paysanne : modèle de production agricole axé vers la recherche d’autonomie dans le fonctionnement de l’exploitation tout en respectant l’environnement.Agriculture raisonnée :  mode de gestion des productions agricoles, cherchant à minimiser l’impact de l’agriculture sur l’environnement en optimisant les intrants tout en maintenant la rentabilité économique des exploitations.Agriculture régénératrice ou régénérative : philosophie de la production agricole et ensemble de techniques adaptables fortement influencés par la permaculture.Agriculture vivrière : agriculture essentiellement tournée vers l'autoconsommation et l'économie de subsistance. La production y est rarement excédentaire.Agriculture urbaine ou périurbaine : (AUP) : forme émergente ou réémergente de pratiques agricoles effectuées en ville.agrobiodiversité ou biodiversité agricole désigne la diversité des espèces, la diversité génétique et celle des écosystèmes associés ou créés par l'agriculture.Agroécologie : ensemble de théories et de pratiques agricoles prenant en compte les connaissances de l'écologie, de la science agronomique et du monde agricole.Agroécosystème ou agrosystème ou écosystème cultivé ː écosystème, terrestre ou aquatique, modifié par l’homme afin d'exploiter une part de la matière organique qu'il produit, généralement à des fins alimentaires.Agroforesterie : mode d’exploitation des terres agricoles associant des arbres et des cultures ou de l'élevage afin d'obtenir des produits ou services utiles à l'Homme.Agrosylvopastoralisme : activité de production qui associe pastoralisme et agriculture à un environnement forestier arboricole.Albédo : pouvoir réfléchissant d’une surface, soit la fraction de l’énergie solaire qui est réfléchie vers l’espace.Allèle : se dit de l’une ou l’autre de deux ou plusieurs variantes d’un gène qui ont la même position relative sur les chromosomes homologues et qui sont responsables de caractéristiques différentes.Allélopathie : ensemble des interactions biochimiques positives ou négatives réalisées par les plantes entre elles, ou avec des micro-organismes.Aménité : aspect de l'environnement appréciable et agréable pour l'humanité.Amphibiens : anciennement appelés batraciens, ils forment une classe de vertébrés tétrapodes ayant la particularité d’avoir une vie larvaire aquatique et une vie adulte aérienne.Anaérobie : organisme existant en l’absence du dioxygène, opposé à « aérobie ».Anémochorie : dispersion des plante par le vent.Anémophilie : transport du pollen et fécondation assurés par le vent.Anthropisation : en écologie, transformation de paysages, d’écosystèmes ou de milieux semi-naturels sous l’action de l’homme.Anthropocène : néologisme désignant une nouvelle époque géologique ayant débuté avec la révolution industrielle.Anthropocentrisme : philosophie qui considère l’homme comme l’entité centrale de l’Univers.Antibiotique : substance naturelle, ou produit de synthèse, qui empêche la croissance des bactéries ou les détruit.Apomixie : production de graines identiques à la plante mère sans fécondation. type particulier de parthénogenèse.Aquaponie :  système de production alimentaire durable qui unit la culture de plantes et l'élevage de poissons.Aquifère : sol ou une roche réservoir originellement poreuse ou fissurée, contenant une nappe d'eau souterraine et suffisamment perméable pour que l'eau puisse y circuler librement.Arbre phylogénétique : représentation schématique des relations de parenté entre des groupes d’êtres vivants.Arbre urbain ou arbre en ville : arbre présent en ville, qu'il y soit spontané ou introduit par l'homme et pouvant notamment contribuer à l'amélioration de la qualité de l'air en ville.Archées : microorganismes unicellulaires procaryotes, constitués d'une cellule unique qui ne comprend ni noyau ni organites.ARN : abréviation d'acide ribonucléique.Arrêté préfectoral de protection de biotope ou APPB est un type d'aires protégées en France permettant au préfet de réglementer ou d'interdire certaines activités humaines, dans l'objectif de protéger les milieux de vie d'espèces protégés au niveau national.Association : relations réciproques entre plusieurs individus ou espèces dans un écosystème.Auto écologie : étude des individus pris séparément dans leurs milieux (ou biotope).Autotrophie : production, par un organisme vivant, de toute la matière organique nécessaire pour son propre développement et sa croissance.Auxiliaire des cultures : organisme vivant facilitateur de la production agricole grâce aux services écosystémiques qu'il rend. Il s'agit soit d'antagoniste aux organismes nuisibles des cultures, soit de pollinisateur.== B ==Bande enherbée :  couvert végétal linéaire bordant une parcelle de culture.Bassine : surface en eau lentique, d'origine anthropique.Bassin versant ː l’ensemble de la surface qui reçoit les eaux qui circulent vers un même cours d’eau ou vers une même nappe d’eau souterraine.Battance : caractère d'un sol qui tend à se désagréger et à former une croûte en surface sous l'action de la pluie ou d'un piétinement important.Bien-être animal : problématique concernant l'amélioration de la condition animale quand elle est dégradée par l'utilisation et l'exploitation des animaux d'élevage.Bilan carbone : ensemble de méthodes permettant de mesurer et de suivre la quantité de gaz à effet de serre (GES) qu'une organisation (entreprise, administration publique, association...) émet du fait de son activité.Biocénose : Ensemble des êtres vivants qui coexistent dans le biotope et par leurs interactions constituent un écosystème.Biodégradable : (adjectif) susceptible, grâce à l'action de divers organismes, de se décomposer en éléments divers sans créer d'effets dommageables sur le milieu naturel.Biodisponibilité : degré auquel des substances chimiques présentes dans le sol peuvent être absorbées ou métabolisées ou être disponibles pour différentes espèces vivantes.Biodiversité : variété des formes de vie sur Terre, y compris la diversité des espèces, des écosystèmes et des gènes.Biodiversité cultivée : ensemble des espèces cultivées par les hommes surtout pour l'alimentation humaine et animale mais aussi pour la construction, l'habillement, l'énergie...Biodiversité du sol : variété des formes de vie, animales, végétales et microbiennes présentes dans un sol pour au moins une partie de leur cycle biologique.Biodynamie : système de production agricole basé sur un dogme pseudo-scientifique de l'anthroposophie.Bioéconomie : désigne plusieurs théories et pratiques telles que l'approche économique des comportements biologiques , la gestion des ressources halieutiques commerciales ou plus largement aujourd'hui la somme des activités fondées sur les bioressources.Biofumigation :  pratique agronomique qui consiste à broyer finement un couvert végétal mis en place pendant la période d’interculture, puis incorporer les résidus dans le sol. Cela provoque la libération de composés organiques volatils toxiques pour certains prédateurs. Les plantes principalement utilisées en biofumigation sont des Brassicaceae.Biogéographie : étude de la répartition de la vie sur la Terre par la géographie physique, la pédologie, l'écologie, la bioclimatologie et la biologie de l'évolution.Biomasse : masse totale des organismes vivants dans un biotope.Biome : ensemble d’écosystèmes caractéristiques d’une aire biogéographique.Biométrie : étude quantitative des êtres vivants.Biomimétisme : capacité à reproduire les organismes vivants.Bioremédiation : décontamination de milieux pollués à l'aide de microorganismes capables de détoxifier des éléments.Biosphère : milieux d'existence de l'ensemble des organismes vivants.Bioturbation :  réarrangement physique du matériel pédologique ou des sédiments grâce au remaniements dû à des organismes vivants au niveau du sol ou des fonds marins.Biotiques (facteurs...) : ensemble des interactions du vivant dans un écosystèmeBiotope : lieu de vie défini par des caractéristiques physiques et chimiques relativement uniformes.Bloom planctonique : aussi appelé efflorescence algale ou planctonique, c'est une prolifération relativement rapide de la concentration de quelques espèces d’algues, généralement appartenant au phytoplancton, dans un système aquatique d’eau douce, saumâtre ou salée.Bryophytes : embranchement du règne végétal, les bryophytes (mousses, hépatiques, sphaignes…) ne possèdent pas de véritable système vasculaire.== C ==Calotte glaciaire : masse de glace de la taille d'un continent, qui s’écoulent sous son propre poids. Actuellement existent les calottes du Groenland et de l’Antarctique.Canopée : étage supérieur de la forêt, directement influencée par le rayonnement solaire.Catalyse : élément qui accélère ou ralentit une réaction chimique. C’est en particulier le cas des enzymes.Cerema : le Centre d'études et d'expertise sur les risques, l'environnement, la mobilité et l'aménagement coordonne des moyens à mettre en œuvre.Chaîne alimentaire ou réseau trophique : échanges d'éléments tels que le carbone et l'azote entre les différents être vivants (végétaux autotrophes et les hétérotrophes).Changement climatique (anthropique) : modifications à long terme des températures et des évènements météorologiques sur la Terre, principalement dues aux activités humaines émettrices de gaz à effet de serre.Charbon actif : granulés ou de poudre, issus de la calcination de matériaux carbonés présentant des propriétés d’adsorption importantes notamment vis-à-vis des micropolluants organiques.Chélicérates : groupe d’arthropodes portant des chélicères comprenant les mérostomes (limules) et les arachnides (araignées, scorpions, etc.).Chimiosynthèse : synthèse de substances organiques sous l'effet d'une source d'énergie chimique.Chimiotropisme : croissance d'un organe ou déplacement d'une cellule par rapport à un gradient de concentration chimique.Chlordécone : insecticide organochloré toxique, écotoxique et persistant.Chloroplaste : organite du cytoplasme des cellules eucaryotes réalisant la photosynthèse.Chorologie : étude explicative de la répartition géographique des espèces vivantes et de ses causes.Climat : conditions de l'atmosphère terrestre dans une région donnée pendant une période donnée.Climatoscepticisme : attitude de déni du dérèglement climatique actuel.Climat et santé : un lien étroit existe entre le climat et la santé, le premier influençant la seconde.Climax : état final d'une succession écologique et l'état le plus stable dans les conditions existantes de terrain et du climat.Collapsologie : courant de pensée transdisciplinaire envisageant les risques, causes et conséquences d'effondrements environnementaux et sociétaux de la civilisation industrielle.combustible fossile : combustible riche en carbone hydrocarbure, issu de la transformation lente de matière organique enfouie dans le sol depuis plusieurs millions d'années.Comité de surveillance biologique du territoire ou « CSBT » : organisme chargé d'évaluer et suivre l'état sanitaire et  « phytosanitaire » des végétaux (notamment dans l'agriculture et la sylviculture).Commensalisme : interaction biologique naturelle entre deux êtres vivants dans laquelle l'hôte fournit une partie de sa propre nourriture au commensal.Compétition : rivalité entre les espèces vivantes pour l'accès aux ressources limitées du milieu.Compostage : traitement biologique de déchets organiques par fermentation aérobie permettant d'obtenir du compost.Compte carbone : quota annuel d'émissions de gaz à effet de serre.Conférence des Parties (COP) : Conférence  mondiale pour établir des conventions internationales.Conservation (Biologie de la...) : discipline traitant de la perte, du maintien ou de la restauration de la biodiversité.Conservation ex situ : conservation de la faune et de la flore sauvages hors de son milieu naturel.Conservation in situ : conservation de la faune et de la flore sauvages dans son milieu naturel.Continuité écologique ou connectivité écologique : se définit comme la libre circulation des espèces, une hydrologie proche des conditions naturelles et le bon déroulement du transit sédimentaire dans les cours d’eau.Convergence évolutive : mécanismes évolutifs amenant des espèces à adopter indépendamment plusieurs traits physiologiques, morphologiques, parfois comportementaux semblables en étant soumises à des contraintes environnementales similaires.COP : Conférence mondiale pour l'établissement de conventions internationales : COP 1, COP 2, .....COP 30.Corridor biologique : milieux reliant entre eux différents habitats vitaux pour une espèce ou une population.Corridor climatique : itinéraire possible pour la migration d'une population animale menacée.COS : carbone organique du sol.Couche d'ozone ou ozonosphère : couche située en haute altitude. En absorbant la plus grande partie du rayonnement solaire ultraviolet, elle protège  les êtres vivants et les écosystèmes.Couvert végétal : végétation recouvrant dans un espace donné le sol de manière permanente ou temporaire.Croissance qualitative ou croissance verte : croissance prenant en compte l'économie du social et de l'environnement en faisant intervenir une fonction de production  par des ressources naturelles à côté des facteurs produits par l’homme.Culture intermédiaire : culture implantée entre la récolte d’une culture principale et le semis de la culture suivante pendant une période plus ou moins longue appelée «interculture».Culture intermédiaire piège à nitrates (CIPAN) :  culture dérobée dont le rôle principal est de capter une partie du reliquat mobile azoté restant de la culture précédente, afin d'éviter sa lixiviation.Cycles biogéochimiques : processus de transformation cyclique d'un élément chimique entre les différentes couches de la biosphère.== D ==Charles Darwin : naturaliste et paléontologue dont les travaux sur l'évolution des espèces vivantes sur terre ont révolutionné la biologie.Débit d’un cours d’eau : volume d'eau qui traverse un point donné du cours d'eau dans un laps de temps déterminé.Débit d’étiage : débit minimum d'un cours d'eau en période de basses eaux.Débit réservé :  débit minimal éventuellement augmenté des prélèvements autorisés sur un tronçon de cours d’eau, garantissant en permanence la vie, la circulation et la reproduction des espèces présentes.Décarbonation ou décarbonisation : bilan de la réduction progressive de la consommation de dioxyde de carbone et de méthane.Déméter peut faire référence à :Déméter, déesse de l'agriculture et des moissonsDemeter, association gérant une marque de certification  de l'agriculture biodynamique..Déméter : cellule de la gendarmerie nationale française en charge de la lutte contre l'AgribashingDéni du changement climatique : attitude climatosceptique.Désertification : dégradation des sols ayant pour origine des changements climatiques et/ou les conséquences des activités humaines. Il se traduit par une dégradation des terres menant à un biome de type désertique.Dessication : élimination de l’eau contenue dans une substance.Déterminisme : théorie selon laquelle chaque événement est déterminé par les événements passés conformément aux lois de la nature.Dette écologique : concept qui désigne une forme de dette non monétaire. Elle peut être liée aux menaces qui vont peser sur les générations futures ou à la détérioration de zones géographiques.Diapause : diminution des activités métaboliques d'un organisme vivant..Développement durable : concept visant à répondre aux besoins du présent sans compromettre la capacité des générations futures à répondre aux leurs.Diatomite : roche sédimentaire formée par l’accumulation en grande quantité de diatomées fossilisées.Durabilité : configuration de la société humaine qui lui permet d'assurer sa pérennité.Dynamique des populations : fluctuation dans le temps du nombre d'individus au sein d'une population d’êtres vivants sous l'influence des variations de l'environnement.== E ==Eau souterraine : eau située sous la surface du sol.Éco-anxiété ou écoanxiété : forme d'anxiété liée à un sentiment d'impuissance face aux problématiques environnementales contemporaines (dérèglement climatique, destruction des écosystèmes, multiplication des catastrophes naturelles, etc.).Écoblanchiment ou greenwashing : méthode de marketing consistant à communiquer auprès du public en utilisant l'argument écologique de manière trompeuse pour améliorer son imageÉcobuage : pratique agricole ancestrale consistant à brûler la végétation présente avant la mise en culture d'une terre.Écocide : destruction d'un écosystème du fait d'activités réalisées par des hommes.Écofascisme ou écototalitarisme ou écoautoritarisme : qualificatif idéologique parfois utilisé pour parler des lois environnementales promulguées par l'Allemagne nazie dans les années 1930 et  servant aussi d'insulte politique contre les mouvements écologistes pour leur autoritarisme supposé. Ce terme peut également désigner certaines dérives antihumanistes et réactionnaires de certains courants de l'écologie politique. Il peut enfin qualifier certains mouvements et figures d'extrême droite se réappropriant les thèmes de l'écologie et de la nature.Écologie de la restauration : science qui sert de base théorique aux pratiques de restauration ou de réhabilitation des écosystèmes.Écologie du paysage : étude des processus écologiques à l'échelle des paysages.Écologie des populations ou démécologie : étude des variations de la taille des populations et de l’occurrence de caractéristiques génétiques, phénotypiques, comportementales et culturelles au sein des populations.Écologie ou écologisme : différence des sens de deux termes utilisés en écologieÉcologie des transports : science du système homme-transport-environnement.Écologie fonctionnelle : centrée sur les rôles et fonctions que les individus et espèces jouent dans leur biocénose, elle s'intéresse moins à la diversité des espèces qu'à la manière dont celles-ci interagissent.Écologie politique : ensemble de courants qui traite des enjeux écologiques dans l'action politique et dans l'organisation sociale.Écologie urbaine : étude de la ville comme écosystème.Écologisme ou environnementalisme :  idéologie ou philosophie dont l'orientation de l'activité politique ou parapolitique vise au respect, à la protection, à la préservation ou à la restauration de l'environnement.Économie écologique : branche de l'économie en interface avec l'écologie, traitant de l'interdépendance et de la coévolution entre les sociétés humaines et les écosystèmes dans le temps et l'espace.Économie verte : activité économique permettant une amélioration du bien-être humain et de l’équité sociale tout en réduisant de manière significative les risques environnementaux et la pénurie de ressources.Écophysiologie : étude des réponses comportementales et physiologiques des organismes à leur environnement.Écophyto : plan gouvernemental français de réduction d'usage, agricole et non agricole, des pesticides.Écosphère : ensemble des écosystèmes dans lesquels plusieurs niveaux interagissent les uns avec les autres.Écosystème : ensemble formé par une association d’êtres vivants (biocénose) et son environnement (biotope) biologique, géologique, édaphique, hydrologique, climatique, etc.Écotoxicologie : discipline scientifique récente située à l'interface entre l'écologie et la toxicologie.Écotype : variété ou population génétiquement distincte d'une espèce donnée qui présente des caractéristiques adaptées à des habitats différents.Effets du changement climatique sur la santé humaine : les maladies liées à l'alimentation et à l'accès à l'eau, ainsi que les zoonoses et les maladies cardiovasculaires et rénales augmentent avec la chaleur.Effondrement écologique ou « collapsus écologique » : scénario de crise écologique majeure caractérisé par un effondrement brutal des écosystèmes.Élévation du niveau de la mer ou remontée du niveau marin : phénomène mondial résultant du réchauffement climatique estimé à plus de 3,5 mm par an.Empreinte carbone ou contenu carbone :  mesure des émissions de gaz à effet de serre d'origine anthropique, c'est-à-dire imputées aux activités humainesEndémique (adjectif) : espèce présente dans une aire de répartition très limitée.Énergie fossile : combustible riche en carbone, par exemple issu de la transformation lente de matière organique enfouie dans le sol depuis plusieurs millions d'années.Énergie propre : énergie renouvelables telles que l'énergie éolienne, l'énergie hydroélectrique, l'énergie solaire et la géothermie.Énergie renouvelable : source d'énergie qui est régénérée naturellement et rapidement, comme l'énergie solaire, éolienne, hydraulique, géothermique, etc.Enherbement : moyen de protéger les sols de l'érosion et des effets délétères d'une exposition directe à la pluie, au gel et aux UV solaires.Environnement : ensemble des conditions naturelles (physiques, chimiques, biologiques) et culturelles (sociologiques) susceptibles d’agir sur les organismes vivants et les activités humaines.Environnementalisme :  idéologie ou philosophie dont l'orientation de l'activité politique ou parapolitique vise au respect, à la protection, à la préservation ou à la restauration de l'environnement.Éolienne : dispositif transformant l'énergie cinétique du vent en énergie mécanique, dite énergie éolienne, qui est ensuite généralement transformée en énergie électrique.Épidémiosurveillance : surveillance préventive des maladies infectieuses humaines, animales et/ou végétales sur un territoire.Épigénétique : étude des mécanismes modifiant de manière réversible, transmissible et adaptative l'expression des gènes sans en changer les séquences.Épiphyte : espèce qui croît sur d'autres plantes sans en tirer sa nourriture.Épisode cévenol ou pluies cévenoles :  pluies particulièrement intenses et fréquentes affectant principalement les Cévennes.Érosion du littoral : ou érosion côtière : recul du trait de côte, des couloirs de ravinement, qui se traduit par l'abaissement des plages et des petits fonds, des éboulements rocheux, des glissements de matériaux ponctuels ou en masse, suivant la hauteur, la pente et la nature de la falaise.Espèce envahissante ou espèces invasive ː espèce déplacée par l‘Homme en dehors de son aire de distribution naturelle et qui prolifère dans son nouvel environnement.Espèce ingénieure : espèces qui par leur seule présence et activité modifient significativement à fortement leur environnement.Espèce introduite : population identifiée présente ou maintenue présente artificiellement, en cours de naturalisation ou déjà naturalisée dans un territoire donné, ayant été importée par une intervention humaine (délibérée ou non).Espèce naturalisée : espèce introduite dans un nouvel environnement et qui s’y reproduit.Espèce pionnière : une des premières formes de vie qui colonisent ou recolonisent un espace écologique donné.Espèce protégée : espéce animale ou végétale menacée d'extinction.Étage benthique : espace vertical du domaine marin où les conditions, en fonction de la situation par rapport au niveau de la mer, sont sensiblement constantes.Éthique environnementale :  branche de la philosophie traitant de la protection de l'environnement.Étrépage : décaissage et exportation du sol superficiel et de la végétation, pratiqué en gestion des milieux et, autrefois, en agriculture.Eucaryotes : organismes unicellulaires ou multicellulaires se caractérisant par la présence d'un noyau et généralement d'organites spécialisés dans la respiration, en particulier mitochondries chez les aérobies mais aussi hydrogénosomes chez certains anaérobies.Eutrophisation : enrichissement de l’eau en matières organiques provoquant une prolifération végétale et bactérienne entraînant une désoxygénation prononcée de l'eau.Évaluation environnementale : évaluation d'un projet, d'un lieu, d'une stratégie, d'un plan, programme ou schéma au regard de ses conséquences sur l'environnement. Elle comprend donc une évaluation de la composition et des conditions de l'environnement biophysique (la part abiotique de l'environnement) et de l'environnement humain et non-humain (le vivant).Évaluation des écosystèmes pour le millénaire (EM), en anglais Millennium Ecosystem Assessment (MEA) : évaluation de l’ampleur et des conséquences des modifications subies par les écosystèmes dont dépend notre survie et le bien-être humain demandé par l'Organisation des Nations unies.Exploitation minière : activité d'extraction de la terre de métaux précieux ou d'autres matériaux utiles.Extinction des espèces : disparition totale d'une espèce ou d'un groupe de taxons, réduisant ainsi la biodiversité.Extinction massive ou crise écologique : événement relativement bref à l’échelle des temps géologiques au cours duquel au moins 50 % des genres et 10 % des familles d'espèces animales et végétales présentes sur la Terre et dans les océans disparaissent.Extinction Rebellion mouvement social écologiste international qui revendique l'usage de la désobéissance civile.== F ==Facteurs anthropiques ; transformation de l'environnement liée la présence des êtres humains.Facteur de transcription : protéine nécessaire à l'initiation ou à la régulation de la transcription d'un gène chez tous les organismes vivants.Facteur limitant : facteur écologique qui par son absence ou sa quantité réduite au-dessous d’un minimum critique, ou lorsqu’il excède le niveau maximal tolérable, va exercer un rôle limitant sur l’espèce.Faune : ensemble des espèces animales présentes dans un espace géographique ou un écosystème déterminé.Faune sessile : Organismes très souvent marin, vivant seuls ou en colonies et fixés généralement sur une roche.Fédération nationale des syndicats d'exploitants agricoles (FNSEA) : syndicat majoritaire des agriculteursFixation du carbone en C4 : un des trois modes de fixation du carbone des plantes. Adapté aux milieux chauds ou arides, permet de limiter les pertes en eau, très commun chez les graminées tropicales.Flore : ensemble des espèces végétales présentes dans un espace géographique ou un écosystème déterminé.FNSEA (Fédération nationale des syndicats d'exploitations agricoles) : syndicat majoritaire des agriculteursFrance Nature Environnement (FNE) : regroupement de très nombreuses associations œuvrant pour la sauvegarde de l'environnement.Forêt urbaine : forêt ou boisements poussant dans une aire urbaine permettant éventuellement les effets délétères du réchauffement climatique.Fréquence allélique : fréquence d'un allèle d’un variant dans une population== G ==Gaz à effet de serre ou (GES) (émissions de...) : gaz (comme le dioxyde de carbone, le méthane, etc.) qui contribuent à l'effet de serre et au réchauffement de la planète.Génétique : étude les lois de l'hérédité.Génétique des populations : étude de la distribution et des changements de la fréquence des versions d'un gène (allèles) dans les populations d'êtres vivants.Génétique moléculaire : branche de la biologie et de la génétique, qui consiste en l’analyse de la fonction des gènes au niveau moléculaire.Génie génétique : Technique consistant à enlever, modifier ou ajouter des gènes à une molécule d'ADN [d'un organisme] de sorte à changer l'information qu'elle contient.Génie écologique : conduite de projets qui, dans sa mise en œuvre et son suivi, applique les principes de l'ingénierie écologique et favorise la résilience des écosystèmes.Génotype : ensemble de l’information génétique d’un individu.Géonomie : science des rapports entre les sociétés humaines et leur environnement naturel.Germinale (lignée) : ensemble des cellules allant des cellules souches aux gamètes.GIEC : Groupe d'experts intergouvernemental sur l’évolution du climat : organisme intergouvernemental chargé d'évaluer l'ampleur, les causes et les conséquences du changement climatique en coursGéotropisme : voir gravitropisme.Gestion des déchets : collecte, négoce, courtage, transport et traitement des rebuts. Réutilisation ou élimination des déchets principalement issus des activités humaines.Graminées ou Poaceae : très importante famille de plantes monocotylédones qui regroupe les espèces communément appelées herbes et céréales.Glycolyse : série de réactions catalysées par des enzymes qui dégradent une molécule de glucose en deux molécules de pyruvate.Glycoprotéines : protéines sur lesquelles des sucres sont ajoutées, sur les membranes cellulaires, elles favorisent les interactions entre les cellules.Gravitropisme ou géotropisme : mouvement effectué par une plante soit vers le haut (tiges) soit vers le bas (racines).Greenwashing : méthode de marketing consistant à communiquer auprès du public en utilisant l'argument écologique de manière trompeuse pour améliorer son image.Goutte froide ou goutte d’air froid : en météorologie, volume limité d’air froid entouré d'isothermes fermées.== H ==Habitat : ensemble d'éléments constituant l'environnement biophysique et offrant les ressources naturelles suffisantes pour permettre à une population d'une espèce de vivre et se reproduire normalement sur ce territoire.Haute qualité environnementale : concept environnemental français enregistré comme marque commerciale et accompagné d'une certification « NF Ouvrage Démarche HQE » par l'AFNOR concernant les bâtiments.Hémiparasite : plante puisant sa sève brute dans une plante hôte, mais produisant sa sève élaborée grâce à la photosynthèse.Hétérotrophie : mode de nutrition des organismes vivants qui se nourrissent de matière organique.Holisme : tendance à constituer des ensembles qui sont supérieurs à la somme de leurs parties, au travers de l'évolution créatrice.Holobionte ou supraorganisme : assemblage d'espèces hôtes (animal, végétal, fongique…) et d'autres espèces comme les micro-organismes) vivant à l'intérieur de cet organisme, sur son enveloppe ou à proximité.HRI : indicateur de l'utilisation de produits phytosanitairesHumus : couche supérieure du sol résultant de la décomposition de la matière organique.Hydrobiologie ; science qui étudie la faune aquatique peuplant les rivières et les cours d'eau.Hydrochorie : transport et dispersion des graines des végétaux ou des diaspores par l'intermédiaire de l'eau.Hydrologie : science concernant tous les aspects du cycle de l'eau, c'est-à-dire les échanges entre la mer, l'atmosphère , la surface terrestre (limnologie) et le sous-sol (hydrogéologie).== I ==Indicateur biologique : espèce végétale, fongique, animale ou bactérienne ou groupe d'espèces dont la présence ou l'état renseigne sur certaines caractéristiques écologiques de l'emplacement où elle se trouve.Ingénierie écologique : ensemble des connaissances scientifiques, des techniques et des pratiques qui prend en compte les mécanismes écologiques, appliqué à la conception et à la réalisation d’aménagements ou d’équipements, et qui est propre à assurer la protection de l’environnement.Insolation : quantité d'énergie solaire reçue par unité de surface (proche d'ensoleillement).Interactions mutualistes : Interaction entre deux espèces à bénéfice réciproque (proche de symbiose).Interférons : protéines faisant partie des cytokines, elles ont une action contre tout agent pathogène (virus, bactérie et parasite) et stimulent les cellules immunitaires.Interspécifique : (adjectif) relations qui s’établissent entre des individus appartenant à des espèces différentes.Intraspécifique : (adjectif) relations qui s’établissent entre des individus appartenant à une seule et même espèce.Ionosphère : haute atmosphère terrestre où l’air est partiellement ionisé.IPBES (en anglais) Plateforme Intergouvernementale Scientifique et Politique sur la Biodiversité : groupe international d'experts sur la biodiversitéIsotope : élément chimique ayant le même nombre de protons et d’électrons mais un nombre différent de neutrons.== L ==Lessivage ou Lixiviation : processus vers le bas des minéraux du sol entraîné par la percolation de l’eau.Liste rouge des espèces menacées de l'UICN : inventaire mondial le plus complet de l'état de conservation global des espèces végétales et animales.Lithosphère : enveloppe rigide de la surface de la Terre. Elle comprend la croûte terrestre et une partie du manteau supérieur.Lombricompostage : amendement organique de qualité agronomique, qui se produit par la transformation de fumiers ou de déchets organiques domestiques par des vers du fumier.Low-tech : techniques durables, simples, appropriables, résilientes produisant des objets facilement réparables et adaptables.Lutte biologique : méthode de lutte contre les nuisibles tels que les ravageurs des cultures, les maladies, ou les (plantes adventices) au moyen d'organismes vivants antagonistes.== M ==Macrophagie : mode de nutrition d’un organisme vivant qui se nourrit de proies de grande taille par rapport à lui.Magnétosphère : région extérieure à la Terre où est localisé le champ magnétique engendré par cette planète.Matériaux biosourcés : matériaux issus de la matière organique renouvelable (biomasse), d'origine microbienne, végétale, fongique ou animale.Matière inorganique : non formée d'un squelette carboné.Méga-bassine : retenue d'eau destinée au stockage agricole de l'eau.Méthanisation : processus biologique de biodégradation anaérobie des matières organiques.Microbiote : ensemble des micro-organismes — bactéries, microchampignons, protistes — vivant dans un environnement spécifique, le microbiome, chez un hôte (animal végétal ou dans le sol ou dans l'air.Microplastique : petites particules de matière plastique qui s'accumulent dans l'environnement.Microphage : se dit des organismes qui se nourrissent de très petites proies.Microplastique : petites particules de matière plastique qui s'accumulent dans l'environnement.Module : Moyenne des débits annuels moyens d’un cours d’eau sur une période d’observation exprimé en m3/sMonétarisation de la nature : fait de donner une valeur monétaire à la nature, à l'environnement ou à un de ses éléments écologiquesMycorhize : association symbiotique entre les racines des plantes et des champignons du sol.Mycoremédiation : capacité de certains champignons de récupérer certaines substances polluantes dans les sols.== N ==Nappe phréatique ː expression désignant toutes les nappes d'eau souterraines.Naturalisée : se dit d'une espèce exotique dont les populations introduites dans un milieu se reproduisent de façon sexuée.Néguentropie : entropie négative », variation générant une baisse du degré de désorganisation d'un système.Neutralité carbone : état d'équilibre à l'échelle mondiale entre les émissions de gaz à effet de serre (GES) d'origine humaine et leur retrait de l'atmosphère par l'homme ou de son fait.Nimby : acronyme de l'expression anglaise «'Not In My BackYard», signifiant littéralement « pas dans mon arrière-cour ». Il décrit l'opposition de résidents à un projet local d’intérêt général dont ils considèrent qu’ils subiront des nuisances.Niveaux trophiques ! rang occupé par un être vivant au sein d’une chaîne alimentaire.NODU ou \"NOmbre de Doses Unités\" : indicateur de suivi du recours aux produits phytosanitaires.Noyau (biologie) : dans une cellule vivante, le noyau contient le génome, constitué d’ADN.Noyau (Terre) : constitué de fer et de nickel, situé sous le manteau de silicates.Nucléotide : èlément de base d’un acide nucléique tel que l’ADN ou l’ARN.== O ==Observatoire national de la biodiversité (ONB) : dispositif partenarial piloté et animé par l'Office français de la biodiversité.Observatoire national de la mer et du littoral (ONML) : Observatoire de l'environnement  collectant et centralisant des données géographiques, démographique, climatique, économiques, sociologiques et environnementales sur la mer et le littoral.Observatoire national sur les effets du réchauffement climatique (ONERC) : organisme français dont le but est d'aider à prendre en compte les problèmes liés à une aggravation du réchauffement climatique.Office français de la biodiversité (OFB) : établissement public de l'État chargé de surveiller les milieux terrestres, aquatiques et marins, pour préserver la biodiversité, ainsi que d'assurer la gestion équilibrée et durable de l'eau en coordination avec la politique nationale de lutte contre le réchauffement climatique.Oligotrophe : contraire d'un milieu eutrophe, milieu particulièrement pauvre en éléments nutritifs, généralement une masse d'eau, pauvre en nutriments.Osmotrophie : mode d’alimentation consistant à se nourrir à partir de substances dissoutes. Elle est assurée par diffusion d’ions ou de petites molécules au travers de la membrane cytoplasmique.Oxysulfure de carbone ou sulfure de carbonyle (COS) : gaz toxique et inflammable, principalement émis dans l'environnement par les sols.== P ==Paléoécologie : étude des relations entre les êtres vivants fossiles avec leur milieu de vie, sous les aspects physico-chimiques (paléobiotope) aussi bien que biologiques (paléobiocénose).Parasitoïde : organisme se développant sur ou dans un autre organisme en deux phases : d’abord biotrophe puis prédateur, amenant à la mort finale de l’hôte.Parc éolien ou centrale éolienne ou ferme éolienne : ensemble d'éoliennes installées dans le but de produire de l'électricité.Pêche durable : application à la pêche des principes du développement durable.Permafrost ou pergélisol : terme géologique désignant un sol dont la température se maintient en dessous de 0 °C pendant plus de deux ans consécutifs. Il occupe plus de 20% de la surface terrestre.PFAS ou substances per- et polyfluoroalkylées : polluants persistants dans l’environnement, autrefois aussi dénommées composés perfluorés. Ce sont des composés organofluorés synthétiques.Phénologie : étude de l'apparition d'événements périodiques dans l'écosystème, déterminée par les variations saisonnières du climat.Phénotype : ensemble des traits observables d'un organisme. Opposé à génotype.Phéromone ou phérormone ou encore phérohormone : substance chimique émise par la plupart des animaux et certains végétaux qui agit auprès des individus d'une même espèce.Photosynthèse : processus permettant à des organismes de fabriquer de la matière organique en utilisant la lumière, l'eau et le dioxyde de carbone.Phototropisme : capacité d'un organisme vivant à s'orienter par rapport à la lumière.Phylogénie : étude des liens de parenté entre les êtres vivants et ceux qui ont disparu permettant de retracer les principales étapes de l’évolution des organismes depuis un ancêtre commun et d’établir les relations de parentés entre les êtres vivants.Phytoplancton : organismes généralement unicellulaire s autotrophes vivant dans les eaux douces, saumâtres et salées. Il comprend des protistes, comme des dinoflagellés, et des bactéries telles que les cyanobactéries (anciennement « algues bleu-vert »).Phytoremédiation : capacité de certaines plantes à capturer par leurs racines des substances chimiques polluantes dans les sols et à  les bloquer dans leurs cellules. Ces plantes servent ainsi de dépoluants.Phytosociologie : processus permettant à des organismes vivants tels que les végétaux de fabriquer de la matière organique grâce à la lumière, l'eau et le dioxyde de carbone.Plaine d'inondation : zone de terre généralement plate, proche des rivières, fleuves ou tout cours d'eau, qui peut subir des inondations.Planification écologique : processus multidisciplinaire visant à intégrer la préservation et la restauration des écosystèmes naturels.Plante pionnière :une des premières formes de vie qui colonisent ou recolonisent un espace écologique donné.Plateforme intergouvernementale scientifique et politique sur la biodiversité et les services écosystémiques (IPBES) : groupe international d'experts sur la biodiversité.Pollution : introduction dans un milieu de substances, de chaleur ou de bruit ayant un impact néfaste sur l'environnement, la santé humaine ou les écosystèmes.Pollution de l'air ou pollution atmosphérique : altération de la qualité de l'air pouvant être caractérisée par des mesures de polluants chimiques, biologiques ou physiques. Elle peut avoir des conséquences préjudiciables sur la santé humaine, sur les êtres vivants, sur le climat et sur les biens matériels.Pollution des sols : toutes les formes de pollution, touchant n'importe quel type de sol, notamment agricole, forestier et urbain. En particulier les résidus toxiques liés à des activités agricoles ou industrielles.Population : ensemble des individus d'une même espèce occupant le même milieu.Prédation : consommation partielle ou entière d'une ou plusieurs espèces par une autre espèce.Procaryote : micro-organisme unicellulairequi ne comporte pas de noyau, et presque jamais d'organites membranés . Les procaryotes actuels sont les bactéries et les archées.Production primaire : production de matière organique végétale (biomasse), issue de la photosynthèse, par des organismes autotrophes, dits producteurs.Production secondaire : production de matière organique par des organismes  qui se nourrissent de matière végétale.Produit phytosanitaire : ou produit de protection des plantes ou produit phytopharmaceutique (PPP) : substance, ou mélange de substances, utilisé pour protéger les plantes cultivées et les produits agricoles stockés contre les différents types de bioagresseurs.Protection des espèces en voie de disparition : ou conservation de la vie sauvage ou conservation) ou conservation de la faune et de la flore :  pratique de la protection des espèces animales et végétales sauvages.Pyramide écologique ou pyramide trophique : représentation graphique montrant des rapports entre différentes catégories d'espèces correspondant à différents niveaux trophiques.== R ==Recyclage : processus de transformation des déchets en matériaux réutilisables afin de réduire la consommation de ressources naturelles.Réduction des émissions : action de diminution de gaz à effet de serre dues à la déforestation et à la dégradation des forêtsRéensauvagement : soit la réimplantation d'espèces animales disparues depuis plusieurs siècles ou millénaires, soit simplement l'absence d'intervention humaine, dans une région donnée, ou encore sur une surface très limitée.Régime hydrologique : le régime d'un cours d'eau désigne les variations saisonnières de son débit, c’est-à-dire l’alternance entre les basses eaux et les hautes eaux.remembrement (rural) : constitution d'exploitations agricoles d’un seul tenant sur de plus grandes parcelles afin de faciliter l'exploitation des terres.Reproduction végétative : mode de multiplication permettant aux organismes végétaux de se multiplier sans reproduction sexuée. On parle aussi de reproduction clonale.Réseau trophique : ensemble systèmes par lesquelles l'énergie et la biomasse circulent  entre les différents niveaux de la chaîne alimentaire.Résilience écologique : capacité d’un écosystème à se régénérer et à revenir à l’état avant perturbation.Restauration écologique : ensemble de pratiques sur des milieux dégradés (naturels, semi-naturels, industriels ou urbains) avec l'objectif d'y restaurer la biodiversité, le bon état écologique, un paysage de qualité ou un état disparu.régime hydrologique : le régime d'un cours d'eau désigne les variations saisonnières de son débit, c’est-à-dire l’alternance entre les basses eaux et les hautes eaux.Retenue de substitution ou méga bassine : retenue d'eau destinée au stockage agricole de l'eau.Rhizobium : bactéries aérobies du sol symbiotiques avec les Fabaceae. Elles se trouvent sur les racines formant des nodosités où elles fixent et réduisent l’azote atmosphérique essentiel pour la croissance des végétaux.== S ==Sans labour Technique de travail du sol favorisant l'agriculture de conservation.Saprophyte ou saprobionte : qui tire les substances qui lui sont nécessaires des matières organiques en décomposition. Champignons saprophytes.Sapromasse ou nécromasse ː masse de matière organique morte présente dans une parcelle, un volume ou un écosystème donné.Saproxylation : décomposition du bois par des organismes saproxyliques ou xylophages.Sélection naturelle : théorie de Darwin sur l'évolution, selon laquelle l'élimination naturelle des individus les moins aptes permet  à d'autres espèces de se perfectionner au cours des générations successives.Semis direct ː technique de semis sans labour.Service écosystémique : services rendus par les écosystèmes dont certains sont vitaux pour de nombreuses espèces ou groupes d'espèces.Shift Project (abrégé « TSP » ou « le Shift ») : association française ayant pour objectif l'atténuation du changement climatique et la réduction de la dépendance de l'économie aux énergies fossiles, particulièrement au pétrole.Sixième extinction :ou extinction de l'Holocène : nom donné à l'extinction massive et étendue des espèces durant l'époque contemporaine.Sociobiologie : étude des fondements biologiques des comportements sociaux.Solastalgie : forme de souffrance et de détresse psychique ou existentielle causée par la conscience des changements environnementaux en cours.(Les) Soulèvements de la terre : mouvement écologiste radical et contestataire français.Spéciation : formation, au cours de l'évolution biologique, d'espèces distinctes, génétiquement isolées les unes par rapport aux autres.Strate (botanique) ou étage, strate végétale : niveaux d'étagement vertical d'un peuplement végétalStratégie nationale bas carbone (SNBC), ou « stratégie nationale de développement à faible intensité de carbone » : feuille de route pour la France qui vise a assurer la transition énergétique.Stratégie nationale pour la biodiversité : déclinaison de la stratégie nationale de développement durable (SNDD), devant répondre à des enjeux locaux comme aux orientations de la Convention pour la diversité biologique.Sub-spontanées : espèces cultivées se reproduisant de façon de façon autonome par multiplication végétative à partir d'un pied mèreSubstance active : substance ou micro-organisme exerçant une action sur ou contre des organismes nuisibles.Substances per- et polyfluoroalkylées ou PFAS : microplastiques classés comme \"polluants éternels\".Sulfure de carbonyle (COS) : gaz toxique et inflammable, principalement émis dans l'environnement par les sols.Superorganisme : organisme composé de nombreux individus, en général une unité d'animaux eusociaux, où la division du travail est hautement spécialisée et où les individus isolés ne sont pas aptes à vivre par eux-mêmes sur de longues périodes.Symbiose : association durable entre deux organismes vivants permettant un bénéfice réciproque.Synécologie : étude des relations entre les populations d'espèces vivantes.Systémique : qualifie un produit qui agit sur tous les organes d'une plante.Systématique ; Science des classifications des êtres vivants, des taxons.== T ==Taxe carbone : taxe environnementale sur les émissions de dioxyde de carbone qui vise à réduire celles-ci, dans le but de contrôler le réchauffement climatique.Taxinomie ou taxonomie : branche des sciences naturelles ayant pour objet l'étude de la diversité du monde vivant en décrivant en termes d'espèces les organismes vivants et à les organiser en catégories hiérarchisées appelées taxons.TFA ou acide trifluoroacétique : acide carboxylique fort acide carboxylique fort polluant des eaux.Thermocline : dans un lac ou dans l’océan, c’est une zone de transition thermique rapide entre les eaux superficielles (généralement plus chaudes et oxygénées) et les eaux profondes (généralement plus froides et anoxiques)..The Shift Project : think tank visant à réduire l'utilisation du carbone.Traité international sur les ressources phytogénétiques pour l'alimentation et l'agriculture (TIRPAA) : accord sur les droits à l'utilisation des ressources génétiques des espèces végétales mondiales.Trame verte et bleue : un réseau formé de continuités écologiques terrestres et aquatiques.Transition écologique : ensemble de principes et de pratiques provenant des observations d'individus, de groupes, de villages, villes ou communes, ayant travaillé sur les problématiques de résilience, d'économie en boucle et de réduction des émissions de CO2.Travail superficiel du sol ː permet de préparer le sol pour les semis sans perturber profondément la vie du sol.== U ==Upwelling : remontée vers la surface des eaux froides profondes, le long de certains littoraux océaniques.== V ==Variabilité : indicateur de dispersion mesurant la variabilité des valeurs d’une série statistique.Principalement la variance, l'écart-type et l'écart interquartile.Végétal local est une espèce indigène ou un taxon, ou un groupement végétal dont la présence est ancienne. C'est aussi une marque développée pat l'OFB.Verdissement dans le sens de greenwashing : méthode de marketing consistant à communiquer auprès du public en utilisant l'argument écologique de manière trompeuse afin d'améliorer son image.Vermicompostage :  amendement organique de qualité agronomique produit par la transformation de fumiers ou de déchets organiques domestiques par des vers du fumier.== X ==Xérophyte : plante adaptée à un milieu aride.Xylophage : organisme vivant se nourrissant de bois, lignivore étant un terme plus générique.== Z ==ZNIEFF : zone naturelle d'intérêt écologique, faunistique et floristique est un espace naturel inventorié en raison de son caractère remarquableZone à défendre (ZAD) : néologisme militant pour désigner des espaces occupés illégalement par des activistes dans le but de s'opposer à des projets de construction jugés néfastes pour l'environnement.Zones humides : terrains, exploités ou non, habituellement inondés ou gorgés d'eau douce, salée ou saumâtre de façon permanente ou temporaire, ou dont la végétation, quand elle existe, y est dominée par des plantes hygrophiles pendant au moins une partie de l'année.Zone hyporhéique : ensemble des sédiments saturés en eau, situés au-dessous et à côtés d'un cours d'eau, contenant une certaine quantité d'eau de surface.Zone inondable :  zone sujette à la submersion.Zone naturelle d'intérêt écologique,  faunistique et floristique (en abrégé ZNIEFF) est un espace naturel inventorié en raison de son caractère remarquable.Zoogéographie : étude de la répartition des espèces animales sur la Terre.== Voir aussi ===== Articles connexes ===Glossaire de géographieGlossaire de géologieGlossaire de botaniqueGlossaire de biologie cellulaire et moléculaireConvention sur la diversité biologiqueÉcologie politiqueÉducation à l'environnement et au développement durableListe des ministres de l'EnvironnementOrganisation mondiale de l'environnementPolitique climatiqueAmericana - Forum sur l'environnement et Salon international des technologies environnementalesÉcologie intégrale=== Liens externes ===Dictionnaire d’agroécologie. Véronique Batifol, Nathalie Couix, Simon Giuliano, Marie-Benoît Magrini, coord., 2024. Versailles, éditions Quæ, 228 pGlossaire pour comprendre l’écologie et la natureEncyclopédie de l'environnement GlossaireVocabulaire Les mots de l'écologieVocabulaire de l'environnement (liste de termes, expressions et définitions adoptés)(en) Glossary of environmental terms=== Bibliographie ===Sylvie Brunel, Jean-Robert Pitte, Le ciel ne va pas nous tomber sur la tête, JC Lattès, 2010, 352 p. (ISBN 978-2-7096-3561-5)Bonneuil C.et Lehn J.B., L'évènement anthropocène La Terre, l'histoire et nous, Paris, Seuil, 2013Valentin Hammoudi, Le pouvoir des plantes, Paris, Humenscience, 2023, 250 p. (ISBN 979-1-0403-0015-1)Christian Lévèque, L'écologie est-elle encore scientifique, Quae, 2013, 144 p. (lire en ligne)Christian Lévèque, Le double visage de la biodiversité, Paris, L'Artilleur, 2023, 287 p. (ISBN 978-2-8100-1117-9)Marc André Selosse, La Symbiose : structures et fonctions, rôle écologique et évolutif, Paris, Vuibert, 2000, 154 p. (ISBN 2-7117-5283-6)Marc André Selosse, Jamais seul : ces microbes qui construisent les plantes, les animaux et les civilisations, Arles, Actes Sud, 2017, 368 p. (ISBN 978-2-330-07749-5)Marc André Selosse, L’Origine du monde : une histoire naturelle du sol à l’intention de ceux qui le piétinent, Arles, Actes Sud, 2021, 480 p. (ISBN 978-2-330-15267-3).== Références ==(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Glossary of ecology » (voir la liste des auteurs). Portail de la linguistique   Portail de l’écologie   Portail de l’environnement   Portail de la biologie   Portail du changement climatique"
        },
        {
            "pageid": 16150694,
            "ns": 0,
            "title": "Arbre aux quarante fruits différents",
            "content": "L'arbre aux quarante fruits est un arbre créé par le professeur Sam Van Aken de l'université de Syracuse (État de New-York) par la technique de la greffe.Chaque arbre produit quarante variétés de fruits à noyau, du genre Prunus, mûrissant tour à tour de juillet à octobre aux États-Unis,.== Historique ==Sam Van Aken est professeur agrégé de sculpture à l'Université de Syracuse.C'est un artiste contemporain qui travaille au-delà de la création artistique traditionnelle et développe de nouveaux projets dans les domaines de la communication, de la botanique et de l'agriculture.Aken est artiste en résidence en 2018 au Centre artistique McColl (McColl Center for Art + Innovation) à Charlotte, en Caroline du Nord.Sa famille est Pennsylvania Dutch, il a grandi dans la ferme familiale.En 2008, alors qu'il recherchait des spécimens pour créer un arbre à fleurs multicolores dans le cadre d'un projet artistique, Van Aken a acquis le verger de 3 acre (1,21 ha) de la Station agricole expérimentale de l'État de New York, qui fermait ses portes en raison de réductions de financement,. Il a commencé à greffer des bourgeons de certaines des plus de 250 variétés patrimoniales cultivées sur place, certaines uniques, sur un porte-greffe. Au cours d'une période d'environ cinq ans, l'arbre a accumulé des branches de quarante arbres \"donneurs\" différents, chacun avec un fruit différent, dont des variétés d'amande, abricot, cerise, nectarine, pêche et prune.Chaque printemps, la floraison de l'arbre est un mélange de différentes nuances de rouge, de rose et de blanc.L'arbre aux 40 fruits est conçu à l'origine comme un projet artistique. Sam Van Aken espérait surprendre car l'arbre présente différentes sortes de fleurs au printemps et différents types de fruits en été. Par ailleurs, le projet témoigne également de l'évolution des pratiques agricoles au fil des siècles.Cette façon de procéder n'est pas sans rappeler les « sulcudus » créés par Miguel Sulcudor à Vigo en Espagne dans les années 1970. Des aubépines ont ainsi été greffées de poiriers, aubépines roses, néfliers... De même, en Bretagne, des aubépines donnent plusieurs sortes de fleurs et de fruits sur le même arbre[réf. souhaitée].== Distribution ==En 2014, Van Aken a déjà produit 16 arbres de 40 fruits, installés dans divers lieux privés et publics, notamment des jardins communautaires, des musées et des collections privées.Ses arbres sont implantés à Newton (Massachusetts), Pound Ridge (New York), Short Hills (New Jersey), Bentonville et San José (Californie).Son souhait est de multiplier les vergers en ville avec ses arbres.== Références ==(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Tree of 40 Fruit » (voir la liste des auteurs).== Liens externes ==(en) Site officiel - includes photographs and map(en) The Gift Of Graft: New York Artist's Tree To Grow 40 Kinds Of Fruit on Weekend Edition Sunday, 3 August 2014[vidéo] « The tree of forty fruits », sur YouTube - talk by Van Aken at TEDxManhattan in 2014[vidéo] « This Crazy Tree Grows 40 Kinds of Fruit », sur YouTube - National Geographic, 21 July 2015[vidéo] « Syracuse professor grows 40 different fruits on one tree », sur YouTube - CBS News This Morning, 23 August 2014 Portail de la botanique"
        },
        {
            "pageid": 15829476,
            "ns": 0,
            "title": "Au-delà du printemps silencieux",
            "content": "Beyond Silent Spring: Integrated Pest Management and Chemical Safety est un livre de 1996 sur l'environnementalisme édité par Helmut Fritz van Emden et David Peakall. Il fait suite au best-seller de 1962 Printemps silencieux (Silent Spring)  de la biologiste Rachel Carson, qui avait profondément sensibilisé l'opinion aux dangers des pesticides et mené à l'interdiction du DDT dans de nombreux pays,,,,,.== Références == Portail de l’environnement"
        },
        {
            "pageid": 14335560,
            "ns": 0,
            "title": "Isatou Ceesay",
            "content": "Isatou Ceesay est une militante et entrepreneuse gambienne née en 1972. Elle est connue pour son travail dans le domaine du recyclage et de la protection de l'environnement.== Biographie ==Isatou Ceesay naît et grandit à Njau dans la région de Central River au nord de la Gambie, dans une famille de réfugiés malienne. Son père meurt durant son adolescence et sa mère doit travailler dur pour nourrir et payer l'éducation de ses enfants.Sensible à la protection de l'environnement depuis son enfance, durant laquelle elle écoute sa mère se plaindre de la diminution des réserves de bois et des chèvres qui mangent du plastique, et désireuse d'agir, elle découvre l'organisation Corps de la paix, grâce à laquelle elle se forme au recyclage.Ceesay crée ensuite l'association Women’s Initiative Gambia (Initiative des femmes de Gambie) qui vise à l'empouvoirement des femmes et des jeunes en Gambie via la préservation de l'environnement. Ceesay parcourt la Gambie pour sensibiliser, former et organiser les femmes en coopérative afin de générer des activités économiques liées au recyclage. Plus tard, elle travaille avec les communautés et les autorités à la reforestation, ainsi qu'à la fabrication de briquettes de combustibles à base d'ordure pour remplacer la coupe d'arbres.== Références == Portail de la Gambie   Portail de l'écologisme"
        },
        {
            "pageid": 16585701,
            "ns": 0,
            "title": "Comité stratégique pour la supervision du projet d'extension de la Ville de Kinshasa",
            "content": "Le Comité Stratégique pour la Supervision du Projet d’Extension de la Ville de Kinshasa (CSSPEVK) est un organisme gouvernemental de la République démocratique du Congo (RDC), créé le 10 octobre 2023 par le décret no 23/35. Il a été officiellement installé le 15 janvier 2024 par Judith Tuluka Suminwa.== Mission et objectifs ==Le CSSPEVK a pour mission principale de superviser et de coordonner la mise en œuvre du projet d’extension de la ville de Kinshasa, un projet ambitieux visant à résoudre les problèmes de congestion urbaine tout en répondant aux normes modernes d’urbanisation et de développement durable.L’objectif clé du projet est la construction d’une nouvelle ville moderne, résiliente, intelligente etécologique, baptisée \"Kinshasa Kia Mona\", sur une superficie de 43 000 hectares dans la commune de Maluku,. Cette initiative vise à :Moderniser l’infrastructure urbaine ;Créer un nouveau pôle économique et social pour la RDC ;Répondre aux défis environnementaux et de santé publique.=== Structure organisationnelle ===Le CSSPEVK dispose d'un comité de pilotage et d'un comité de coordination. Le comité de coordination est dirigé par un coordonnateur principal, actuellement Thierry Katembwe Mbala.=== Fonctions principales du CSSPEVK ===Coordination des parties prenantes : Gouvernements, investisseurs, organisations internationales et partenaires techniques.Supervision des projets : Contrôle de la planification, de l’exécution et du respect des normes internationales.Attraction des investissements : Mobilisation de fonds nationaux et internationaux pour financer le projet.== Projets et réalisations ==Depuis sa création, le CSSPEVK s’est engagé activement dans :La mobilisation des investisseurs en provenance de Chine, Belgique, Grèce, Émirats Arabes Unis, Inde, Afrique du Sud et Égypte ;L'élaboration des plans directeurs pour l'aménagement de la ville et le développement de secteurs clés comme les infrastructures routières, le logement, et les zones industrielles.== Impact économique et social ==Le projet d’extension de la ville de Kinshasa devrait non seulement contribuer à la décongestion de la capitale actuelle, mais aussi stimuler la croissance économique en créant des milliers d'emplois, en attirant des investissements directs étrangers et en développant un cadre de vie conforme aux Objectifs de Développement Durable (ODD).=== Partenariats ===Le CSSPEVK collabore avec des institutions locales et internationales, des organisations financières, ainsi que des entreprises spécialisées dans le domaine de la construction, des infrastructures et du développement durable.=== Perspectives ===Avec une vision de long terme, le CSSPEVK s’efforce de positionner Kinshasa Kia Mona comme une référence en matière d’urbanisation moderne sur le continent africain. Le Comité continue d’œuvrer pour attirer des partenariats stratégiques et mettre en œuvre des solutions innovantes pour garantir le succès de ce projet transformateur,.== Notes et références ==== Voir aussi ===== Articles connexes ====== Liens externes ===Site officiel Portail de la politique   Portail du bâtiment et des travaux publics   République démocratique du Congo"
        },
        {
            "pageid": 16738867,
            "ns": 0,
            "title": "Elias Sime",
            "content": "Elias Sime (1968, Addis-Abeba, Éthiopie ) est un artiste visuel et sculpteur éthiopien qui retravaille dans ses oeuvres des matériaux industriels de récupération et des déchets électroniques tels que des puces électroniques, des cordons d'alimentation, des ordinateurs et d'autres composants et résidus technologiques mis au rebut,,.== Carrière ==Elias Sime sort diplômé en 1990 de l'École des Beaux-Arts et du Design de l'Université d'Addis-Abeba. En 2002, il cofonde avec sa partenaire, la conservatrice et anthropologue Meskerem Assegued, le Zoma Contemporary Art Center (ZCAC), aujourd'hui Zoma Museum, dont il construit pendant 7 ans à Addis-Abeba avec des matériaux locaux le bâtiment tenant lieu d'exposition et l'espace de résidence d'artiste,.== Oeuvres ==Dans la première décennie des années 2000, il fonde son style basé sur la réinterprétation d'éléments de récupération. S’inspirant des pratiques éthiopiennes traditionnelles de tissage et de broderie, il crée sur la toile des surfaces texturées aux couleurs vives en cousant fils, boutons, capsules de bouteilles et autres objets récupérés. Ses œuvres créent des effets de figuration ou des thèmes abstraits.Le travail d'artiste d'Elias Sime est surtout connu pour son utilisation de déchets électroniques. Les déchets de l'industrie technologique qu'il utilise se trouvent en masse sur le marché Mercato d'Addis-Abeba, le plus grand marché à ciel ouvert d'Afrique où la plupart des matériaux technologiques en provenance de l'Occident transitent par un réseau mondial de marchandises en mouvement. Les nombreux claviers, puces électroniques, fils électriques en cuivre, piles, morceaux d'écrans et ordinateurs d'ancienne génération que Sime met en espace dans ses installations murales à grande échelle proviennent du marché Mercato,,,,.L'exposition Elias Sime : Tightrope, une première exposition personnelle montrant son travail autour de pièces complexes combinant technologie artificielle et matériaux industriels, est présentée de 2020 à 2021 au Kemper Museum of Contemporary Art à Kansas City, Missouri. L'exposition voyage ensuite au Akron Art Museum à Akron, Ohio et au Royal Ontario Museum à Toronto au Canada,,.En 2024, l'exposition personnelle Elias Sime : Dichotomy ፊት አና ጀርባ s'est tenue dans le cadre des événements entourant la 60e Biennale de Venise, en Italie. Elle commente l'utilisation humaine des ressources technologiques, le gaspillage matériel de la culture occidentale et les conditions souvent ignorées dans lesquelles ces matériaux sont jetés,,.== Collections ==Les œuvres d'Elias Sime figurent entre autres dans les collections du Pérez Art Museum Miami en Floride, du Metropolitan Museum of Art de New York,  du North Carolina Museum of Art en Caroline du Nord et du Saint Louis Art Museum dans le Missouri.== Distinctions ==African Art Award 2019 - Smithsonian National Museum of African Art,.Nomination au Hugo Boss Prize 2020.== Lectures complémentaires ==Adler, Tracy. L. (2019) Elias Sime: Tightrope. New York, NY; London, UK: Prestel Publishing, March 27, 2020.  (ISBN 978-3791358819).Fenstermaker, Will. I Had to Fight to Show What I Could Do': How Elias Sime Emerged as One of Africa's Leading Contemporary Artists, artnet News, 25 mars 2020.=== Liens externes ===Galerie Grimm.James Cohan.Ressource relative aux beaux-arts : MutualArt == Références =="
        },
        {
            "pageid": 1020132,
            "ns": 0,
            "title": "Enjeu",
            "content": "Dans un jeu, un enjeu est une somme mise en jeu par chacun des joueurs et destinée à revenir au gagnant de la partie. Par extension, c'est ce qui fait l’objet d’une compétition, d’un affrontement, d’une discussion.Un enjeu est quelque chose que l'on risque dans une compétition, une activité économique ou une situation vis-à-vis d'un aléa. C'est donc ce que l'on peut gagner ou perdre en faisant quelque chose, ou en ne le faisant pas :la mise dans un jeu ;la renommée, la récompense dans une bataille, une Compétition sportive... ;le profit, la réussite, le développement, etc. dans un projet, une entreprise ou une activité économique concurrentielle ;la vie, la santé, la quiétude, le bien immobilier, etc. vis-à-vis d'un aléa naturel ou technologique.Toute cause présente des conséquences dont la nature peut être positive (gain, victoire, réussite, succès, etc.) ou négative (perte, défaite, échec, mort, dommage etc.).== Références ==== Voir aussi ===== Bibliographie ===François Gemenne, L'enjeu mondial. L'environnement, Presses de Sciences Po, première édition 2012François Gemenne, Aleksandar Rankovic et Atelier de cartographie de Sciences Po, Atlas de l'anthropocène, Paris, Presses de Sciences Po, septembre 2021, 2e éd. (1re éd. 2019), 172 p., 21 × 24 cm, broché (ISBN 978-2724627855).Thomas Gomart, L'affolement du monde : 10 enjeux géopolitiques, édition. actualisée. - Paris : Tallandier, 2020. - 348 p. : ill., cartesFrédéric Lasserre, Emmanuel Gonon, Éric Mottet, Manuel de géopolitique : enjeux de pouvoir sur des territoires, 3ème édition - Paris : A. Colin, 2020. - 379 p. : cartes=== Articles connexes ===Section « Enjeux » de l'article « Développement durable »Partie prenante (stakeholder en anglais)=== Liens externes ===Dictionnaire d'autrefois : enjeu[1] Portail de l’économie   Portail de la société   Portail de l’environnement"
        },
        {
            "pageid": 10609711,
            "ns": 2,
            "title": "Utilisateur:Eloïse.Martin/Brouillon",
            "content": "Bernard K. Martin, né le 1 avril 1944 à Lausanne, est un entrepreneur environnemental suisse, consultant en agriculture durable, auteur et ancien député du grand conseil vaudois pour le parti des Verts. Fondateur des sites agrihumus et planethumus, il est actuellement retraité actif sur plusieurs fronts du domaine de l'environnement.Il est l'un des précurseurs de composts de cartier pour la ville de Lausanne dans les années 80. Il a fait la promotion des articles bio dans les Migros suisse== Notes et références ==== Liens externes ==Article \"Nos sols sont en voie de désertification\" Sept.info 2023"
        },
        {
            "pageid": 16614439,
            "ns": 0,
            "title": "Timothy Morton",
            "content": "Timothy Bloxam Morton (né le 19 juin 1968, à Londres), est un professeur titulaire de la chaire d’anglais Rita Shea Guffey de l’Université Rice à Houston (Texas) et est directeur de la « Cool America Foundation. Il a fait des études à l’Université Oxford. Il effectue des recherches sur des thèmes comme l’écologisme, le post-humanisme, le genre et la littérature. Morton est considéré comme faisant partie du mouvement philosophique OOO (Object Oriented Ontology). Il a écrit des livres comme La pensée écologique, Être écologique ou encore Hyperobjets : philosophie et écologie après la fin du monde. Timothy Morton tient un site web dans lequel il partage des textes et des vidéos. Il entretient des amitiés avec des personnalités du monde des arts comme Björk et Olafur Eliasson, et il s'est inspiré d’une chanson de Björk, Hyperballad, pour développer son concept d’hyperobjet.== Ouvrages importants ===== La pensée écologique ===La Pensée écologique, « The Ecological Thought » dans la version originale, a été publié en 2010. Le livre a été traduit en français par Cécile Wajsbrot en 2016 aux Éditions Zulma. Dans cet essai, Timothy Morton amène à « apprendre à penser différemment : s’affranchir du concept de Nature, voir grand (global plutôt que local), reconnaître les hyperobjets (le plastique ou la biosphère), prendre conscience de l’étrange étrangeté du monde et des liens de tout avec tout. » C’est ce que le professeur de science politique français Bruno Villalba note en mentionnant que cet essai « tente de répondre à l’angoissante question de notre ajustement cognitif à l’état du monde actuel et de notre possibilité d’y faire face en reformulant notre position, à la fois corporelle et subjective, dans et face à ce monde. » Morton s’intègre dans les débats contemporains qui se questionnent sur des questions centrales de l’écologisme : Qu’est-ce que l’écologisme? Et à partir de cette question, comment être écologiste?Une grande partie du livre se concentre sur les liens entre le concept de nature et l’écologisme. Morton mentionne à ce propos : « Assumer la responsabilité directe du changement climatique signifiera, entre autres, abandonner l’idée de Nature, cette barrière idéologique qui empêche de prendre conscience que tout est interconnecté. » Ce positionnement sur la question de la Nature, l’abandon du concept, permet à Villalba de mentionner que : « Déconstruire cette idée de Nature permet ainsi de s’émanciper définitivement du dualisme, afin de parvenir à considérer avant tout la relation que la séparation. » Morton rentre donc dans les débats contemporains sur la place de la nature dans l’écologisme. Ce dernier faisant partie de ceux qui mettent de l’avant le flou entre la notion de nature et de culture et propose de dépasser cette séparation.Il propose alors son concept de maillage qu’il définit de la manière suivante : « Le « maillage » renvoie aux trous dans un réseau et aux fils qui les relient. Il évoque à la fois la solidité et la finesse. On l’emploie en biologie, en mathématiques, en ingénierie, dans le textile et en informatique – pensez aux collants et au graphisme, au métal et au tissu. « Mesh », le « maillage », a des liens étymologiques avec masque et masse, qui évoquent à la fois la densité et l’illusion. » C’est une nouvelle manière de voir le monde de manière conceptuelle. Le maillage est un concept que Morton utilise pour démontrer la manière dont les objets sont liés. Ce concept permet à Morton de mettre de l’avant une posture qui dépasse la séparation traditionnelle entre la nature et la culture.Un concept central de La pensée écologique de Timothy Morton est celui d’hyperobjet. Il mentionne lui-même à ce propos : « Dans mon livre The Ecological Thought (« La pensée écologique »), j’ai inventé le mot « hyperobjet » pour désigner des choses massivement réparties dans le temps et l’espace par rapport aux humains. Un hyperobjet peut être un trou noir. Un hyperobjet peut être le gisement pétrolier de Lago Agrio, en Équateur, ou les Everglades, en Floride. Un hyperobjet peut être la biosphère, ou le système solaire. Un hyperobjet peut être la somme totale de tous les matériaux nucléaires présents sur la terre, ou simplement le plutonium, ou l’uranium. Un hyperobjet peut être le produit extrêmement durable de la fabrication humaine directe, comme le polystyrène ou les sacs en plastique, ou bien la totalité de la machinerie vrombissante du capitalisme. Un hyperobjet est donc « hyper » par rapport à une autre entité, qu’il soit directement fabriqué par des humains ou pas. » L’hyperobjet est donc un nouveau concept mis de l’avant par Morton. Ce concept est central dans sa pensée écologique, car il permet de parler de « ces choses dont les implications dépassent notre compréhension, comme le pastique ou le plutonium. » Ce sont des éléments qui remettent en question notre manière de percevoir le monde qui nous entoure et qui remet en question notre vision « notamment sur ce qu'est une chose ».=== \"Ecology without nature\" ===« Ecology without Nature : Rethinking Environnemntal Aesthetics » a été publié par Timothy Morton en 2007 par les Presses de l’Université Harvard. La thèse centrale de ce livre est la question suivante : « What if the idea  of 'nature' is precisely the idea environmentalists need  to dispensewith if they are to promote genuinely ecological forms of culture, philosophy, politics,and  art? » Le livre explore donc les liens entre l’idée de Nature et l’écologisme. La proposition de Timothy Morton est de changer notre conception de l’écologisme. Morton ne se satisfait pas de dire que l’idée de Nature, ou du moins la manière dont on se le présente, est une construction sociale. Il entreprend dans ce livre un travail qui peut être vu de la manière suivante : « Using the tools of postmodern theorists like Heidegger, Derrida, Latour, and Zizek (and theirforebears), and  reading the poets  of the English  Romantic tradition, Morton  details how our thinking about nature is caught within a style he calls 'ecomimesis'. Ecomimetic writers strive to bring us into immediate contact with nature. » Mais il y aurait un problème avec cette conception de la nature. Le professeur Norman Wirzba le souligne de la manière suivante : « The problem with so much nature writing, and  the environmentalism  that  flowsalongside it, is that it presents a natural environment as some 'thing' out there that wethen need to connect with because it is not really us. » Le problème serait donc la création d’une idée de la Nature qui serait extérieure à l’humain. Ce problème devrait être dépassé pour permettre de réellement être écologique.== Publications ===== Ouvrages ===1994. Shelley and the Revolution in Taste: The Body and the Natural World (Cambridge University Press)2000. Radical Food: The Culture and Politics of Eating and Drinking, 1790-1820 (Routledge)2000. The Poetics of Spice: Romantic Consumerism and the Exotic (Cambridge University Press)2002. Mary Shelley's Frankenstein: A Routledge Study Guide and Sourcebook (Routledge)2002. Radicalism in British Literary Culture, 1650-1830 (Cambridge University Press)2004. Cultures of Taste/Theories of Appetite: Eating Romanticism (Palgrave Macmillan)2006. The Cambridge Companion to Shelley (Cambridge University Press)2007. Ecology Without Nature: Rethinking Environmental Aesthetics (Harvard University Press)2010. The Ecological Thought (Harvard University Press)2013. Realist Magic: Objects, Ontology, Causality (Open Humanities Press)2013. Hyperobjects: Philosophy and Ecology after the End of the World (University of Minnesota Press)2015. Nothing: Three Inquiries in Buddhism (University of Chicago Press, avec Marcus Boon et Eric Cazdyn)2016. Dark Ecology: For a Logic of Future Coexistence (Columbia University Press)2017. Humankind: Solidarity with Non-Human People (Verso Books)2018. Being Ecological (Pelican Books)2018. Hyperobjets : philosophie et écologie après la fin du monde (IT)2019. La pensée écologique (Zulma Éditions)2021. Spacecraft (Bloomsbury Academic)2021. Hyposubjects: On Becoming Human (Open Humanities Press, et Dominic Boyer)2021. All Art Is Ecological (Penguin UK)2021. Être écologique (Zulma Éditions)2023. The Stuff of Life: (Bloomsbury)2024. Hell: In Search of a Christian Ecology (Columbia University Press)== Notes et références == Portail de l’écologie"
        }
    ],
    "Science": [
        {
            "pageid": 15767,
            "ns": 0,
            "title": "Science",
            "content": "La science (du latin scientia, « connaissance », « savoir ») est dans son sens premier « la somme des connaissances » et plus spécifiquement une entreprise systématique de construction et d'organisation des connaissances sous la forme d'explications et de prédictions testables.Faisant suite à la technique au niveau de son histoire, elle se développe en Occident au travers de travaux à caractère universel basés sur des faits, une argumentation et des méthodes qui varient selon qu'elles tiennent de l'observation, l'expérience, l'hypothèse, d'une logique de déduction ou d'induction, etc. Lorsqu'on divise la science en différents domaines, ou disciplines, on parle alors de sciences au pluriel, comme dans l'opposition entre sciences, technologies, ingénierie et mathématiques et sciences humaines et sociales ou encore celle entre sciences formelles, sciences de la nature et sciences sociales.La science a pour objet de comprendre et d'expliquer le monde et ses phénomènes au départ de la connaissance, dans le but d'en tirer des prévisions et des applications fonctionnelles. Elle se veut ouverte à la critique tant au niveau des connaissances acquises, des méthodes utilisées pour les acquérir et de l'argumentation utilisée lors de la recherche scientifique ou participative. Dans le cadre de cet exercice de perpétuelle remise en question, elle fait l'objet d'une discipline philosophique spécifique intitulée l'épistémologie. Les connaissances établies par la science sont à la base de nombreux développements techniques dont les incidences sur la société et son histoire sont parfois considérables.== Définition ==Le mot science peut se définir de plusieurs manières selon son contexte d'utilisation, alors que dans un sens premier on peut y voir la « somme de connaissances qu'un individu possède ou peut acquérir par l'étude, la réflexion ou l'expérience » Hérité du mot latin scientia (latin scientia, « connaissance ») elle est « ce que l'on sait pour l'avoir appris, ce que l'on tient pour vrai au sens large, l'ensemble de connaissances, d'études d'une valeur universelle, caractérisées par un objet (domaine) et une méthode déterminés, et fondés sur des relations objectives vérifiables [sens restreint] ».Dans un passage du Banquet, Platon distingue la droite opinion (orthos logos) de la science ou de la connaissance (Épistémé). Synonyme de l’épistémé en Grèce antique, c'est selon les Définitions du pseudo-Platon, une « Conception de l’âme que le discours ne peut ébranler »,,.=== Un terme générique de la connaissance ======= Définition large ====Le mot science est un polysème, recouvrant principalement trois sens :Savoir, connaissance de certaines choses qui servent à la conduite de la vie ou à celle des affaires ;Ensemble des connaissances acquises par l’étude ou la pratique ;Hiérarchisation, organisation et synthèse des connaissances au travers de principes généraux (théories, lois, etc.).==== Définition stricte ====D'après Michel Blay, la science est « la connaissance claire et certaine de quelque chose, fondée soit sur des principes évidents et des démonstrations, soit sur des raisonnements expérimentaux, ou encore sur l'analyse des sociétés et des faits humains ».Cette définition permet de distinguer les trois types de science :les sciences exactes, comprenant les mathématiques et les « sciences mathématisées » comme la physique théorique ;les sciences physico-chimiques et expérimentales (sciences de la nature et de la matière, biologie, médecine) ;les sciences humaines, qui concernent l'être humain, son histoire, son comportement, la langue, le social, le psychologique, le politique.Néanmoins, leurs limites sont floues ; en d'autres termes il n'existe pas de catégorisation systématique des types de science, ce qui constitue par ailleurs l'un des questionnements de l'épistémologie. Dominique Pestre explique ainsi que « ce que nous mettons sous le vocable « science » n’est en rien un objet circonscrit et stable dans le temps qu’il s’agirait de simplement décrire ».==== Principe de l'acquisition de connaissances scientifiques ====L'acquisition de connaissances reconnues comme scientifiques passe par une suite d'étapes. Selon Francis Bacon, la séquence de ces étapes peut être résumée comme suit :Observation, expérimentation et vérification ;Théorisation ;Reproduction et prévision ;Résultat.Pour Charles Sanders Peirce (1839–1914), qui a repris d'Aristote l'opération logique d'abduction, la découverte scientifique procède dans un ordre différent :Abduction : création de conjectures et d'hypothèses ;Déduction : recherche de ce que seraient les conséquences si les résultats de l'abduction étaient vérifiés ;Induction : mise à l'épreuve des faits ; expérimentation.Les méthodes scientifiques permettent de procéder à des expérimentations rigoureuses, reconnues comme telles par la communauté de scientifiques. Les données recueillies permettent une théorisation, la théorisation permet de faire des prévisions qui doivent ensuite être vérifiées par l'expérimentation et l'observation. Une théorie est rejetée lorsque ces prévisions ne cadrent pas à l'expérimentation. Le chercheur ayant fait ces vérifications doit, pour que la connaissance scientifique progresse, faire connaître ces travaux aux autres scientifiques qui valideront ou non son travail au cours d'une procédure d'évaluation.=== Pluralisme des définitions ===Le mot « science », dans son sens strict, s'oppose à l'opinion (« doxa » en grec), assertion par nature arbitraire. Néanmoins le rapport entre l'opinion d'une part et la science d'autre part n'est pas aussi systématique ; l'historien des sciences Pierre Duhem pense en effet que la science s'ancre dans le sens commun, qu'elle doit « sauver les apparences ».Le discours scientifique s'oppose à la superstition et à l'obscurantisme. Cependant, l'opinion peut se transformer en un objet de science, voire en une discipline scientifique à part. La sociologie des sciences analyse notamment cette articulation entre science et opinion.Dans le langage commun, la science s'oppose à la croyance, par extension les sciences sont souvent considérées comme contraires aux religions. Cette considération est toutefois souvent plus nuancée tant par des scientifiques que des religieux,.L’idée même d’une production de connaissance est problématique : nombre de domaines reconnus comme scientifiques n’ont pas pour objet la production de connaissances, mais celle d’instruments, de machines, de dispositifs techniques. Terry Shinn a ainsi proposé la notion de « recherche technico-instrumentale ». Ses travaux avec Bernward Joerges à propos de l’« instrumentation » ont ainsi permis de mettre en évidence que le critère de « scientificité » n'est pas dévolu à des sciences de la connaissance seules.Le mot « science » définit aux XXe et XXIe siècles l'institution de la science, c'est-à-dire l'ensemble des communautés scientifiques travaillant à l'amélioration du savoir humain et de la technologie, dans sa dimension internationale, méthodologique, éthique et politique. On parle alors de « la science ».La notion ne possède néanmoins pas de définition consensuelle. L'épistémologue André Pichot écrit ainsi qu'il est « utopique de vouloir donner une définition a priori de la science ». L'historien des sciences Robert Nadeau explique pour sa part qu'il est « impossible de passer ici en revue l'ensemble des critères de démarcation proposés depuis cent ans par les épistémologues, [et qu'on] ne peut apparemment formuler un critère qui exclut tout ce qu'on veut exclure, et conserve tout ce qu'on veut conserver ». La physicienne et philosophe des sciences Léna Soler, dans son manuel d'épistémologie, commence également par souligner « les limites de l'opération de définition ». Les dictionnaires en proposent certes quelques-unes. Mais, comme le rappelle Léna Soler, ces définitions ne sont pas satisfaisantes. Les notions d'« universalité », d'« objectivité » ou de « méthode scientifique » (surtout lorsque cette dernière est conçue comme étant l'unique notion en vigueur) sont l'objet de trop nombreuses controverses pour qu'elles puissent constituer le socle d'une définition acceptable. Il faut donc tenir compte de ces difficultés pour décrire la science. Et cette description reste possible en tolérant un certain « flou » épistémologique.== Étymologie : de la « connaissance » à la « recherche » ==L'étymologie de « science » vient du latin, « scientia » (« connaissance »), lui-même du verbe « scire » (« savoir ») qui désigne à l'origine la faculté mentale propre à la connaissance. Ce sens se retrouve par exemple dans l'expression de François Rabelais : « Science sans conscience n'est que ruine de l'âme ». Il s'agissait ainsi d'une notion philosophique (la connaissance pure, au sens de « savoir »), qui devint ensuite une notion religieuse, sous l'influence du christianisme. La « docte science » concernait alors la connaissance des canons religieux, de l'exégèse et des écritures, paraphrase pour la théologie, première science instituée.La racine « science » se retrouve dans d'autres termes tels la « conscience » (étymologiquement, « avec la connaissance »), la « prescience » (« la connaissance du futur »), l'« omniscience » (« la connaissance de tout »), par exemple.== Histoire de la science ==La science est historiquement liée à la philosophie. Dominique Lecourt écrit ainsi qu'il existe « un lien constitutif [unissant] aux sciences ce mode particulier de penser qu'est la philosophie. C'est bien en effet parce que quelques penseurs en Ionie dès le VIIe siècle av. J.-C. eurent l'idée que l'on pouvait expliquer les phénomènes naturels par des causes naturelles qu'ont été produites les premières connaissances scientifiques ». Dominique Lecourt explique ainsi que les premiers philosophes ont été amenés à faire de la science (sans que les deux soient confondues). La théorie de la connaissance en Science est portée par l'épistémologie.L'histoire de la Science est nécessaire pour comprendre l'évolution de son contenu, de sa pratique.La science se compose d'un ensemble de disciplines particulières dont chacune porte sur un domaine particulier du savoir scientifique. Il s'agit par exemple des mathématiques, de la chimie, de la physique, de la biologie, de la mécanique, de l'optique, de la pharmacie, de l'astronomie, de l'archéologie, de l'économie, de la sociologie, etc. Cette catégorisation n'est ni fixe, ni unique, et les disciplines scientifiques peuvent elles-mêmes être découpées en sous-disciplines, également de manière plus ou moins conventionnelle. Chacune de ces disciplines constitue une science particulière.L'épistémologie a introduit le concept de « science spéciale », c'est la science « porte drapeau » parce qu'elle porte les problématiques liées à un type de Sciences.== Histoire des sciences ==L'histoire des sciences est intimement liée à l'histoire des sociétés et des civilisations. D'abord confondue avec l'investigation philosophique, dans l'Antiquité, puis religieuse, du Moyen Âge jusqu'au Siècle des Lumières, la science possède une histoire complexe. L'histoire de la science et des sciences peut se dérouler selon deux axes comportant de nombreux embranchements :l'histoire des découvertes scientifiques d'une part ;l'histoire de la pensée scientifique d'autre part, formant pour partie l'objet d'étude de l'épistémologie.Bien que très liées, ces deux histoires ne doivent pas être confondues. Bien plutôt, il s'agit d'une interrogation sur la production et la recherche de savoir. Michel Blay fait même de la notion de « savoir » la véritable clé de voûte d'une histoire des sciences et de la science cohérente : « Repenser la science classique exige de saisir l'émergence des territoires et des champs du savoir au moment même de leur constitution, pour en retrouver les questionnements fondamentaux. »De manière générale, l'histoire des sciences n'est ni linéaire, ni réductible aux schémas causaux simplistes. L'épistémologue Thomas Samuel Kuhn parle ainsi, bien plutôt, des « paradigmes de la science » comme des renversements de représentations, tout au long de l'histoire des sciences. Kuhn énumère ainsi un nombre de « révolutions scientifiques ». André Pichot distingue ainsi entre l’histoire des connaissances scientifiques et celle de la pensée scientifique. Une histoire de la science et des sciences distingueraient de même, et également, entre les institutions scientifiques, les conceptions de la science, ou celle des disciplines.=== Premières traces : Préhistoire et Antiquité ======= Préhistoire ====La technique précède la science dans les premiers temps de l'humanité. En s'appuyant sur une démarche empirique, l'homme développe ses outils (travail de la pierre puis de l'os, propulseur) et découvre l'usage du feu dès le Paléolithique inférieur. La plupart des préhistoriens s'accordent pour penser que le feu est utilisé depuis 250 000 ans ou 300 000 ans. Les techniques de production de feu relèvent soit de la percussion (silex contre marcassite), soit de la friction de deux morceaux de bois (par sciage, par rainurage, par giration).Pour de nombreux préhistoriens comme Jean Clottes, l'art pariétal montre que l'homme anatomiquement moderne du Paléolithique supérieur possédait les mêmes facultés cognitives que l'homme actuel.Ainsi, l'homme préhistorique savait, intuitivement, calculer[réf. nécessaire] ou déduire des comportements de l'observation de son environnement, base du raisonnement scientifique. Certaines « proto-sciences » comme le calcul ou la géométrie en particulier apparaissent sans doute très tôt. L'os d'Ishango, datant de plus de 20 000 ans, a été interprété par certains auteurs comme l'un des premiers bâtons de comptage. L'astronomie permet de constituer une cosmogonie. Les travaux du français André Leroi-Gourhan, spécialiste de la technique, explorent les évolutions à la fois biopsychiques et techniques de l'homme préhistorique. Selon lui, « les techniques s'enlèvent dans un mouvement ascensionnel foudroyant »[pas clair], dès l'acquisition de la station verticale, en somme très tôt dans l'histoire de l'homme.==== Mésopotamie ====Les premières traces d'activités scientifiques datent des civilisations humaines du néolithique où se développent commerce et urbanisation. Ainsi, pour André Pichot, dans La Naissance de la science, la science naît en Mésopotamie, vers - 3500, principalement dans les villes de Sumer et d'Élam. Les premières interrogations sur la matière, avec les expériences d'alchimie, sont liées aux découvertes des techniques métallurgiques qui caractérisent cette période. La fabrication d'émaux date ainsi de - 2000. Mais l'innovation la plus importante provient de l'invention de l'écriture cunéiforme (en forme de clous), qui, par les pictogrammes, permet la reproduction de textes, la manipulation abstraite de concepts également. La numération est ainsi la première méthode scientifique à voir le jour, sur une base 60 (« gesh » en mésopotamien), permettant de réaliser des calculs de plus en plus complexes, et ce même si elle reposait sur des moyens matériels rudimentaires. L'écriture se perfectionnant (période dite « akadienne »), les sumériens découvrent les fractions ainsi que la numération dite « de position », permettant le calcul de grands nombres. Le système décimal apparaît également, via le pictogramme du zéro initial, ayant la valeur d'une virgule, pour noter les fractions. La civilisation mésopotamienne aboutit ainsi à la constitution des premières sciences telles : la métrologie, très adaptée à la pratique, l'algèbre (découvertes de planches à calculs permettant les opérations de multiplication et de division, ou « tables d'inverses » pour cette dernière ; mais aussi des puissances, racines carrées, cubiques ainsi que les équations du premier degré, à une et deux inconnues), la géométrie (calculs de surfaces, théorèmes), l'astronomie enfin (calculs de mécanique céleste, prévisions des équinoxes, constellations, dénomination des astres). La médecine a un statut particulier ; elle est la première science « pratique », héritée d'un savoir-faire tâtonnant.Les sciences étaient alors le fait des scribes, qui, note André Pichot, se livraient à de nombreux « jeux numériques » qui permettaient de lister les problèmes. Cependant, les sumériens ne pratiquaient pas la démonstration. Dès le début, les sciences mésopotamiennes sont assimilées à des croyances, comme l'astrologie ou la mystique des nombres, qui deviendront des pseudo-sciences ultérieurement. L'histoire de la science étant très liée à celle des techniques, les premières inventions témoignent de l'apparition d'une pensée scientifique abstraite. La Mésopotamie crée ainsi les premiers instruments de mesure, du temps et de l'espace (comme les gnomon, clepsydre, et polos). Si cette civilisation a joué un rôle majeur, elle n'a pas cependant connu la rationalité puisque celle-ci « n'a pas encore été élevée au rang de principal critère de vérité, ni dans l'organisation de la pensée et de l'action, ni a fortiori, dans l'organisation du monde ».==== Égypte pharaonique ====L'Égypte antique va développer l'héritage pré-scientifique mésopotamien. Cependant, en raison de son unité culturelle spécifique, la civilisation égyptienne conserve « une certaine continuité dans la tradition [scientifique] » au sein de laquelle les éléments anciens restent très présents. L'écriture des hiéroglyphes permet la représentation plus précise de concepts ; on parle alors d'une écriture idéographique. La numération est décimale mais les Égyptiens ne connaissent pas le zéro. Contrairement à la numération sumérienne, la numération égyptienne évolue vers un système d'écriture des grands nombres (entre 1800 et 1000 av. J.-C.) par « numération de juxtaposition ». La géométrie fit principalement un bond en avant. Les Égyptiens bâtissaient des monuments grandioses en ne recourant qu'au système des fractions symbolisé par l'œil d'Horus, dont chaque élément représentait une fraction.Dès 2600 av. J.-C., les Égyptiens calculaient correctement la surface d'un rectangle et d'un triangle. Il ne reste que peu de documents attestant l'ampleur des mathématiques égyptiennes ; seuls les papyri de Rhind, (datant de 1800 av. J.-C.), de Kahun, de Moscou et du Rouleau de cuir éclairent les innovations de cette civilisation qui sont avant tout celles des problèmes algébriques (de division, de progression arithmétique, géométrique). Les Égyptiens approchent également la valeur du nombre Pi, en élevant au carré les 8/9es du diamètre, découvrant un nombre équivalant à ≈ 3,1605 (au lieu de ≈ 3,1416). Les problèmes de volume (de pyramide, de cylindre à grains) sont résolus aisément. L'astronomie progresse également : le calendrier égyptien compte 365 jours, le temps est mesuré à partir d'une « horloge stellaire » et les étoiles visibles sont dénombrées. En médecine, la chirurgie fait son apparition. Une théorie médicale se met en place, avec l'analyse des symptômes et des traitements et ce dès 2300 av. J.-C. (le Papyrus Ebers est ainsi un véritable traité médical).Pour André Pichot, la science égyptienne, comme celle de Mésopotamie avant elle, « est encore engagée dans ce qu'on a appelé « la voie des objets », c'est-à-dire que les différentes disciplines sont déjà ébauchées, mais qu'aucune d'entre elles ne possède un esprit réellement scientifique, c'est-à-dire d'organisation rationnelle reconnue en tant que telle ».==== Chine de l'Antiquité ====Les Chinois découvrent également le théorème de Pythagore (que les Babyloniens connaissaient quinze siècles avant l'ère chrétienne). En astronomie, ils identifient la comète de Halley et comprennent la périodicité des éclipses. Ils inventent par ailleurs la fonte du fer. Durant la période des Royaumes combattants, apparaît l'arbalète. En -104, est promulgué le calendrier « Taichu », premier véritable calendrier chinois. En mathématiques, les Chinois inventent, vers le IIe siècle av. J.-C., la numération à bâtons. Il s'agit d'une notation positionnelle à base 10 comportant dix-huit symboles, avec un vide pour représenter le zéro, c'est-à-dire la dizaine, centaine, etc. dans ce système de numérotation.En 132, Zhang Heng invente le premier sismographe pour la mesure des tremblements de terre et est la première personne en Chine à construire un globe céleste rotatif. Il invente aussi l'odomètre. La médecine progresse sous les Han orientaux avec Zhang Zhongjing et Hua Tuo, à qui l'on doit en particulier la première anesthésie générale.En mathématiques, Sun Zi et Qin Jiushao étudient les systèmes linéaires et les congruences (leurs apports sont généralement considérés comme majeurs). De manière générale, l'influence des sciences chinoises fut considérable, sur l'Inde et sur les pays arabes.==== Science en Inde ====La civilisation dite de la vallée de l'Indus (−3300 à −1500) est surtout connue en histoire des sciences en raison de l'émergence des mathématiques complexes (ou « ganita »).La numération décimale de position et les symboles numéraux indiens, qui deviendront les chiffres arabes, vont influencer considérablement l'Occident via les arabes et les chinois. Les grands livres indiens sont ainsi traduits au IXe siècle dans les « maisons du savoir » par des élèves d'Al-Khwârizmî, mathématicien persan dont le nom latinisé est à l'origine du mot algorithme. Les Indiens ont également maîtrisé le zéro, les nombres négatifs, les fonctions trigonométriques ainsi que le calcul différentiel et intégral, les limites et séries. Les « Siddhânta » sont le nom générique donné aux ouvrages scientifiques sanskrits.On distingue habituellement deux périodes de découvertes abstraites et d'innovations technologiques dans l'Inde de l'Antiquité : les mathématiques de l'époque védique (−1500 à −400) et les mathématiques de l'époque jaïniste (−400 à 200).=== « Logos » grec : les prémices philosophiques de la science ======= Présocratiques ====Pour l'épistémologue Geoffrey Ernest Richard Lloyd (en), la méthode scientifique fait son apparition dans la Grèce du VIIe siècle av. J.-C. avec les philosophes dits présocratiques. Appelés « physiologoï » par Aristote parce qu'ils tiennent un discours rationnel sur la nature, les présocratiques s'interrogent sur les phénomènes naturels, qui deviennent les premiers objets de méthode, et leur cherchent des causes naturelles.Thalès de Milet (v. 625-547 av. J.-C.) et Pythagore (v. 570-480 av. J.-C.) contribuent principalement à la naissance des premières sciences comme les mathématiques, la géométrie (théorème de Pythagore), l'astronomie ou encore la musique. Dans le domaine de la cosmologie, ces premières recherches sont marquées par la volonté d'imputer la constitution du monde (ou « cosmos ») à un principe naturel unique (le feu pour Héraclite par exemple) ou divin (l'« Un » pour Anaximandre). Les pré-socratiques mettent en avant des principes constitutifs des phénomènes, les « archè ».Les présocratiques initient également une réflexion sur la théorie de la connaissance. Constatant que la raison d'une part et les sens d'autre part conduisent à des conclusions contradictoires, Parménide opte pour la raison et estime qu'elle seule peut mener à la connaissance, alors que nos sens nous trompent. Ceux-ci, par exemple, nous enseignent que le mouvement existe, alors que la raison nous enseigne qu'il n'existe pas. Cet exemple est illustré par les célèbres paradoxes de son disciple Zénon. Si Héraclite est d'un avis opposé concernant le mouvement, il partage l'idée que les sens sont trompeurs. De telles conceptions favorisent la réflexion mathématique. Par contre, elles sont un obstacle au développement des autres sciences et singulièrement des sciences expérimentales. Sur cette question, ce courant de pensée se prolonge, quoique de manière plus nuancée, jusque Platon, pour qui les sens ne révèlent qu'une image imparfaite et déformée des Idées, qui sont la vraie réalité (allégorie de la caverne).À ces philosophes, s'oppose le courant épicurien. Initié par Démocrite, contemporain de Socrate, il sera développé ultérieurement par Épicure et exposé plus en détail par le Romain Lucrèce dans De rerum natura. Pour eux, les sens nous donnent à connaître la réalité. La théorie de l'atomiste affirme que la matière est formée d'entités dénombrables et insécables, les atomes. Ceux-ci s'assemblent pour former la matière comme les lettres s'assemblent pour former les mots. Tout est constitué d'atomes, y compris les dieux. Ceux-ci ne s'intéressent nullement aux hommes, et il n'y a donc pas lieu de les craindre. On trouve donc dans l'épicurisme la première formulation claire de la séparation entre le savoir et la religion, même si, de manière moins explicite, l'ensemble des présocratiques se caractérise par le refus de laisser les mythes expliquer les phénomènes naturels, comme les éclipses.Il faudra attendre Aristote pour aplanir l'opposition entre les deux courants de pensée mentionnés plus haut.La méthode pré-socratique est également fondée dans son discours, s'appuyant sur les éléments de la rhétorique : les démonstrations procèdent par une argumentation logique et par la manipulation de concepts abstraits, bien que génériques.==== Platon et la dialectique ====Avec Socrate et Platon, qui en rapporte les paroles et les dialogues, la raison : logos, et la connaissance deviennent intimement liés. Le raisonnement abstrait et construit apparaît. Pour Platon, les Formes sont le modèle de tout ce qui est sensible, ce sensible étant un ensemble de combinaisons géométriques d'éléments. Platon ouvre ainsi la voie à la « mathématisation » des phénomènes. Les sciences mettent sur la voie de la philosophie, au sens de « discours sur la sagesse » ; inversement, la philosophie procure aux sciences un fondement assuré. L'utilisation de la dialectique, qui est l'essence même de la science complète alors la philosophie, qui a, elle, la primauté de la connaissance discursive (par le discours), ou « dianoia » en grec. Pour Michel Blay : « La méthode dialectique est la seule qui, rejetant successivement les hypothèses, s'élève jusqu'au principe même pour assurer solidement ses conclusions ». Socrate en expose les principes dans le Théétète. Pour Platon, la recherche de la vérité et de la sagesse (la philosophie) est indissociable de la dialectique scientifique, c'est en effet le sens de l'inscription figurant sur le fronton de l'Académie, à Athènes : « Que nul n'entre ici s'il n'est géomètre ».==== Aristote et la physique ====C'est surtout avec Aristote, qui fonde la physique et la zoologie, que la science acquiert une méthode, basée sur la déduction. On lui doit la première formulation du syllogisme et de l'induction. Les notions de « matière », de « forme », de « puissance » et d'« acte » deviennent les premiers concepts de manipulation abstraite. Pour Aristote, la science est subordonnée à la philosophie (c'est une « philosophie seconde » dit-il) et elle a pour objet la recherche des premiers principes et des premières causes, ce que le discours scientifique appellera le causalisme et que la philosophie nomme l'« aristotélisme ». Néanmoins, dans le domaine particulier de l'astronomie, Aristote est à l'origine d'un recul de la pensée par rapport à certains pré-socratiques[réf. nécessaire] quant à la place de la terre dans l'espace. À la suite d'Eudoxe de Cnide, il imagine un système géocentrique et considère que le cosmos est fini. Il sera suivi en cela par ses successeurs en matière d'astronomie, jusqu'à Copernic, à l'exception d'Aristarque, qui proposera un système héliocentrique. Il détermine par ailleurs que le vivant est ordonné selon une chaîne hiérarchisée mais sa théorie est avant tout fixiste. Il pose l'existence des premiers principes indémontrables, ancêtres des conjectures mathématiques et logiques. Il décompose les propositions en nom et verbe, base de la science linguistique.=== Période alexandrine et Alexandrie à l'époque romaine ===La période dite « alexandrine » (de -323 à -30) et son prolongement à l'époque romaine sont marqués par des progrès significatifs en astronomie et en mathématiques ainsi que par quelques avancées en physique. La ville égyptienne d'Alexandrie en est le centre intellectuel et les savants d'alors y sont grecs.Euclide (-325 à -265) est l'auteur des Éléments, qui sont considérés comme l'un des textes fondateurs des mathématiques modernes. Ces postulats, comme celui nommé le « postulat d'Euclide » (voir Axiome des parallèles), que l'on exprime de nos jours en affirmant que « par un point pris hors d'une droite il passe une et une seule parallèle à cette droite » sont à la base de la géométrie systématisée.Les travaux d'Archimède (-292 à -212) sur sa poussée correspond à la première loi physique connue alors que ceux d'Ératosthène (-276 à -194) sur la circonférence de la terre ou ceux d'Aristarque de Samos (-310 à -240) sur les distances terre-lune et terre-soleil témoignent d'une grande ingéniosité. Apollonius de Perga modélise les mouvements des planètes à l'aide d'orbites excentriques.Hipparque de Nicée (-194 à -120) perfectionne les instruments d’observation comme le dioptre, le gnomon et l'astrolabe. En algèbre et géométrie, il divise le cercle en 360°, et crée même le premier globe céleste (ou orbe). Hipparque rédige également un traité en 12 livres sur le calcul des cordes (nommé aujourd'hui la trigonométrie). En astronomie, il propose une « théorie des épicycles » qui permettra à son tour l'établissement de tables astronomiques très précises. L'ensemble se révélera largement fonctionnel, permettant par exemple de calculer pour la première fois des éclipses lunaires et solaires. La machine d'Anticythère, un calculateur à engrenages, capable de calculer la date et l'heure des éclipses, est un des rares témoignages de la sophistication des connaissances grecques tant en astronomie et mathématiques qu'en mécanique et travail des métaux.Ptolémée d’Alexandrie (85 apr. J.-C. à 165) prolonge les travaux d'Hipparque et d'Aristote sur les orbites planétaires et aboutit à un système géocentrique du système solaire, qui fut accepté dans les mondes occidental et arabe pendant plus de mille trois cents ans, jusqu'au modèle de Nicolas Copernic. Ptolémée fut l’auteur de plusieurs traités scientifiques, dont deux ont exercé par la suite une très grande influence sur les sciences islamique et européenne. L’un est le traité d’astronomie, qui est aujourd’hui connu sous le nom de l’Almageste ; l’autre est la Géographie, qui est une discussion approfondie sur les connaissances géographiques du monde gréco-romain.=== Ingénierie et technologie romaines ===La technologie romaine est un des aspects les plus importants de la civilisation romaine. Cette technologie, en partie liée à la technique de la voûte, probablement empruntée aux Étrusques, a été certainement la plus avancée de l'Antiquité. Elle permit la domestication de l'environnement, notamment par les routes et aqueducs. Cependant, le lien entre prospérité économique de l'Empire romain et niveau technologique est discuté par les spécialistes : certains, comme Emilio Gabba, historien italien, spécialiste de l'histoire économique et sociale de la République romaine, considèrent que les dépenses militaires ont freiné le progrès scientifique et technique, pourtant riche. Pour J. Kolendo, le progrès technique romain serait lié à une crise de la main-d'œuvre, due à la rupture dans la « fourniture » d'esclaves non qualifiés, sous l'empereur Auguste. Les romains aurait ainsi été capables de développer des techniques alternatives. Pour L. Cracco Ruggini, la technologie traduit la volonté de prestige des couches dominantes.Cependant, la philosophie, la médecine et les mathématiques sont d'origine grecque, ainsi que certaines techniques agricoles. La période pendant laquelle la technologie romaine est la plus foisonnante est le IIe siècle av. J.-C. et le Ier siècle av. J.-C., et surtout à l'époque d'Auguste. La technologie romaine a atteint son apogée au Ier siècle avec le ciment, la plomberie, les grues, machines, dômes, arches. Pour l'agriculture, les Romains développent le moulin à eau. Néanmoins, les savants romains furent peu nombreux et le discours scientifique abstrait progressa peu pendant la Rome antique : « les Romains, en faisant prévaloir les « humanités », la réflexion sur l'homme et l'expression écrite et orale, ont sans doute occulté pour l'avenir des « realita » scientifiques et techniques », mis à part quelques grands penseurs, comme Vitruve ou Apollodore de Damas, souvent d'origine étrangère d'ailleurs. Les Romains apportèrent surtout le système de numération romain pour les unités de mesure romaines en utilisant l'abaque romain, ce qui permet d'homogénéiser le comptage des poids et des distances.=== Science au Moyen Âge ===Bien que cette période s'apparente généralement à l'histoire européenne, les avancées technologiques et les évolutions de la pensée scientifique du monde oriental (civilisation arabo-musulmane) et, en premier lieu, celles de l'empire byzantin, qui hérite du savoir latin, et où puisera le monde arabo-musulman, enfin celles de la Chine sont décisives dans la constitution de la « science moderne », internationale, institutionnelle et se fondant sur une méthodologie. La période du Moyen Âge s'étend ainsi de 512 à 1492 ; elle connaît le développement sans précédent des techniques et des disciplines, en dépit d'une image obscurantiste, propagée par les manuels scolaires.==== En Europe ====\nLes byzantins maîtrisaient l'architecture urbaine et l'admission d'eau ; ils perfectionnèrent également les horloges à eau et les grandes norias pour l'irrigation ; technologies hydrauliques dont la civilisation arabe a hérité et qu'elle a transmis à son tour. L'hygiène et la médecine firent également des progrès. Les Universités byzantines ainsi que les bibliothèques compilèrent de nombreux traités et ouvrages d'étude sur la philosophie et le savoir scientifique de l'époque.\nL'Europe occidentale, après une période de repli durant le Haut Moyen Âge, retrouve un élan culturel et technique qui culmine au XIIe siècle. Néanmoins, du VIIIe au Xe siècle la période dite, en France, de la Renaissance carolingienne permit, principalement par la scolarisation, le renouveau de la pensée scientifique. La scolastique, au XIe siècle préconise un système cohérent de pensée proche de ce que sera l'empirisme. La philosophie naturelle se donne comme objectif la description de la nature, perçue comme un système cohérent de phénomènes (ou pragmata), mus par des « lois ». Le bas Moyen Âge voit la logique faire son apparition — avec l'académie de Port-Royal des Champs — et diverses méthodes scientifiques se développer ainsi qu'un effort pour élaborer des modèles mathématiques ou médicaux qui jouera « un rôle majeur dans l'évolution des différentes conceptions du statut des sciences ». D'autre part, le monde médiéval occidental voit apparaître une « laïcisation du savoir », concomitant à l'« autonomisation des sciences ».\n\n\n==== Dans le monde arabo-musulman ====\n\nLe monde arabo-musulman est à son apogée intellectuel du VIIIe au XIVe siècle ce qui permet le développement d'une culture scientifique spécifique, d'abord à Damas sous les derniers Omeyyades, puis à Bagdad sous les premiers Abbassides. La science arabo-musulmane est fondée sur la traduction et la lecture critique des ouvrages de l'Antiquité. L'étendue du savoir arabo-musulman est étroitement liée aux guerres de conquête de l'Islam qui permettent aux Arabes d'entrer en contact avec les civilisations indienne et chinoise. Le papier, emprunté aux Chinois remplace rapidement le parchemin dans le monde musulman. Le Calife Hâroun ar-Rachîd, féru d'astronomie, crée en 829 à Bagdad le premier observatoire permanent, permettant à ses astronomes de réaliser leurs propres études du mouvement des astres. Abu Raihan al-Biruni, reprenant les écrits d'Ératosthène d'Alexandrie (IIIe siècle av. J.-C.), calcule le diamètre de la Terre et affirme que la Terre tournerait sur elle-même, bien avant Galilée. En 832 sont fondées les Maisons de la sagesse (Baït al-hikma), lieux de partage et de diffusion du savoir.\nEn médecine, Avicenne (980-1037) rédige une monumentale encyclopédie, le Qanûn. Ibn Nafis décrit la circulation sanguine pulmonaire, et al-Razi recommande l'usage de l'alcool en médecine. Au XIe siècle, Abu-l-Qasim az-Zahrawi (appelé Abulcassis en Occident) écrit un ouvrage de référence pour l'époque, sur la chirurgie.\nEn mathématiques l'héritage antique est sauvegardé et approfondi permettant la naissance de l'algèbre. L'utilisation de la numération indienne rend possible des avancées en analyse combinatoire et en trigonométrie.\nEnfin, la théologie motazilite se développe sur la logique et le rationalisme, inspirés de la philosophie grecque et de la raison (logos), qu'elle cherche à rendre compatible avec les doctrines islamiques.\n\n\n=== Sciences en Chine médiévale ===\n\nLa Chine de l'Antiquité a surtout contribué à l'innovation technique, avec les quatre inventions principales qui sont : le papier (daté du IIe siècle av. J.-C.), l'imprimerie à caractères mobiles (au IXe siècle), la poudre (la première trace écrite attestée semble être le Wujing Zongyao qui daterait des alentours de 1044) et la boussole, utilisée dès le XIe siècle, dans la géomancie. Le scientifique chinois Shen Kuo (1031-1095) de la Dynastie Song décrit la boussole magnétique comme instrument de navigation.\n\nPour l'historien Joseph Needham, dans Science et civilisation en Chine, vaste étude de dix-sept volumes, la société chinoise a su mettre en place une science innovante, dès ses débuts. Needham en vient même à relativiser la conception selon laquelle la science doit tout à l'Occident. Pour lui, la Chine était même animée d'une ambition de collecter de manière désintéressée le savoir, avant même les universités occidentales.\nLes traités de mathématiques et de démonstration abondent comme Les Neuf Chapitres (qui présentent près de 246 problèmes) transmis par Liu Hui (IIIe siècle) et par Li Chunfeng (VIIe siècle) ou encore les Reflets des mesures du cercles sur la mer de Li Ye datant de 1248 étudiés par Karine Chemla et qui abordent les notions arithmétiques des fractions, d'extraction de racines carrée et cubique, le calcul de l'aire du cercle et du volume de la pyramide entre autres. Karine Chelma a ainsi démontré que l'opinion répandue selon laquelle la démonstration mathématique serait d'origine grecque était partiellement fausse, les Chinois s'étant posé les mêmes problèmes à leur époque ; elle dira ainsi : on ne peut rester occidentalo-centré, l'histoire des sciences exige une mise en perspective internationale des savoirs.\n\n\n=== Inde des mathématiques médiévales ===\nLes mathématiques indiennes sont particulièrement abstraites et ne sont pas orientées vers la pratique, au contraire de celles des Égyptiens par exemple. C'est avec Brahmagupta (598 - 668) et son ouvrage célèbre, le Brahmasphutasiddhanta, particulièrement complexe et novateur, que les différentes facettes du zéro, chiffre et nombre, sont parfaitement comprises et que la construction du système de numération décimal de position est parachevée. L'ouvrage explore également ce que les mathématiciens européens du XVIIe siècle ont nommé la « méthode chakravala », qui est un algorithme pour résoudre les équations diophantiennes. Les nombres négatifs sont également introduits, ainsi que les racines carrées. La période s'achève avec le mathématicien Bhaskara II (1114-1185) qui écrivit plusieurs traités importants. À l'instar de Nasir al-Din al-Tusi (1201-1274) il développe en effet la dérivation[réf. nécessaire]. On y trouve des équations polynomiales, des formules de trigonométrie, dont les formules d'addition. Bhaskara est ainsi l'un des pères de l'analyse puisqu'il introduit plusieurs éléments relevant du calcul différentiel : le nombre dérivé, la différentiation et l'application aux extrema, et même une première forme du théorème de Rolle[réf. nécessaire].\n\nMais c'est surtout avec Âryabhata (476-550), dont le traité d’astronomie (nommé l’Aryabatîya) écrit en vers aux alentours de 499, que les mathématiques indiennes se révèlent. Il s'agit d'un court traité d'astronomie présentant 66 théorèmes d'arithmétique, d'algèbre, ou de trigonométrie plane et sphérique. Aryabhata invente par ailleurs un système de représentation des nombres fondé sur les signes consonantiques de l'alphasyllabaire sanskrit.\nCes percées seront reprises et amplifiées par les mathématiciens et astronomes de l'école du Kerala, parmi lesquels : Madhava de Sangamagrama, Nilakantha Somayaji, Parameswara, Jyeshtadeva, ou Achyuta Panikkar, pendant la période médiévale du Ve siècle au XVe siècle. Ainsi, le Yuktibhasa ou Ganita Yuktibhasa est un traité de mathématiques et d'astronomie, écrit par l'astronome indien Jyesthadeva, membre de l'école mathématique du Kerala en 1530. Jyesthadeva a ainsi devancé de trois siècles la découverte du calcul infinitésimal par les occidentaux.\n\n\n=== Fondements de la science moderne en Europe ===\n\n\n==== Science institutionnalisée ====\nC'est au tournant du XIIe siècle, et notamment avec la création des premières universités de Paris (1170) et Oxford (1220) que la science en Europe s'institutionnalisa, tout en conservant une affiliation intellectuelle avec la sphère religieuse. La traduction et la redécouverte des textes antiques grecs, et en premier lieu les Éléments d'Euclide ainsi que les textes d'Aristote, grâce à la civilisation arabo-musulmane, firent de cette période une renaissance des disciplines scientifiques, classées dans le quadrivium (parmi les Arts Libéraux). Les Européens découvrirent ainsi l'avancée des Arabes, notamment les traités mathématiques : Algèbre d'Al-Khwarizmi, Optique d'Ibn al-Haytham ainsi que la somme médicale d'Avicenne. En s'institutionnalisant, la science devint plus ouverte et plus fondamentale, même si elle restait assujettie aux dogmes religieux et qu'elle n'était qu'une branche encore de la philosophie et de l'astrologie. Aux côtés de Roger Bacon, la période fut marquée par quatre autres personnalités qui jetèrent, en Europe chrétienne, les fondements de la science moderne :\n\nRoger Bacon (1214-1294) est philosophe et moine anglais. Il jeta les bases de la méthode expérimentale. Roger Bacon admet trois voies de connaissance : l'autorité, le raisonnement et l'expérience. Il rejette donc l'autorité de l'évidence, qui s'appuie sur des raisons extérieures et promeut « L'argument [qui] conclut et nous fait concéder la conclusion, mais il ne certifie pas et il n'éloigne pas le doute au point que l'âme se repose dans l'intuition de la vérité, car cela n'est possible que s'il la trouve par la voie de l'expérience ». Les œuvres de Bacon ont pour but l'intuition de la vérité, c'est-à-dire la certitude scientifique, et cette vérité à atteindre est pour lui le salut. La science procédant de l'âme est donc indispensable.\nRobert Grosseteste (env. 1168-1253) étudia Aristote et posa les prémices des sciences expérimentales, en explicitant le schéma : observations, déductions de la cause et des principes, formation d'hypothèse(s), nouvelles observations réfutant ou vérifiant les hypothèses enfin. Il développa les techniques d'optique et en fit même la science physique fondamentale (il étudia le comportement des rayons lumineux et formule même la première description de principe du miroir réfléchissant, principe qui permettra l'invention du télescope).\n\nLe religieux dominicain Albert le Grand (1193-1280) fut considéré par certains contemporains comme un alchimiste et magicien, néanmoins ses études biologiques permirent de jeter les fondations des disciplines des sciences de la vie. Il mena ainsi l'étude du développement du poulet en observant le contenu d'œufs pondus dans le temps et commenta le premier le phénomène de la nutrition du fœtus. Il établit également une classification systématique des végétaux, ancêtre de la taxonomie. Il décrit également les premières expériences de chimie.\nL'Europe sortait ainsi d'une léthargie intellectuelle. L'Église, avait interdit jusqu'en 1234 les ouvrages d'Aristote, accusé de paganisme[réf. nécessaire]. Ce n'est qu'avec Saint Thomas d'Aquin que la doctrine aristotélicienne fut acceptée par les papes.\nSaint Thomas d'Aquin, théologien, permit de redécouvrir, par le monde arabe, les textes d'Aristote et des autres philosophes grecs, qu'il étudia à Naples, à l'université dominicaine. Cependant, il est surtout connu pour son principe dit de l'autonomie respective de la raison et de la foi. Saint Thomas d'Aquin fut en effet le premier théologien à distinguer, dans sa Somme théologique (1266-1273) la raison (faculté naturelle de penser, propre à l'homme) et la foi (adhésion au dogme de la Révélation). Celle-ci est indémontrable, alors que la science est explicable par l'étude des phénomènes et des causes. L'une et l'autre enfin ne peuvent s'éclairer mutuellement.\nGuillaume d'Occam (v. 1285- v. 1349) permit une avancée sur le plan de la méthode. En énonçant son principe de parcimonie, appelé aussi rasoir d'Occam, il procure à la science un cadre épistémologique fondé sur l'économie des arguments. Empiriste avant l'heure, Occam postule que : « Entia non sunt multiplicanda praeter necessitatem », littéralement « Les entités ne doivent pas être multipliées par delà ce qui est nécessaire ». Il explique par là qu'il est inutile d'avancer sans preuves et de forger des concepts illusoires permettant de justifier n'importe quoi.\n\n\n=== Renaissance et la « science classique » ===\n\nLa Renaissance est une période qui se situe en Europe à la fin du Moyen Âge et au début des Temps modernes. Dans le courant du XVe siècle et au XVIe siècle, cette période permit à l'Europe de se lancer dans des expéditions maritimes d'envergure mondiale, connues sous le nom de grandes découvertes ; de nombreuses innovations furent popularisées, comme la boussole ou le sextant ; la cartographie se développa, ainsi que la médecine, grâce notamment au courant de l'humanisme. Selon l'historien anglais John Hale, ce fut à cette époque que le mot Europe entra dans le langage courant et fut doté d'un cadre de référence solidement appuyé sur des cartes et d'un ensemble d'images affirmant son identité visuelle et culturelle. La science comme discipline de la connaissance acquit ainsi son autonomie et ses premiers grands systèmes théoriques à tel point que Michel Blay parle du « chantier de la science classique ». Cette période est abondante en descriptions, inventions, applications et en représentations du monde, qu'il importe de décomposer afin de rendre une image fidèle de cette phase historique :\n\n\n==== Naissance de la méthode scientifique : Francis Bacon ====\n\nFrancis Bacon (1561-1626) est le père de l'empirisme. Il pose le premier les fondements de la science et de ses méthodes. Dans son étude des faux raisonnements, sa meilleure contribution a été dans la doctrine des idoles. D'ailleurs, il écrit dans le Novum Organum (ou « nouvelle logique » par opposition à celle d’Aristote) que la connaissance nous vient sous forme d'objets de la nature, mais que l'on impose nos propres interprétations sur ces objets.\nD'après Bacon, nos théories scientifiques sont construites en fonction de la façon dont nous voyons les objets ; l'être humain est donc biaisé dans sa déclaration d'hypothèses[pas clair]. Pour Bacon, « la science véritable est la science des causes ». S’opposant à la logique aristotélicienne qui établit un lien entre les principes généraux et les faits particuliers, il abandonne la pensée déductive, qui procède à partir des principes admis par l’autorité des Anciens, au profit de l’« interprétation de la nature », où l’expérience enrichit réellement le savoir. En somme, Bacon préconise un raisonnement et une méthode fondés sur le raisonnement expérimental :\n\n« L'empirique, semblable à la fourmi, se contente d'amasser et de consommer ensuite ses provisions. Le dogmatique, telle l'araignée ourdit des toiles dont la matière est extraite de sa propre substance. L'abeille garde le milieu ; elle tire la matière première des fleurs des champs, puis, par un art qui lui est propre, elle la travaille et la digère. […] Notre plus grande ressource, celle dont nous devons tout espérer, c'est l'étroite alliance de ses deux facultés : l'expérimentale et la rationnelle, union qui n'a point encore été formée. »\n\nPour Bacon, comme plus tard pour les scientifiques, la science améliore la condition humaine. Il expose ainsi une utopie scientifique, dans la Nouvelle Atlantide (1627), qui repose sur une société dirigée par « un collège universel » composé de savants et de praticiens.\n\n\n==== De l'« imago mundi » à l'astronomie ====\n\nDirectement permise par les mathématiques de la Renaissance, l'astronomie s'émancipe de la mécanique aristotélicienne, retravaillée par Hipparque et Ptolémée. La théologie médiévale se fonde quant à elle, d'une part sur le modèle d'Aristote, d'autre part sur le dogme de la création biblique du monde. C'est surtout Nicolas Copernic, avec son ouvrage De revolutionibus (1543) qui met fin au modèle aristotélicien de l'immuabilité de la Terre. Sa doctrine a permis l'instauration de l'héliocentrisme : « avec Copernic, et avec lui seul, s'amorce un bouleversement dont sortiront l'astronomie et la physique modernes » explique Jean-Pierre Verdet, Docteur ès sciences. Repris et développé par Georg Joachim Rheticus, l'héliocentrisme sera confirmé par des observations, en particulier celles des phases de Vénus et de Jupiter par Galilée (1564-1642), qui met par ailleurs au point une des premières lunettes astronomiques, qu'il nomme « télescope ». Dans cette période, et avant que Galilée n'intervienne, la théorie de Copernic reste confinée à quelques spécialistes, de sorte qu'elle ne rencontre que des oppositions ponctuelles de la part des théologiens, les astronomes restant le plus souvent favorables à la thèse géocentrique. Néanmoins, en 1616, le Saint-Office publie un décret condamnant le système de Copernic et mettant son ouvrage à l'index. En dépit de cette interdiction, « Galilée adoptera donc la cosmologie de Copernic et construira une nouvelle physique avec le succès et les conséquences que l'on sait », c'est-à-dire qu'il permettra la diffusion des thèses héliocentriques. Kepler dégagera les lois empiriques des mouvements célestes alors que Huygens décrira la force centrifuge. Newton unifiera ces approches en découvrant la gravitation universelle.\n\nLe danois Tycho Brahe observera de nombreux phénomènes astronomiques comme une nova et fondera le premier observatoire astronomique, « Uraniborg ». Il y fit l'observation d'une comète en 1577. Johannes Kepler, l'élève de Brahe qu'il rencontre en 1600, va, quant à lui, amorcer les premiers calculs à des fins astronomiques, en prévoyant précisément un lever de Terre sur la Lune[Quoi ?] et en énonçant ses « trois lois » publiées en 1609 et 16l9. Avec Huygens la géométrie devient la partie centrale de la science astronomique, faisant écho aux mots de Galilée se paraphrasant par l'expression : « le livre du monde est écrit en mathématique ».\nAvec tous ces astronomes, et en l'espace d'un siècle et demi (jusqu'aux Principia de Newton en 1687), la représentation de l'univers passe d'un « monde clos à un monde infini » selon l'expression d'Alexandre Koyré.\n\n\n==== De l'alchimie à la chimie ====\n\nArt ésotérique depuis l'Antiquité, l'alchimie est l'ancêtre de la physique au sens d'observation de la matière. Selon Serge Hutin, docteur ès Lettres spécialiste de l'alchimie, les « rêveries des occultistes » bloquèrent néanmoins le progrès scientifique, surtout au XVIe siècle et au XVIIe siècle. Il retient néanmoins que ces mirages qui nourrirent l'allégorie alchimique ont considérablement influencé la pensée scientifique. L'expérimentation doit ainsi beaucoup aux laboratoires des alchimistes, qui découvrirent de nombreux corps que répertoriés plus tard par la chimie : l'antimoine, l'acide sulfurique ou le phosphore par exemple. Les instruments des alchimistes furent ceux des chimistes modernes, l'alambic par exemple. Selon Serge Hutin, c'est surtout sur la médecine que l'alchimie eut une influence notable, par l'apport de médications minérales et par l'élargissement de la pharmacopée.\nEn dépit de ces faits historiques, le passage de l'alchimie à la chimie demeure complexe. Pour le chimiste Jean-Baptiste Dumas : « La chimie pratique a pris naissance dans les ateliers du forgeron, du potier, du verrier et dans la boutique du parfumeur ». « L'alchimie n'a donc pas joué le rôle unique dans la formation de la chimie ; il n'en reste pas moins que ce rôle a été capital ». Pour la conscience populaire, ce sont les premiers chimistes modernes — comme Antoine Laurent de Lavoisier surtout, au XVIIIe siècle, qui pèse et mesure les éléments chimiques — qui consomment le divorce entre chimie et alchimie. De nombreux philosophes et savants sont ainsi soit à l'origine des alchimistes (Roger Bacon ou Paracelse), soit s'y intéressent, tels Francis Bacon et même, plus tard Isaac Newton. Or, « c'est une erreur de confondre l'alchimie avec la chimie. La chimie moderne est une science qui s'occupe uniquement des formes extérieures dans lesquelles l'élément de la matière se manifeste [alors que] […] L'alchimie ne mélange ou ne compose rien » selon F. Hartmann, pour qui elle est davantage comparable à la botanique. En somme, bien que les deux disciplines soient liées, par l'histoire et leurs acteurs, la différence réside dans la représentation de la matière : combinaisons chimiques pour la chimie, manifestations du monde inanimé comme phénomènes biologiques pour l'alchimie. Pour Bernard Vidal, l'alchimie a surtout « permis d'amasser une connaissance manipulatoire, pratique, de l'objet chimique (…) L'alchimiste a ainsi commencé à débroussailler le champ d'expériences qui sera nécessaire aux chimistes des siècles futurs ».\nLa chimie naît ainsi comme discipline scientifique avec Andreas Libavius (1550-1616) qui publie le premier recueil de chimie, en lien avec la médecine et la pharmacie (il classifie les composés chimiques et donne les méthodes pour les préparer) alors que plus tard Nicolas Lémery (1645-1715) publiera le premier traité de chimie faisant autorité avec son Cours de chimie, contenant la manière de faire les opérations qui sont en usage dans la médecine, par une méthode facile, avec des raisonnements sur chaque opération, pour l’instruction de ceux qui veulent s’appliquer à cette science en 1675. Johann Rudolph Glauber (1604-1668) ou Robert Boyle apportent quant à eux de considérables expérimentations portant sur les éléments chimiques.\n\n\n==== Émergence de la physiologie moderne ====\n\nLes découvertes médicales et les progrès effectués dans la connaissance de l’anatomie, en particulier après la première traduction de nombreuses œuvres antiques d’Hippocrate et de Galien aux XVe siècle et XVIe siècle permettent des avancées en matière d'hygiène et de lutte contre la mortalité. André Vésale jette ainsi les bases de l'anatomie moderne alors que le fonctionnement de la circulation sanguine est découverte par Michel Servet et les premières ligatures des artères sont réalisées par Ambroise Paré.\n\n\n==== Diffusion du savoir ====\nLe domaine des techniques progresse considérablement grâce à l’invention de l’imprimerie par Johannes Gutenberg au XVe siècle, invention qui bouleverse la transmission du savoir.\nLe nombre de livres publiés devient ainsi exponentiel, la scolarisation de masse est possible, par ailleurs les savants peuvent débattre par l'intermédiaire des comptes-rendus de leurs expérimentations. La science devient ainsi une communauté de savants. Les académies des sciences surgissent, à Londres, Paris, Saint-Pétersbourg et Berlin.\nLes journaux et périodiques prolifèrent, tels le Journal des sçavans, Acta Eruditorum, Mémoires de Trevoux, etc. mais les domaines du savoir y sont encore mêlés et ne constituent pas encore totalement des disciplines. La science, bien que s'institutionnalisant, fait encore partie du champ de l'investigation philosophique. Michel Blay dit ainsi : « il est très surprenant et finalement très anachronique de séparer, pour la période classique, l'histoire des sciences de l'histoire de la philosophie, et aussi de ce que l'on appelle l'histoire littéraire ».\n\nFinalement la Renaissance permet, pour les disciplines scientifiques de la matière, la création de disciplines et d'épistémologies distinctes mais réunies par la scientificité, elle-même permise par les mathématiques, car, selon l'expression de Pascal Brioist : « la mathématisation d’une pratique conduit à lui donner le titre spécifique de science ». Michel Blay voit ainsi dans les débats autour de concepts clés, comme ceux d'absolu ou de mouvement, de temps et d'espace, les éléments d'une science classique.\n\n\n=== Les « Lumières » et les grands systèmes scientifiques ===\nAu XVIIe siècle, la « révolution scientifique » est permise par la mathématisation de la science. Les universités occidentales avaient commencé à apparaître au XIe siècle, mais ce n'est qu'au cours du XVIIe siècle qu'apparaissent les autres institutions scientifiques, notamment l'Accademia dei Lincei, fondée en 1603 (ancêtre de l'Académie pontificale des sciences), les académies des sciences, les sociétés savantes. Les sciences naturelles et la médecine surtout se développèrent durant cette période.\n\n\n==== L'Encyclopédie ====\nUn second changement important dans le mouvement des Lumières par rapport au siècle précédent trouve son origine en France, avec les Encyclopédistes. Ce mouvement intellectuel défend l’idée qu’il existe une architecture scientifique et morale du savoir. Le philosophe Denis Diderot et le mathématicien Jean Le Rond d'Alembert publient en 1751 l’Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers qui permet de faire le point sur l'état du savoir de l'époque. L'Encyclopédie devient ainsi un hymne au progrès scientifique.\n\nAvec l'Encyclopédie naît également la conception classique que la science doit son apparition à la découverte de la méthode expérimentale. d'Alembert explique ainsi, dans le Discours préliminaire de l'Encyclopédie (1759) que :\n\n« Ce n'est point par des hypothèses vagues et arbitraires que nous pouvons espérer de connaître la nature, c'est (…) par l'art de réduire autant qu'il sera possible, un grand nombre de phénomènes à un seul qui puisse en être regardé comme le principe (…). Cette réduction constitue le véritable esprit systématique, qu'il faut bien se garder de prendre pour l'esprit de système »\n\n\n==== Rationalisme et science moderne ====\n\nLa période dite des Lumières initie la montée du courant rationaliste, provenant de René Descartes puis des philosophes anglais, comme Thomas Hobbes et David Hume, qui adoptent une démarche empirique, mettant l’accent sur les sens et l’expérience dans l’acquisition des connaissances, au détriment de la raison pure. Des penseurs, également scientifiques (comme Gottfried Wilhelm Leibniz, qui développe les mathématiques et le calcul infinitésimal, ou Emmanuel Kant, le baron d'Holbach, dans Système de la nature, dans lequel il soutient l’athéisme contre toute conception religieuse ou déiste, le matérialisme et le fatalisme c'est-à-dire le déterminisme scientifique, ou encore Pierre Bayle avec ses Pensées diverses sur la comète) font de la Raison (avec une majuscule) un culte au progrès et au développement social. Les découvertes d'Isaac Newton, sa capacité à confronter et à assembler les preuves axiomatiques et les observations physiques en un système cohérent donnent le ton de tout ce qui suit son exemplaire Philosophiae Naturalis Principia Mathematica. En énonçant en effet la théorie de la gravitation universelle, Newton inaugure l'idée d'une science comme discours tendant à expliquer le monde, considéré comme rationnel car ordonné par des lois reproductibles.\nL'avènement du sujet pensant, en tant qu'individu qui peut décider par son raisonnement propre et non plus sous le seul joug des us et coutumes, avec John Locke, permet la naissance des sciences humaines, comme l'économie, la démographie, la géographie ou encore la psychologie.\n\n\n==== Naissance des grandes disciplines scientifiques ====\n\nLa majorité des disciplines majeures de la science se consolident, dans leurs épistémologies et leurs méthodes, au XVIIIe siècle. La botanique apparaît avec Carl von Linné qui publie en 1753 Species plantarum, point du départ du système du binôme linnéen et de la nomenclature botanique. La chimie naît par ailleurs avec Antoine Laurent de Lavoisier, qui énonce en 1778 la loi de conservation de la matière, identifie et baptise l'oxygène. Les sciences de la terre font aussi leur apparition. Comme discipline, la médecine progresse également avec la constitution des examens cliniques et les premières classification des maladies par William Cullen et François Boissier de Sauvages de Lacroix.\n\n\n=== XIXe siècle ===\nLa biologie connaît au XIXe siècle de profonds bouleversements avec la naissance de la génétique, à la suite des travaux de Gregor Mendel, le développement de la physiologie, l'abandon du vitalisme à la suite de la synthèse de l'urée qui démontre que les composés organiques obéissent aux mêmes lois physico-chimiques que les composés inorganiques. L'opposition entre science et religion se renforce avec la parution de L'Origine des espèces en 1859 de Charles Darwin. Les sciences humaines naissent, la sociologie avec Auguste Comte, la psychologie avec Charcot et Wilhelm Maximilian Wundt.\n\n\n==== Claude Bernard et la méthode expérimentale ====\n\nClaude Bernard (1813-1878) est un médecin et physiologiste, connu pour l'étude du syndrome de Claude Bernard-Horner. Il est considéré comme le fondateur de la médecine expérimentale,. Il rédige la première méthode expérimentale, considérée comme le modèle à suivre de la pratique scientifique. Il énonce ainsi les axiomes de la méthode médicale dans Introduction à l'étude de la médecine expérimentale (1865) et en premier lieu l'idée que l'observation doit réfuter ou valider la théorie : \n« La théorie est l’hypothèse vérifiée après qu’elle a été soumise au contrôle du raisonnement et de la critique. Une théorie, pour rester bonne, doit toujours se modifier avec le progrès de la science et demeurer constamment soumise à la vérification et la critique des faits nouveaux qui apparaissent. Si l’on considérait une théorie comme parfaite, et si on cessait de la vérifier par l’expérience scientifique, elle deviendrait une doctrine »\n\n\n==== Révolution industrielle ====\n\nLes Première et Seconde Révolutions Industrielles sont marquées par de profonds bouleversements économiques et sociaux, permis par les innovations et découvertes scientifiques et techniques. La vapeur, puis l'électricité comptent parmi ces progrès notables qui ont permis l'amélioration des transports et de la production. Les instruments scientifiques sont plus nombreux et plus sûrs, tels le microscope (à l'aide duquel Louis Pasteur découvre les microbes) ou le télescope se perfectionnent. La physique acquiert ses principales lois, notamment avec James Clerk Maxwell qui, énonce les principes de la théorie cinétique des gaz ainsi que l'équation d'onde fondant l'électromagnétisme. Ces deux découvertes permirent d'importants travaux ultérieurs notamment en relativité restreinte et en mécanique quantique. Il esquisse ainsi les fondements des sciences du XXe siècle, notamment les principes de la physique des particules, à propos de la nature de la lumière.\n\n\n=== Une science « post-industrielle » ===\nTout comme le XIXe siècle, le XXe siècle connaît une accélération importante des découvertes scientifiques. On note l'amélioration de la précision des instruments, qui eux-mêmes reposent sur les avancées les plus récentes de la science ; l'informatique qui se développe à partir des années 1950 et permet un meilleur traitement d'une masse d'informations toujours plus importante et aboutit à révolutionner la pratique de la recherche, est un de ces instruments.\nLes échanges internationaux des connaissances scientifiques sont de plus en plus rapides et faciles (ce qui se traduit par des enjeux linguistiques) ; toutefois, les découvertes les plus connues du XXe siècle précèdent la véritable mondialisation et l'uniformisation linguistique des publications scientifiques. En 1971, la firme Intel met au point le premier micro-processeur et, en 1976, Apple commercialise le premier ordinateur de bureau. Dans La Société post-industrielle. Naissance d'une société, le sociologue Alain Touraine présente les caractéristiques d'une science au service de l'économie et de la prospérité matérielle.\n\n\n==== Complexification des sciences ====\nDe « révolutions scientifiques » en révolutions scientifiques, la science voit ses disciplines se spécialiser. La complexification des sciences explose au XXe siècle, conjointement à la multiplication des champs d'étude. Parallèlement, les sciences viennent à se rapprocher voire à travailler ensemble. C'est ainsi que, par exemple, la biologie fait appel à la chimie et à la physique, tandis que cette dernière utilise l'astronomie pour confirmer ou infirmer ses théories (développant l'astrophysique). Les mathématiques deviennent le « langage » commun des sciences ; les applications étant multiples. Le cas de la biologie est exemplaire. Elle se divise en effet en de nombreuses branches : biologie moléculaire, biochimie, biologie génétique, agrobiologie, etc.\n\nLa somme des connaissances devient telle qu'il est impossible pour un scientifique de connaître parfaitement plusieurs branches de la science. C'est ainsi qu'ils se spécialisent de plus en plus et, pour contrebalancer cela, le travail en équipe devient la norme. Cette complexification rend la science de plus en plus abstraite pour ceux qui ne participent pas aux découvertes scientifiques, en dépit de programmes nationaux et internationaux (sous l'égide de l'ONU, avec l'Organisation des Nations unies pour l'éducation, la science et la culture (UNESCO)) de vulgarisation des savoirs.\n\n\n==== Développement des sciences sociales ====\nLe siècle est également marqué par le développement des sciences sociales. Celles-ci comportent de nombreuses disciplines comme l'anthropologie, la sociologie, l'ethnologie, l'histoire, la psychologie, la linguistique, la philosophie, l'archéologie, l'économie, entre autres.\n\n\n=== Éthique et science : l'avenir de la science au XXIe siècle ===\nLe XXIe siècle est caractérisé par une accélération des découvertes de pointe, comme la nanotechnologie. Par ailleurs, au sein des sciences naturelles, la génétique promet des changements sociaux ou biologiques sans précédent. L'informatique est par ailleurs à la fois une science et un instrument de recherche puisque la simulation informatique permet d'expérimenter des modèles toujours plus complexes et gourmands en termes de puissance de calcul. La science se démocratise d'une part : des projets internationaux voient le jour (lutte contre le SIDA et le cancer, programme SETI, astronomie, détecteurs de particules, etc.) ; d'autre part la vulgarisation scientifique permet de faire accéder toujours plus de personnes au raisonnement et à la curiosité scientifique.\n\nL'éthique devient une notion concomitante à celle de science. Les nanotechnologies et la génétique surtout posent les problèmes de société futurs, à savoir, respectivement, les dangers des innovations pour la santé et la manipulation du patrimoine héréditaire de l'homme. Les pays avancés technologiquement créent ainsi des organes institutionnels chargé d'examiner le bien-fondé des applications scientifiques. Par exemple, des lois bioéthiques se mettent en place à travers le monde, mais pas partout de la même manière, étant très liées aux droits locaux. En France, le Comité Consultatif National d'Éthique est chargé de donner un cadre légal aux découvertes scientifiques.\n\n\n== Disciplines scientifiques ==\nLa science peut être organisée en grandes disciplines scientifiques, notamment : mathématiques, chimie, biologie, géologie, physique, mécanique, informatique, psychologie, optique, pharmacie, médecine, astronomie, archéologie, économie, sociologie, anthropologie, linguistique, géographie. Les disciplines ne se distinguent pas seulement par leurs méthodes ou leurs objets, mais aussi par leurs institutions : revues, sociétés savantes, chaires d'enseignement, ou même leurs diplômes.\n\n\n== Classification des sciences ==\nPlusieurs axes de classification des disciplines existent et sont présentées dans cette section :\naxe de la finalité : sciences fondamentales (ex. : l'astronomie) / sciences appliquées (ex. : les sciences de l'ingénieur) ;\naxe par nature (catégories). Après un classement par deux, puis par trois dans l'histoire des sciences, la pratique retient maintenant quatre catégories :\nles sciences formelles (ou sciences logico-formelles),\nles sciences physiques,\nles sciences de la vie,\nles sciences sociales ;\naxe méthodologique.\nPar ailleurs, le terme de « science pure » est parfois employé pour catégoriser les sciences formelles (la mathématique et la logique, essentiellement) ou fondamentales, selon le sens, qui sont construites sur des entités purement abstraites, tandis que les sciences, technologies, ingénierie et mathématiques (STEM) regroupent les sciences formelles et naturelles.\n\nLes sciences sociales, comme la sociologie, portent sur l'étude des phénomènes sociaux, les secondes, comme la physique, portent sur l'étude des phénomènes naturels. Plus récemment, quelques auteurs, comme Herbert Simon,, ont évoqué l'apparition d'une catégorie intermédiaire, celle des sciences de l'artificiel, qui portent sur l'étude de systèmes créés par l'homme, mais qui présentent un comportement indépendant ou relativement à l'action humaine. Il s'agit par exemple des sciences de l'ingénieur.\nOn peut également distinguer les sciences empiriques, qui portent sur l'étude des phénomènes accessibles par l'observation et l'expérimentation, des sciences logico-formelles, comme la logique ou les mathématiques, qui portent sur des entités purement abstraites. Une autre manière de catégoriser les sciences consiste à distinguer les sciences fondamentales, dont le but premier est de produire des connaissances, des sciences appliquées, qui visent avant tout à appliquer ces connaissances à la résolution de problèmes concrets. D'autres catégorisations existent, notamment la notion de science exacte ou de science dure. Ces dernières catégorisations, bien que très courantes, sont beaucoup plus discutables que les autres, car elles sont porteuses d'un jugement (certaines sciences seraient plus exactes que d'autres, certaines sciences seraient « molles »).\nEn outre, certains savants, comme Paul Oppenheim, ont proposé une classification des sciences les imbriquant les unes dans les autres, selon le principe des poupées russes.\nDe manière générale, aucune catégorisation n'est complètement exacte ni entièrement justifiable, et les zones épistémologiques entre elles demeurent floues. Pour Robert Nadeau : « on reconnaît généralement qu’on peut classer [les sciences] selon leur objet (…), selon leur méthode (…), et selon leur but. »\n\n\n=== Sciences fondamentales et appliquées ===\n\nCette classification première repose sur la notion d'utilité : certaines sciences produisent des connaissances en sorte d’agir sur le monde (les sciences appliquées, qu'il ne faut pas confondre avec la technique en tant qu'application de connaissances empiriques), c’est-à-dire dans la perspective d’un objectif pratique, économique ou industriel, tandis que d'autres (les sciences fondamentales) visent en priorité l’acquisition de nouvelles connaissances.\nNéanmoins, cette limite est floue. Les mathématiques, la physique, la chimie, la sociologie ou la biologie peuvent ainsi aussi bien être fondamentales qu'appliquées, selon le contexte. En effet, Les découvertes issues de la science fondamentale trouvent des fins utiles (exemple : le laser et son application au son numérique sur CD-ROM). De même, certains problèmes techniques mènent parfois à de nouvelles découvertes en science fondamentale. Ainsi, les laboratoires de recherche et les chercheurs peuvent faire parallèlement de la recherche appliquée et de la recherche fondamentale. Par ailleurs, la recherche en sciences fondamentales utilise les technologies issues de la science appliquée, comme la microscopie, les possibilités de calcul des ordinateurs par la simulation numérique, par exemple.\n\nCertaines disciplines restent cependant plus ancrées dans un domaine que dans un autre. La cosmologie et l'astronomie sont par exemple des sciences exclusivement fondamentales tandis que la médecine, la pédagogie ou l'ingénierie sont des sciences essentiellement appliquées.\nPar ailleurs, les mathématiques sont souvent considérées comme autre chose qu'une science, en partie parce que la vérité mathématique n'a rien à voir avec la vérité des autres sciences. L'objet des mathématiques est en effet interne à cette discipline. Ainsi, sur cette base, les mathématiques appliquées souvent perçues davantage comme une branche mathématique au service d'autres sciences (comme le démontrent les travaux du mathématicien Jacques-Louis Lions qui explique : « Ce que j'aime dans les mathématiques appliquées, c'est qu'elles ont pour ambition de donner du monde des systèmes une représentation qui permette de comprendre et d'agir ») seraient bien plutôt sans finalité pratique. A contrario, les mathématiques possèdent un nombre important de branches, d'abord abstraites, s'étant développées au contact avec d'autres disciplines comme les statistiques, la théorie des jeux, la logique combinatoire, la théorie de l'information, la théorie des graphes entre autres exemples, autant de branches qui ne sont pas catalogués dans les mathématiques appliquées mais qui pourtant irriguent d'autres branches scientifiques.\n\n\n=== Sciences nomothétiques et idiographiques ===\nUn classement des sciences peut s'appuyer sur les méthodes mises en œuvre. Une première distinction de cet ordre peut être faite entre les sciences nomothétiques et les sciences idiographiques :\n\nles sciences nomothétiques cherchent à établir des lois générales pour des phénomènes susceptibles de se reproduire : on y retrouve la physique et la biologie, mais également des sciences humaines ou sociales comme l'économie, la psychologie ou même la sociologie ;\nles sciences idiographiques s'occupent au contraire du singulier, de l'unique, du non récurrent. L'exemple de l'histoire montre qu'il n'est pas absurde de considérer que le singulier peut être justiciable d'une approche scientifique.\n\nC'est à Wilhelm Windelband, philosophe allemand du XIXe siècle, que l'on doit la première ébauche de cette distinction, la réflexion de Windelband portant sur la nature des sciences sociales. Dans son Histoire et science de la nature (1894), il soutient que l'opposition entre sciences de la nature et de l'esprit repose sur une distinction de méthode et de « formes d'objectivation ». Jean Piaget reprendra le vocable de nomothétique pour désigner les disciplines cherchant à dégager des lois ou des relations quantitatives en utilisant des méthodes d'expérimentation stricte ou systématique. Il cite la psychologie scientifique, la sociologie, la linguistique, l'économie et la démographie. Il distingue ces disciplines des sciences historiques, juridiques et philosophiques.\n\n\n=== Sciences empiriques et logico-formelles ===\n\nUne catégorisation a été proposée par l'épistémologie, distinguant les « sciences empiriques » et les « sciences logico-formelles ». Leur point commun reste les mathématiques et leur usage dans les disciplines liées ; cependant, selon les mots de Gilles-Gaston Granger, « la réalité n'est pas aussi simple. Car, d'une part, c'est souvent à propos de questions posées par l'observation empirique que des concepts mathématiques ont été dégagés ; d'autre part, si la mathématique n'est pas une science de la nature, elle n'en a pas moins de véritables objets ». Selon Léna Soler, dans son Introduction à l’épistémologie, distingue d’une part les sciences formelles des sciences empiriques, d’autre part les sciences de la nature des sciences humaines et sociale :\n\nles sciences dites empiriques portent sur le monde accessible par l'expérience et par les sens. Elles regroupent : les sciences de la nature, qui étudient les phénomènes naturels ; les sciences humaines étudiant l'Homme et ses comportements individuels et collectifs, passés et présents ;\nde leur côté, les sciences logico-formelles (ou sciences formelles) explorent par la déduction, selon des règles de formation et de démonstration, des systèmes axiomatiques. Il s'agit par exemple des mathématiques ou de la logique.\n\n\n=== Sciences de la nature et sciences sociales ===\n\nSelon Gilles Gaston Granger, il existe une autre sorte d'opposition épistémologique, distinguant d'une part les sciences de la nature, qui ont des objets émanant du monde sensible, mesurables et classables ; d'autre part les sciences de l'homme aussi dites sciences humaines, pour lesquelles l'objet est abstrait. Gilles-Gaston Granger récuse par ailleurs de faire de l'étude du phénomène humain une science proprement dite :\n\nles sciences sociales sont celles qui ont pour objet d'étude les phénomènes sociaux; les sociétés, leur histoire, leurs cultures, leurs réalisations et leurs comportements ;\nles sciences de la nature, ou « sciences naturelles » (« Natural science » en anglais) ont pour objet le monde naturel, la Terre et l'Univers.\nLe sens commun associe une discipline à un objet. Par exemple la sociologie s’occupe de la société, la psychologie de la pensée, la physique s’occupe de phénomènes mécaniques, thermiques, la chimie s’occupe des réactions de la matière. La recherche moderne montre néanmoins l’absence de frontière et la nécessité de développer des transversalités ; par exemple, pour certaines disciplines on parle de « physico-chimique » ou de « chimio-biologique », expressions qui permettent de montrer les liens forts des spécialités entre elles. Une discipline est finalement définie par l’ensemble des référentiels qu’elle utilise pour étudier un ensemble d’objets, ce qui forme sa scientificité. Néanmoins, ce critère n'est pas absolu.\nPour le sociologue Raymond Boudon, il n'existe pas une scientificité unique et transdisciplinaire. Il s’appuie ainsi sur la notion d’« airs de famille », notion déjà théorisée par le philosophe Ludwig Wittgenstein selon laquelle il n'existe que des ressemblances formelles entre les sciences, sans pour autant en tirer une règle générale permettant de dire ce qu'est « la science ». Raymond Boudon, dans L’art de se persuader des idées douteuses, fragiles ou fausses explique que le relativisme « s'il est une idée reçue bien installée […], repose sur des bases fragiles » et que, contrairement à ce que prêche Feyerabend, « il n'y a pas lieu de congédier la raison ».\n\n\n=== Classification des Sciences de l'Homme et sociales (SHS) en France ===\n\nAu niveau de la recherche scientifique en France, le classement des disciplines est le suivant dans la nouvelle nomenclature (2010) de la stratégie nationale pour la recherche et l'innovation (SNRI) des Sciences de l'Homme et de la Société (SHS) :\n\nSHS1 : Marchés et organisations (économie, finances, management)\nSHS2 : Normes, institutions et comportements sociaux (Droit, science politique, sociologie, anthropologie, ethnologie, démographie, information et communication)\nSHS3 : Espace, environnement et sociétés (Études environnementales, géographie physique, géographie sociale, géographie urbaine et régionale, aménagement du territoire)\nSHS4 : Esprit humain, langage, éducation (Sciences cognitives, sciences du langage, psychologie, sciences de l'éducation, STAPS)\nSHS5 : Langues, textes, arts et cultures (Langues, littérature, arts, philosophie, religion, histoire des idées)\nSHS6 : Mondes anciens et contemporains (Préhistoire, archéologie, histoire, histoire de l'art)\n\n\n== Raisonnement scientifique ==\n\n\n=== Type formel pur ===\n\nSelon Emmanuel Kant, la logique formelle est « science qui expose dans le détail et prouve de manière stricte, uniquement les règles formelles de toute pensée ». Les mathématiques et la logique formalisées composent ce type de raisonnement. Cette classe se fonde par ailleurs sur deux principes constitutifs des systèmes formels : l'axiome et les règles de déduction ainsi que sur la notion de syllogisme, exprimée par Aristote le premier et liée au « raisonnement déductif » (on parle aussi de raisonnement « hypothético-déductif »), qu'il expose dans ses Topiques et dans son traité sur la logique : Les Analytiques.\nIl s'agit également du type qui est le plus adéquat à la réalité, celui qui a fait le plus ses preuves, par la technique notamment. Le maître-mot du type formel pur est la démonstration logique et non-contradictoire (entendu comme la démonstration qu'on ne pourra dériver dans le système étudié n'importe quelle proposition). En d'autres termes, il ne s'agit pas à proprement parler d'un raisonnement sur l'objet mais bien plutôt d'une méthode pour traiter les faits au sein des démonstrations scientifiques et portant sur les propositions et les postulats.\nOn distingue ainsi dans ce type deux disciplines fondamentales :\n\nla logique de la déduction naturelle ;\nla logique combinatoire.\nLe type formel fut particulièrement développé au XXe siècle, avec le logicisme et la philosophie analytique. Bertrand Russell développe en effet une « méthode atomique » (ou atomisme logique) qui s’efforce de diviser le langage en ses parties élémentaires, ses structures minimales, la phrase simple en somme. Wittgenstein projetait en effet d’élaborer un langage formel commun à toutes les sciences permettant d'éviter le recours au langage naturel, et dont le calcul propositionnel représente l'aboutissement. Cependant, en dépit d'une stabilité épistémologique propre, a contrario des autres types, le type formel pur est également largement tributaire de l'historicité des sciences\n\n\n=== Type empirico-formel ===\n\nLe modèle de ce type, fondé sur l'empirisme, est la physique. L'objet est ici concret et extérieur, non construit par la discipline (comme dans le cas du type formel pur). Ce type est en fait la réunion de deux composantes :\n\nd'une part il se fonde sur la théorique formelle, les mathématiques (la physique fondamentale par exemple) ;\nd'autre part la dimension expérimentale est complémentaire (la méthode scientifique).\n\nLe type empirico-formel progresse ainsi de la théorie — donnée comme a priori — à l'empirie, puis revient sur la première via un raisonnement circulaire destiné à confirmer ou réfuter les axiomes. Le « modèle » est alors l'intermédiaire entre la théorie et la pratique. Il s'agit d'une schématisation permettant d'éprouver ponctuellement la théorie. La notion de « théorie » est depuis longtemps centrale en philosophie des sciences, mais elle est remplacée, sous l'impulsion empiriste, par celle de modèle, dès le milieu du XXe siècle. L'expérience (au sens de mise en pratique) est ici centrale, selon l'expression de Karl Popper : « Un système faisant partie de la science empirique doit pouvoir être réfuté par l'expérience ».\nParmi les sciences empiriques, on distingue deux grandes familles de sciences : les sciences de la nature et les sciences humaines. Néanmoins, l'empirisme seul ne permet pas, en se coupant de l'imagination, d'élaborer des théories novatrices, fondées sur l'intuition du scientifique, permettant de dépasser des contradictions que la simple observation des faits ne pourrait résoudre.\nDes débats portent néanmoins quant à la nature empirique de certaines sciences humaines, comme l'économie ou l'histoire, qui ne reposent pas sur une méthode totalement empirique, l'objet étant virtuel dans les deux disciplines.\n\n\n=== Type herméneutique ===\n\nLes sciences herméneutiques (du grec hermeneutikè, « art d'interpréter ») décodent les signes naturels et établissent des interprétations. Ce type de discours scientifique est caractéristique des sciences humaines, où l'objet est l'homme. Dans la méthode herméneutique, les effets visibles sont considérés comme un texte à décoder, à la signification cachée. La phénoménologie est ainsi l'explication philosophique la plus proche de ce type, qui regroupe, entre autres, la sociologie, la linguistique, l'économie, l'ethnologie, la théorie des jeux, etc.\nIl peut s'agir dès lors de deux catégories de discours :\n\nl'intention première est alors l'objet de la recherche herméneutique, exemple : dans la psychologie ;\nl'interprétation est aussi possible : la théorie prévoit les phénomènes, simule les relations et les effets mais l'objet reste invisible (cas de la psychanalyse).\nPar rapport aux deux autres types formels, le statut scientifique du type herméneutique est contesté par les tenants d'une science mathématique, dite « dure ».\n\nÀ la conception de l’unité de la science postulée par le positivisme tout un courant de pensée va, à la suite de Wilhelm Dilthey (1833-1911), affirmer l’existence d’une coupure radicale entre les sciences de la nature et les sciences de l’esprit. Les sciences de la nature ne cherchent qu'à expliquer leur objet, tandis que les sciences de l'homme, et l'histoire en particulier, demandent également à comprendre de l'intérieur et donc à prendre en considération le vécu. Ces dernières ne doivent pas adopter la méthode en usage dans les sciences de la nature car elles ont un objet qui lui est totalement différent. Les sciences sociales doivent être l'objet d'une introspection, ce que Wilhelm Dilthey appelle une « démarche herméneutique », c’est-à-dire une démarche d’interprétation des manifestations concrètes de l’esprit humain. Le type herméneutique marque le XXe siècle, avec des auteurs comme Hans-Georg Gadamer qui publia en 1960, Vérité et Méthode qui, s'opposant à l'empirisme tout-puissant, affirme que « la méthode ne suffit pas ».\n\n\n== Scientificité et méthode scientifique ==\n\nLa connaissance acquise ne peut être qualifiée de scientifique que si la scientificité des processus d'obtention a été démontrée.\nLa « méthode scientifique » (grec ancien méthodos, « poursuite, recherche, plan ») est « l'ensemble des procédés raisonnés pour atteindre un but ; celui-ci peut être de conduire un raisonnement selon des règles de rectitude logique, de résoudre un problème de mathématique, de mener une expérimentation pour tester une hypothèse scientifique ». Elle est étroitement liée au but recherché et à l'histoire des sciences. La méthode scientifique suit par ailleurs cinq opérations distinctes :\n\nexpérimentation ;\nobservation ;\nthéorie et modèle ;\nsimulation ;\npublication et validation.\n\n\n=== Scientificité ===\nLa scientificité est la qualité des pratiques et des théories qui cherchent à établir des régularités reproductibles, mesurables et réfutables dans les phénomènes par le moyen de la mesure expérimentale, et à en fournir une représentation explicite.\nPlus généralement, c'est le « caractère de ce qui répond aux critères de la science ». De manière générale à toutes les sciences, la méthode scientifique repose sur quatre critères :\n\nelle est systématique (le protocole doit s'appliquer à tous les cas, de la même façon) ;\nelle fait preuve d'objectivité (c'est le principe du « double-aveugle » : les données doivent être contrôlées par des collègues chercheurs - c'est le rôle de la publication) ;\nelle est rigoureuse, testable (par l'expérimentation et les modèles scientifiques) ;\net enfin, elle doit être cohérente (les théories ne doivent pas se contredire, dans une même discipline).\nNéanmoins, chacun de ces points est problématique, et les questionnements de l'épistémologie portent principalement sur les critères de scientificité. Ainsi, concernant la cohérence interne aux disciplines, l'épistémologue Thomas Samuel Kuhn bat en brèche ce critère de scientificité, en posant que les paradigmes subissent des « révolutions scientifiques » : un modèle n'est valable tant qu'il n'est pas remis en cause. Le principe d'objectivité, qui est souvent présenté comme l'apanage de la science, est, de même, source d'interrogations, surtout au sein des sciences humaines.\nPour le sociologue de la science Roberto Miguelez : « Il semble bien que l'idée de la science suppose, premièrement, celle d'une logique de l'activité scientifique ; deuxièmement, celle d'une syntaxe du discours scientifique. En d'autres termes, il semble bien que, pour pouvoir parler de la science, il faut postuler l'existence d'un ensemble de règles - et d'un seul - pour le traitement des problèmes scientifiques - ce qu'on appellera alors « la méthode scientifique » -, et d'un ensemble de règles - et d'un seul - pour la construction d'un discours scientifique ». La sociologie des sciences étudie en effet de plus en plus les critères de scientificité, au sein de l'espace social scientifique, passant d'une vision interne, celle de l'épistémologie, à une vision davantage globale.\n\n\n=== Expérimentation ===\n\nL'« expérimentation » est une méthode scientifique qui consiste à tester par des expériences répétées la validité d'une hypothèse et à obtenir des données quantitatives permettant de l'affiner. Elle repose sur des protocoles expérimentaux permettant de normaliser la démarche. La physique ou la biologie reposent sur une démarche active du scientifique qui construit et contrôle un dispositif expérimental reproduisant certains aspects des phénomènes naturels étudiés. La plupart des sciences emploient ainsi la méthode expérimentale, dont le protocole est adapté à son objet et à sa scientificité. De manière générale, une expérience doit apporter des précisions quantifiées (ou statistiques) permettant de réfuter ou d'étayer le modèle. Les résultats des expériences ne sont pas toujours quantifiables, comme dans les sciences humaines. L'expérience doit ainsi pouvoir réfuter les modèles théoriques.\nL'expérimentation a été mise en avant par le courant de l'empirisme. Néanmoins, le logicien et scientifique Charles Sanders Peirce (1839-1914), et plus tard mais indépendamment, l'épistémologue Karl Popper (1902-1994), lui opposent l'abduction (ou méthode par conjecture et réfutation) comme étape première de la recherche scientifique. L'abduction (ou conjecture) est un procédé consistant à introduire une règle à titre d’hypothèse afin de considérer ce résultat comme un cas particulier tombant sous cette règle. Elle consiste en l'invention a priori d'une conjecture précédant l'expérience. En somme, cela signifie que l'induction fournit directement la théorie, alors que dans le processus abductif, la théorie est inventée avant l'expérience et cette dernière ne fait que répondre par l'affirmative ou par la négative à l'hypothèse.\n\n\n=== Observation ===\n\nL’« observation » est l’action de suivi attentif des phénomènes, sans volonté de les modifier, à l’aide de moyens d’enquête et d’étude appropriés. Les scientifiques y ont recours principalement lorsqu'ils suivent une méthode empirique. C'est par exemple le cas en astronomie ou en physique. Il s'agit d'observer le phénomène ou l'objet sans le dénaturer, ou même interférer avec sa réalité. Certaines sciences, comme la physique quantique ou la psychologie, prennent en compte l'observation comme un paradigme explicatif à part entière, influençant le comportement de l'objet observé. La philosophe Catherine Chevalley résume ainsi ce nouveau statut de l'observation : « Le propre de la théorie quantique est de rendre caduque la situation classique d’un « objet » existant indépendamment de l’observation qui en est faite ».\nLa science définit la notion d’observation dans le cadre de l’approche objective de la connaissance, observation permise par une mesure et suivant un protocole fixé d'avance.\n\n\n=== Théorie et modèle ===\nUne « théorie » (du grec theoria soit « vision du monde ») est un modèle ou un cadre de travail pour la compréhension de la nature et de l'humain. En physique, le terme de théorie désigne généralement le support mathématique, dérivé d'un petit ensemble de principes de base et d'équations, permettant de produire des prévisions expérimentales pour une catégorie donnée de systèmes physiques. Un exemple est la « théorie électromagnétique », habituellement confondue avec l'électromagnétisme classique, et dont les résultats spécifiques sont obtenus à partir des équations de Maxwell. L’adjectif « théorique » adjoint à la description d'un phénomène indique souvent qu'un résultat particulier a été prédit par une théorie mais qu'il n'a pas encore été observé. La théorie est ainsi bien souvent plus un modèle entre l'expérimentation et l'observation qui reste à confirmer.\n\nLa conception scientifique de la théorie devient ainsi une phase provisoire de la méthode expérimentale. Claude Bernard, dans son Introduction à la médecine expérimentale appuie sur le rôle clé des questions et sur l'importance de l'imagination dans la construction des hypothèses, sorte de théories en voie de développement. Le neurobiologiste Jean-Pierre Changeux explique ainsi : \n« Le scientifique construit des « modèles » qu'il confronte au réel. Il les projette sur le monde ou les rejette en fonction de leur adéquation avec celui-ci sans toutefois prétendre l'épuiser. La démarche du scientifique est débat critique, « improvisation déconcertante », hésitation, toujours consciente de ses limites »\n\nEn effet, si l'expérimentation est prépondérante, elle ne suffit pas, conformément à la maxime de Claude Bernard : « La méthode expérimentale ne donnera pas d'idée neuve à ceux qui n'en ont pas », la théorie et le modèle permettant d'éprouver la réalité a priori.\n\n\n=== Simulation ===\n\nLa « simulation » est la « reproduction artificielle du fonctionnement d'un appareil, d'une machine, d'un système, d'un phénomène, à l'aide d'une maquette ou d'un programme informatique, à des fins d'étude, de démonstration ou d'explication ». Elle est directement liée à l'utilisation de l'informatique au XXe siècle. Il existe deux types de simulations :\n\nLa modélisation physique consiste spécifiquement à utiliser un autre phénomène physique que celui observé, mais en y appliquant des lois ayant les mêmes propriétés et les mêmes équations. Un modèle mathématique est ainsi une traduction de la réalité pour pouvoir lui appliquer les outils, les techniques et les théories mathématiques. Il y a alors deux types de modélisations : les modèles prédictifs (qui anticipent des événements ou des situations, comme ceux qui prévoient le temps avec la météorologie) et les modèles descriptifs (qui représentent des données historiques).\nLa simulation numérique utilise elle un programme spécifique ou éventuellement un progiciel plus général, qui génère davantage de souplesse et de puissance de calcul. Les simulateurs de vol d’avions par exemple permettent d'entraîner les pilotes. En recherche fondamentale les simulations que l'on nomme aussi « modélisations numériques » permettent de reproduire des phénomènes complexes, souvent invisibles ou trop ténus, comme la collision de particules.\n\n\n=== Publication et littérature scientifique ===\n\nLe terme de « publication scientifique » regroupe plusieurs types de communications que les chercheurs font de leurs travaux en direction d'un public de spécialistes, et ayant subi une forme d'examen de la rigueur de la méthode scientifique employée pour ces travaux, comme l'examen par un comité de lecture indépendant par exemple. La publication scientifique est donc la validation de travaux par la communauté scientifique. C'est aussi le lieu de débats contradictoires à propos de sujets polémiques ou de discussions de méthodes.\nIl existe ainsi plusieurs modes de publications :\n\nles revues scientifiques à comité de lecture ;\nles comptes-rendus de congrès scientifique à comité de lecture ;\ndes ouvrages collectifs rassemblant des articles de revue ou de recherche autour d'un thème donné, coordonnés par un ou plusieurs chercheurs appelés éditeurs ;\ndes monographies sur un thème de recherche.\n\nLes publications qui entrent dans un des cadres ci-dessus sont généralement les seules considérées pour l'évaluation des chercheurs et les études bibliométriques, à tel point que l'adage « publish or perish » (publier ou périr) est fondé. La scientométrie est en effet une méthode statistique appliquée aux publications scientifiques. Elle est utilisée par les organismes finançant la recherche comme outil d'évaluation. En France, ces indicateurs, tel le facteur d'impact, occupent ainsi une place importante dans la LOLF (pour : Loi Organique relative aux Lois de Finances). Les politiques budgétaires dévolues aux laboratoires et aux unités de recherche dépendent ainsi souvent de ces indicateurs scientométriques.\n\n\n== Discours sur la science ==\n\n\n=== Épistémologie ===\n\nLe vocable d'« épistémologie » remplace celui de philosophie des sciences au début du XXe siècle. Il s'agit d'un néologisme construit par James Frederick Ferrier, dans son ouvrage Institutes of metaphysics (1854). Le mot est composé sur la racine grecque επιστήμη / épistémê signifiant « science au sens de savoir et de connaissance » et sur le suffixe λόγος / lógos, « le discours ». Ferrier l'oppose au concept antagoniste de l'« agnoiology », ou théorie de l'ignorance. Le philosophe analytique Bertrand Russell l'emploie ensuite, dans son Essai sur les fondements de la géométrie en 1901, sous la définition d'analyse rigoureuse des discours scientifiques, pour examiner les modes de raisonnement qu'ils mettent en œuvre et décrire la structure formelle de leurs théories. En d'autres mots, les « épistémologues » se concentrent sur la démarche de la connaissance, sur les modèles et les théories scientifiques, qu'ils présentent comme autonomes par rapport à la philosophie.\nJean Piaget proposait de définir l’épistémologie « en première approximation comme l’étude de la constitution des connaissances valables », dénomination qui, selon Jean-Louis Le Moigne, permet de poser les trois grandes questions de la discipline :\n\nQu’est ce que la connaissance et quel est son mode d'investigation (c'est la question « gnoséologique ») ?\nComment la connaissance est-elle constituée ou engendrée (c'est la question méthodologique) ?\nComment apprécier sa valeur ou sa validité (question de sa scientificité) ?\n\n\n=== Philosophie des sciences ===\n\nAvant ces investigations, la science était conçue comme un corpus de connaissances et de méthodes, objet d’étude de la Philosophie des sciences, qui étudiait le discours scientifique relativement à des postulats ontologiques ou philosophiques, c'est-à-dire non-autonomes en soi. L'épistémologie permettra la reconnaissance de la science et des sciences comme disciplines autonomes par rapport à la philosophie. Les analyses de la science (l'expression de « métascience » est parfois employée) ont tout d’abord porté sur la science comme corpus de connaissance, et ont longtemps relevé de la philosophie. C'est le cas d'Aristote, de Francis Bacon, de René Descartes, de Gaston Bachelard, du cercle de Vienne, puis de Popper, Quine, Lakatos enfin, parmi les plus importants. L’épistémologie, au contraire, s'appuie sur l'analyse de chaque discipline particulière relevant des épistémologies dites « régionales ». Aurel David explique ainsi que « La science est parvenue à se fermer chez elle. Elle aborde ses nouvelles difficultés par ses propres moyens et ne s'aide en rien des productions les plus élevées et les plus récentes de la pensée métascientifique ».\nPour le prix Nobel de physique Steven Weinberg, auteur de Le Rêve d'une théorie ultime (1997) la philosophie des sciences est inutile car elle n'a jamais aidé la connaissance scientifique à avancer.\n\n\n=== Science au service de l'humanité : le progrès ===\n\nLe terme de progrès vient du latin « progressus » qui signifie l'action d'avancer. Selon cette étymologie le progrès désigne un passage à un degré supérieur, c'est-à-dire à un état meilleur, participant à l'effort économique. La civilisation se fonde ainsi, dans son développement, sur une série de progrès dont le progrès scientifique. La science serait avant tout un moyen de faire le bonheur de l'humanité, en étant le moteur du progrès matériel et moral. Cette identification de la science au progrès est très ancienne et remonte aux fondements philosophiques de la science. Cette thèse est distincte de celle de la science dite pure (en elle-même), et pose le problème de l'autonomie de la science, en particulier dans son rapport au pouvoir politique. Les questions éthiques limitent également cette définition de la science comme un progrès. Certaines découvertes scientifiques ont des applications militaires ou même peuvent être létales en dépit d'un usage premier bénéfique.\n\nSelon les tenants de la science comme moyen d'amélioration de la société, dont Ernest Renan ou Auguste Comte sont parmi les plus représentatifs, le progrès offre :\n\nune explication du fonctionnement du monde : il est donc vu comme un pouvoir explicatif réel et illimité ;\ndes applications technologiques toujours plus utiles permettant de transformer l'environnement afin de rendre la vie plus facile.\nLa thèse de la science pure pose, quant à elle, que la science est avant tout le propre de l'humain, ce qui fait de l'homme un animal différent des autres. Dans une lettre du 2 juillet 1830 adressée à Legendre, le mathématicien Charles Gustave Jacob Jacobi écrit ainsi, à propos du physicien Joseph Fourier : « M. Fourier avait l’opinion que le but principal des mathématiques était l’utilité publique et l’explication des phénomènes naturels ; mais un philosophe comme lui aurait dû savoir que le but unique de la science, c’est l’honneur de l’esprit humain, et que sous ce titre, une question de nombres vaut autant qu’une question du système du monde ». D'autres courants de pensée comme le scientisme envisagent le progrès sous un angle plus utilitariste.\nEnfin des courants plus radicaux posent que la science et la technique permettront de dépasser la condition ontologique et biologique de l'homme. Le transhumanisme ou l'extropisme sont par exemple des courants de pensée stipulant que le but de l'humanité est de dépasser les injustices biologiques (comme les maladies génétiques, grâce au génie génétique) et sociales (par le rationalisme), et que la science est le seul moyen à sa portée. À l'opposé, les courants technophobes refusent l'idée d'une science salvatrice, et pointent au contraire les inégalités sociales et écologiques, entre autres, que la science génère.\n\n\n=== Interrogations de l'épistémologie ===\n\nL'épistémologie pose des questions philosophiques à la Science, et à la « science en train de se faire ». La science progressant de manière fondamentalement discontinue, les renversements de « représentations » des savants, appelées également « paradigmes scientifiques » selon l'expression de Thomas Samuel Kuhn qui, dans son livre La structure des révolutions scientifique (1962),  distingue :\n\nla nature de la production des connaissances scientifiques (par exemple, les types de raisonnements sont-ils fondés ?) ;\nla nature des connaissances en elles-mêmes (l'objectivité est-elle toujours possible, etc.). Ce problème d'épistémologie concerne plus directement la question de savoir comment identifier ou démarquer les théories scientifiques des théories métaphysiques ;\nl'organisation des connaissances scientifiques (notions de théories, de modèles, d'hypothèses, de lois) ;\nl'évolution des connaissances scientifiques (quel mécanisme meut la science et les disciplines scientifiques).\nUne autre formulation en quatre paradigmes, intégrant les apports de l'informatique, est proposée par Tansley & Tolle en 2009, intégrant les progrès de l'informatique , reformulée au début des années 2000 par Jim Gray (chercheur en informatique)  : \n\nScience empirique puis Science expérimentale (antiquité - XVIIIe siècle) : Ce paradigme repose sur une science caractérisée basée sur l'observation directe des phénomènes naturels et sur des expériences simples à partir desquelles les scientifiques recueillaient des données et formulaient des hypothèses.  Depuis la fin du 16ème siècle c'est le principal moteur des avancées scientifiques, pour notamment mieux comprendre le monde naturel ;\nScience théorique  (XIXe - milieu du XXe siècle) : Ce paradigme se concentre sur le développement de modèles et de théories pour expliquer des phénomènes observés ; des concepts abstraits permettent de « formuler » des lois et des principes qui régissent le comportement de la nature ; avec l'essor conjoint des mathématiques, de la physique théorique et de la chimie, de la biologie, la science se tourne vers la construction de modèles et de théories unificatrices abstraites, pour expliquer les phénomènes naturels de l'infiniment petit à l'infiniment grand. Des expériences visent à vérifier ou infirmer ces théories ;\nScience de la simulation et/ou la science informatique (milieu du XXe siècle - années 2000) : Avec l’avènement des ordinateurs et la croissance de la puissance de calcul informatique, ce paradigme décrit une science capable de simuler des phénomènes complexes qui seraient difficiles ou impossibles à étudier expérimentalement. Les simulations informatiques aident à prédire le comportement des systèmes et à tester des théories dans des environnements virtuels. Les simulations numériques, les analyses statistiques et la modélisation complexe sont devenues des outils indispensables dans de nombreux domaines scientifiques\nScience des données, ou eScience pour Jim Gray (années 2000 à aujourd'hui): Ce paradigme émerge avec l'internet et la capacité de collecter et d’analyser de grandes quantités de données (big data, dont une partie en open data). Il met l’accent sur le partage et la réanalyse des données dans différents contextes pour générer de nouvelles hypothèses et découvertes scientifiques. Les plateformes de type data commons  pourraient jouent un rôle crucial dans ce paradigme en facilitant la gestion, l’analyse et le partage des données. Cette dernière phase est marquée par la nécessité de gérer, analyser et, pour partie, partager à grande échelle des flux et stocks de données, éventuellement collaborativement, via des outils logiciels de plus en plus sophistiqués. Malgré plusieurs \"hivers\", l'IAg (IA générative) émerge dans les années 2020.\nNombre de philosophes ou d'épistémologues ont ainsi interrogé la nature de la science et en premier lieu la thèse de son unicité. L'épistémologue Paul Feyerabend, dans Contre la méthode, est l'un des premiers, dans les années soixante-dix, à se révolter contre les idées reçues à l'égard de la science et à relativiser l'idée trop simple de « méthode scientifique ». Il expose une théorie anarchiste de la connaissance plaidant pour la diversité des raisons et des opinions, et explique en effet que « la science est beaucoup plus proche du mythe qu’une philosophie scientifique n’est prête à l’admettre ». Le philosophe Louis Althusser, qui a produit un cours sur cette question dans une perspective marxiste, soutient que « tout scientifique est affecté d’une idéologie ou d’une philosophie scientifique » qu’il appelle « Philosophie Spontanée des Savants » (« P.S.S »). Dominique Pestre s'attache lui à montrer l'inutilité d'une distinction entre « rationalistes » et « relativistes », dans Introduction aux Science Studies.\n\n\n=== Grands modèles épistémologiques ===\n\nL'histoire des sciences et de la philosophie a produit de nombreuses théories quant à la nature et à la portée du phénomène scientifique. Il existe ainsi un ensemble de grands modèles épistémologiques qui prétendent expliquer la spécificité de la science. Le XXe siècle a marqué un tournant radical. Très schématiquement, aux premières réflexions purement philosophiques et souvent normatives sont venus s’ajouter des réflexions plus sociologiques et psychologiques, puis des approches sociologiques et anthropologiques dans les années 1980, puis enfin des approches fondamentalement hétérogènes à partir des années 1990 avec les Science studies. Le discours sera également interrogé par la psychologie avec le courant du constructivisme. Enfin, l'épistémologie s'intéresse à la « science en action » (expression de Bruno Latour), c'est-à-dire à sa mise en œuvre au quotidien et plus seulement à la nature des questions théoriques qu'elle produit.\n\n\n==== Cartésianisme et rationalisme ====\n\n\n==== Empirisme ====\n\n\n==== Positivisme d'Auguste Comte ====\n\n\n==== Critique de l'induction de Mach ====\n\n\n==== Réfutabilité de Karl Popper et les « programmes de recherche scientifique » de Imre Lakatos ====\n\n\n==== « Science normale » de Thomas Kuhn ====\n\n\n==== Constructivisme ====\n\n\n== Science et société ==\n\n\n=== Histoire ===\n\nLe Concile de Nicée de 325 avait instauré dans l'Église l'argument dogmatique selon lequel Dieu avait créé le ciel et la terre en six jours. Cependant, des explications scientifiques furent possibles dès ce credo, qui ne se prononçait pas sur l'engendrement du monde, œuvre du Christ. Cette lacune théologique avait permis une certaine activité scientifique au Moyen Âge, dont, en premier lieu, l'astronomie. Dès le VIIIe siècle, la science arabo-musulmane prospérait et développait la médecine, les mathématiques, l'astronomie, et d'autres sciences. À cette époque, dans l'islam, la science était particulièrement encouragée, le monde étant vu comme un code à déchiffrer pour comprendre les messages divins. Les pays de culture chrétienne en profitèrent largement à partir du XIIe siècle lors d'une période de renouveau appelée Renaissance du XIIe siècle par l'historien Charles H. Haskins.\nAu sein du christianisme, le premier pas en faveur de l'héliocentrisme (qui place la Terre en orbitation autour du Soleil) est fait par le chanoine Nicolas Copernic, avec le De revolutionibus (1543). Le Concile de Trente (1545-1563) encouragea les communautés religieuses à mener des recherches scientifiques. Mais Galilée se heurte à la position de l'Église en faveur du géocentrisme, en vertu d'une interprétation littérale de la Bible, qui recoupait la représentation du monde des savants grecs de l'Antiquité (Ptolémée et Aristote). Le procès de Galilée, en 1633, marque un divorce entre la pensée scientifique et la pensée religieuse, pourtant initiée par l'exécution de Giordano Bruno en 1600. L'opposition des autorités religieuses aux implications des découvertes faites par des scientifiques, telle qu'elle s'est manifestée dans le cas de Galilée, est apparue a posteriori comme une singularité dans l'Histoire. Le procès de Galilée devint le symbole d'une science devenant indépendante de la religion, voire opposée à elle. Cette séparation est consommée au XVIIIe siècle, pendant les Lumières.\nAu XIXe siècle, les scientismes posent que la science seule peut expliquer l'univers et que la religion est l'« opium du peuple » comme dira plus tard Karl Marx qui fonde la vision matérialiste de l'histoire. Les réussites scientifiques et techniques, qui améliorent la civilisation et la qualité de vie, le progrès scientifique en somme, bat en brèche les dogmes religieux, quelle que soit la confession. Les théories modernes de la physique et de la biologie (avec Charles Darwin et l'évolution), les découvertes de la psychologie, pour laquelle le sentiment religieux demeure un phénomène intérieur voire neurologique, supplantent les explications mystiques et spirituelles.\nAu XXe siècle, l'affrontement des partisans de la théorie de l'évolution et des créationnistes, souvent issus des courants religieux radicaux, cristallise le dialogue difficile de la foi et de la raison. Le « procès du singe » (à propos de l'« ascendance » simiesque de l'homme) illustre ainsi un débat permanent au sein de la société civile. Enfin, nombre de philosophes ou d'épistémologues se sont interrogés sur la nature de la relation entre les deux institutions. Le paléontologue Stephen Jay Gould dans « Que Darwin soit ! » parle de deux magistères, chacun restant maître de son territoire mais ne s'empiétant pas, alors que Bertrand Russell mentionne dans son ouvrage Science et Religion les conflits les opposant. Nombre de religieux tentent, comme Pierre Teilhard de Chardin ou Georges Lemaître (père de la théorie du Big bang), d'allier explication scientifique et ontologie religieuse.\nL'encyclique de 1998, Fides et ratio, de Jean-Paul II cherche à réconcilier la religion et la science en proclamant que « la foi et la raison sont comme les deux ailes qui permettent à l'esprit humain de s'élever vers la contemplation de la vérité ».\nLes explications de la science restent limitées aux phénomènes. La question des fins ultimes reste donc ouverte, et comme le remarquait Karl Popper :\n\n« Toutes nos actions ont des fins, des fins ultimes, et la science n’a affaire qu’aux moyens que nous pouvons régulièrement et rationnellement mettre en œuvre pour atteindre certaines fins. »\n\n\n=== Science et pseudo-sciences ===\n\nUne « pseudo-science » (grec ancien pseudês, « faux ») est une démarche prétendument scientifique qui ne respecte pas les canons de la méthode scientifique, dont celui de réfutabilité.\n\nCe terme, de connotation normative, est utilisé dans le but de dénoncer certaines disciplines en les démarquant des démarches au caractère scientifique reconnu. C'est au XIXe siècle (sous l'influence du positivisme d'Auguste Comte, du scientisme et du matérialisme) que fut exclu du domaine de la science tout ce qui n'est pas vérifiable par la méthode expérimentale. Un ensemble de critères explique en quoi une théorie peut être classée comme pseudo-science. Karl Popper relègue ainsi la psychanalyse au rang de pseudo-science, au même titre que, par exemple, l'astrologie, la phrénologie ou la divination. Le critère de Popper est cependant contesté pour certaines disciplines ; pour la psychanalyse, parce que la psychanalyse ne prétend pas être une science exacte. De plus, Popper a été assez ambigu sur le statut de la théorie de l'évolution dans son système.\nLes sceptiques, comme Richard Dawkins, Mario Bunge, Carl Sagan, Richard Feynman ou encore James Randi considèrent toute pseudo-science comme dangereuse. Le mouvement zététique œuvre quant à lui principalement à mettre à l'épreuve ceux qui affirment réaliser des actions scientifiquement inexplicables.\n\n\n=== Science et protoscience ===\nSi le terme normatif « pseudoscience » démarque les vraies sciences des fausses sciences, le terme protoscience (du grec πρῶτος / prỗtos, « premier, initial ») inscrit les champs de recherche dans un continuum temporel : est protoscientifique ce qui pourrait, dans l'avenir, être intégré dans la science, ou ne pas l'être. Le terme anglophone de fringe science désigne un domaine situé en marge de la science, entre la pseudo-science et la protoscience.\n\n\n=== Science ou technique ? ===\n\nLa technique (τέχνη / téchnê, « art, métier, savoir-faire ») « concerne les applications de la science, de la connaissance scientifique ou théorique, dans les réalisations pratiques, les productions industrielles et économiques ». La technique couvre ainsi l'ensemble des procédés de fabrication, de maintenance, de gestion, de recyclage et, même d'élimination des déchets, qui utilisent des méthodes issues de connaissances scientifiques ou simplement des méthodes dictées par la pratique de certains métiers et l'innovation empirique. On peut alors parler d'art, dans son sens premier, ou de « science appliquée ». La science est elle autre chose, une étude plus abstraite. Ainsi l'épistémologie examine entre autres les rapports entre la science et la technique, comme l'articulation entre l'abstrait et le savoir-faire. Néanmoins, historiquement, la technique est première. « L’homme a été Homo faber, avant d’être Homo sapiens », explique le philosophe Bergson. Contrairement à la science, la technique n’a pas pour vocation d’interpréter le monde, elle est là pour le transformer, sa vocation est pratique et non théorique.\nLa technique est souvent considérée comme faisant partie intégrante de l’histoire des idées ou à l'histoire des sciences. Pourtant il faut bien admettre la possibilité d’une technique « a-scientifique », c'est-à-dire évoluant en dehors de tout corpus scientifique et que résume les paroles de Bertrand Gille : « le progrès technique s'est fait par une somme d'échecs que vinrent corriger quelques spectaculaires réussites ». La technique au sens de connaissance intuitive et empirique de la matière et des lois naturelles est ainsi la seule forme de connaissance pratique, et ce jusqu'au XVIIIe siècle, époque où se développeront les théories et avec elles de nouvelles formes de connaissance axiomatisées.\nEn définitive, on oppose généralement le technicien (qui applique une science) avec le théoricien (qui théorise la science).\n\n\n=== Arts et science ===\n\nHervé Fischer parle, dans La société sur le divan, publié en 2007, d'un nouveau courant artistique prenant la science et ses découvertes comme inspiration et utilisant les technologies telles que les biotechnologies, les manipulations génétiques, l'intelligence artificielle, la robotique, qui inspirent de plus en plus d'artistes. Par ailleurs, le thème de la science a été souvent à l'origine de tableaux ou de sculptures. Le mouvement du futurisme par exemple considère que le champ social et culturel doit se rationaliser. Enfin, les découvertes scientifiques aident les experts en Art. La connaissance de la désintégration du carbone 14 par exemple permet de dater les œuvres. Le laser permet de restaurer, sans abîmer les surfaces, les monuments. Le principe de la synthèse additive des couleurs restaure les autochromes. Les techniques d'analyse physico-chimiques permettent d'expliquer la composition des tableaux, voire de découvrir des palimpsestes. La radiographie permet de sonder l'intérieur d'objets ou de pièces sans polluer le milieu. La spectrographie est utilisée enfin pour dater et restaurer les vitraux.\n\n\n=== Vulgarisation scientifique ===\nLa vulgarisation est le fait de rendre accessibles les découvertes ainsi que le monde scientifique à tous et dans un langage adapté.\n\nLa compréhension de la science par le grand public est l’objet d’études à part entière ; les auteurs parlent de « Public Understanding of Science » (expression consacrée en Grande-Bretagne, « science literacy » aux États-Unis) et de « culture scientifique » en France. Il s'agit du principal vecteur de la démocratisation et de la généralisation du savoir selon les sénateurs français Marie-Christine Blandin et Ivan Renard.\nDans nombre de démocraties, la vulgarisation de la science est au cœur de projets mêlant différents acteurs économiques, institutionnels et politiques. En France, l'Éducation nationale a ainsi pour mission de sensibiliser l'élève à la curiosité scientifique, au travers de conférences, de visites régulières ou d'ateliers d'expérimentation. La Cité des sciences et de l'industrie met à disposition de tous des expositions sur les découvertes scientifiques alors que les quelque trente centres de culture scientifique, technique et industrielle ont « pour mission de favoriser les échanges entre la communauté scientifique et le public. Cette mission s'inscrit dans une démarche de partage des savoirs, de citoyenneté active, permettant à chacun d'aborder les nouveaux enjeux liés à l'accroissement des connaissances ».\nLe Futuroscope ou Vulcania ou le Palais de la découverte sont d'autres exemples de mise à disposition de tous des savoirs scientifiques. Les États-Unis possèdent également des institutions telles que l'Exploratorium de San Francisco, qui se veulent plus près d'une expérience accessible par les sens et où les enfants peuvent expérimenter. Le Québec a développé quant à lui le Centre des sciences de Montréal.\nLa vulgarisation se concrétise donc au travers d'institutions, de musées, mais aussi d'animations publiques comme les Nuits des étoiles par exemple, de revues, et de personnalités (Hubert Reeves pour l'astronomie), qu'énumère Bernard Schiele dans Les territoires de la culture scientifique.\n\n\n=== Science et idéologie ===\n\n\n==== Scientisme ou « religion » de la science ====\n\nLa valeur universelle de la science est débattue depuis le début du XXe siècle, tous les systèmes de connaissances n'étant pas forcément assujettis à la science. La croyance en une universalité de la science constitue le scientisme.\nLe scientisme est une idéologie apparue au XVIIIe siècle, selon laquelle la connaissance scientifique permettrait d'échapper à l'ignorance dans tous les domaines et donc, selon la formule d'Ernest Renan dans L'Avenir de la science d'« organiser scientifiquement l'humanité ».\nIl s'agit donc d'une foi dans l'application des principes de la science dans tous les domaines. Nombre de détracteurs y voient une véritable religion de la science, particulièrement en Occident. Sous des acceptions moins techniques, le scientisme peut être associé à l'idée que seules les connaissances scientifiquement établies sont vraies. Il peut aussi renvoyer à un certain excès de confiance en la science qui se transformerait en dogme. Le courant zététique, qui s'inspire du scepticisme philosophique, essaye d'appréhender efficacement la réalité par le biais d'enquêtes et d'expériences s'appuyant sur la méthode scientifique et a pour objectif de contribuer à la formation chez chaque individu d'une capacité d'appropriation critique du savoir humain, est en ce sens une forme de scientisme.\nPour certains épistémologues, le scientisme prend de toutes autres formes. Robert Nadeau, en s’appuyant sur une étude réalisée en 1984, considère que la culture scolaire est constituée de « clichés épistémologiques » qui formeraient une sorte de « mythologie des temps nouveaux » qui ne serait pas sans rapport avec une sorte de scientisme. Ces clichés tiennent soit à l'histoire de la science, résumée et réduite à des découvertes qui jalonnent le développement de la société, soit à des idées comme celles qui met en avant que les lois, et plus généralement les connaissances scientifiques, sont des vérités absolues et dernières, et que les preuves scientifiques sont non moins absolues et définitives alors que, selon les mots de Thomas Samuel Kuhn, elles ne cessent de subir révolutions et renversements.\nEnfin, c'est surtout la sociologie de la connaissance, dans les années 1940 à 1970, qui a mis fin à l'hégémonie du scientisme. Les travaux de Ludwig Wittgenstein, Alexandre Koyré et Thomas Samuel Kuhn surtout ont démontré l'incohérence du positivisme. Les expériences ne constituent pas, en effet, des preuves absolues des théories et les paradigmes sont amenés à disparaître. Pour Paul Feyerabend, ce sont des forces politiques, institutionnelles et même militaires qui ont assuré à la science sa dominance, et qui la maintiennent encore dans cette position.\n\n\n==== Science au service de la guerre ====\n\nPendant la Première Guerre mondiale, les sciences ont été utilisées par l'État afin de développer de nouvelles armes chimiques et de développer des études balistiques. C'est la naissance de l'économie de guerre, qui s'appuie sur des méthodes scientifiques. L'« OST », ou Organisation Scientifique du Travail de Frederick Winslow Taylor est ainsi un effort d'améliorer la productivité industrielle grâce à l'ordonnancement des tâches, permis notamment par le chronométrage. Néanmoins, c'est pendant la Seconde Guerre mondiale que la science est le plus utilisée à des fins militaires. Les armes secrètes de l'Allemagne nazie comme les V2 sont au centre des découvertes de cette époque.\nToutes les disciplines scientifiques sont ainsi dignes d'intérêt pour les gouvernements. Le kidnapping de scientifiques allemands à la fin de la guerre, soit par les Soviétiques, soit par les Américains, fait naître la notion de « guerre des cerveaux », qui culminera avec la course à l'armement de la Guerre froide. Cette période est en effet celle qui a le plus compté sur les découvertes scientifiques, notamment la bombe atomique, puis la bombe à hydrogène. De nombreuses disciplines naissent d'abord dans le domaine militaire, telle la cryptographie informatique ou la bactériologie, pour la guerre biologique. Amy Dahan et Dominique Pestre expliquent ainsi, à propos de cette période de recherches effrénées, qu'il s'agit d'un régime épistémologique particulier. Commentant leur livre, Loïc Petitgirard explique : « Ce nouveau régime de science se caractérise par la multiplication des nouvelles pratiques et des relations toujours plus étroites entre science, État et société ». La conception de ce qu'on nomme alors le complexe militaro-industriel apparaît, en lien très intime avec le politique.\nDès 1945, avec la constatation de la montée des tensions due à l'opposition des blocs capitalistes et communistes, la guerre devient en elle-même l'objet d'une science : la polémologie. Le sociologue français Gaston Bouthoul (1896-1980), dans « le Phénomène guerre », en fonde les principes.\nEnfin, si la science est par définition neutre, elle reste l'affaire d'hommes, sujets aux idéologies dominantes. Ainsi, selon les sociologues relativistes Barry Barnes et David Bloor de l'Université d'Édimbourg, les théories sont d'abord acceptées au sein du pouvoir politique. Une théorie s'imposerait alors non parce qu'elle est vraie mais parce qu'elle est défendue par les plus forts. En d'autres termes, la science serait, sinon une expression élitiste, une opinion majoritaire reconnue comme une vérité scientifique et le fait d'un groupe, ce que démontrent les travaux d'Harry Collins. La sociologie des sciences s'est ainsi beaucoup intéressée, dès les années 1970, à l'influence du contexte macro-social sur l'espace scientifique. Robert King Merton a montré, dans « Éléments de théorie et de méthode sociologique » (1965) les liens étroits entre le développement de la Royal Society de Londres, fondée en 1660, et l'éthique puritaine de ses acteurs. Pour lui, la vision du monde des protestants de l'époque a permis l'accroissement du champ scientifique.\n\n\n=== Science et religion ===\n\nHistoriquement, la science et la religion ont longtemps été apparentées. Dans « Les Formes élémentaires de la vie religieuse » (1912), Émile Durkheim montre que les cadres de pensée scientifique comme la logique ou les notions de temps et d'espace trouvent leur origine dans les pensées religieuses et mythologiques. L’Église Catholique s'intéresse de près à la science et à son évolution comme en témoigne le fait qu'elle ait organisé pour la quatrième fois une conférence internationale au Vatican intitulée « Unite to Cure » en avril 2018. Cette conférence a pour but d'unir différentes opinions dans différentes disciplines scientifiques afin de réfléchir sur le futur de la science et de l'Homme.\n\n\n==== Le non-recouvrement ====\n\nLes conflits entre la science et la religion se produisent dès lors que l'une des deux prétend répondre à la question dévolue à l'autre.\nCette violation peut se produire dans les deux sens. La religion empiète sur la science quand des personnes prétendent déduire des textes religieux des informations sur le fonctionnement du monde. Le conflit de ce type le plus évident est celui du créationnisme face à la théorie de l'évolution. Scientifiquement, la création de l'ensemble des êtres vivants en six jours n'est pas tenable. Mais différents courants religieux radicaux défendent l'exactitude du récit de la Genèse (depuis, l'Église catholique, par exemple, a résolu la contradiction apparente en déclarant que ce récit est métaphorique, ce qui assure de ne pas empiéter sur le domaine scientifique).\nL'autre cas de violation est celui où on extrapole à partir de données scientifiques une vision du monde tout à fait irréfutable (au sens de Popper), empiétant sur le domaine du religieux. Dans le cadre du non-recouvrement, les propositions scientifiques doivent rester compatibles avec toutes les positions religieuses qui cherchent à donner du sens à l'univers (sauf celles qui violent elles-mêmes la démarcation). Albert Einstein et Paul Dirac utilisent le concept de Dieu en commentant la physique quantique, mais les résultats qu'ils établissent ne dépendent pas de son existence.\nLe pape François, dans l'encyclique Laudato si' « sur la sauvegarde de la maison commune » (2015), estime cependant que « la science et la religion, qui proposent des approches différentes de la réalité, peuvent entrer dans un dialogue intense et fécond pour toutes deux ».\n\n\n== Communauté scientifique internationale ==\n\n\n=== Du savant au chercheur ===\nSi la science est avant tout une affaire de méthode, elle dépend aussi beaucoup du statut de ceux qui la font. L'ancêtre du chercheur reste, dans l'Antiquité, le scribe. Le terme de « savant » n'apparaît qu'au XVIIe siècle ; se distinguant du clerc et de l'humaniste. Au XIXe siècle cette figure s'estompe et laisse place à celle du « scientifique universitaire » et du « chercheur spécialisé » aux côtés desquels évoluent le « chercheur industriel » et le « chercheur fonctionnaire ». Aujourd'hui c'est la figure du « chercheur entrepreneur » qui domine selon les auteurs Yves Gingras, Peter Keating et Camille Limoges, dans l'ouvrage Du scribe au savant. Les porteurs du savoir, de l'Antiquité à la révolution industrielle. C'est la création d'institutions comme le Jardin royal des plantes médicinales ou l'Académie royale des sciences de Paris qui marquent l'avènement du statut de chercheur spécialisé au XIXe siècle. Elles fournissent en effet des revenus et un cadre de recherche exceptionnels. C'est en Allemagne, avec Wilhelm von Humboldt, en 1809, que la recherche est affiliée aux Universités. Dès lors commence l'industrialisation de la production de chercheurs, qui accéléra la spécialisation du savoir. Depuis la Seconde Guerre mondiale, ce sont les instituts de recherche et les organismes gouvernementaux qui dominent, à travers la figure du chercheur fonctionnaire.\nLes sociologues et anthropologues Bruno Latour, Steve Woolgar, Karin Knorr-Cetina ou encore Michael Lynch ont étudié l'espace scientifique, les laboratoires et les chercheurs. Latour s'est en particulier intéressé à la production du discours scientifique, qui semble suivre un processus de stabilisations progressives, ce qui permet aux énoncés d'acquérir de la crédibilité au fur et à mesure alors que Jean-François Sabouret et Paul Caro, dans « Chercher. Jours après jours, les aventuriers du savoir » présentent des portraits de chercheurs venant de tous les domaines et travaillant au quotidien,.\n\n\n=== Des communautés scientifiques ===\nLa communauté scientifique désigne, dans un sens assez large, l'ensemble des chercheurs et autres personnalités dont les travaux ont pour objet les sciences et la recherche scientifique, selon des méthodes scientifiques. Parfois cette expression se réduit à un domaine scientifique particulier : la communauté des astrophysiciens pour l'astrophysique, par exemple. La sociologie des sciences s'intéresse à cette communauté, à la façon dont elle fonctionne et s'inscrit dans la société.\n\nOn peut parler de « société savante » lorsqu'il s'agit d'une association d’érudits et de savants. Elle leur permet de se rencontrer, de partager, confronter et exposer le résultat de leurs recherches, de se confronter avec leurs pairs d'autres sociétés du même type ou du monde universitaire, spécialistes du même domaine, et le cas échéant, de diffuser leurs travaux via une revue, des conférences, séminaires, colloques, expositions et autres réunions scientifiques. Un congrès ou conférence scientifique est un événement qui vise à rassembler des chercheurs et ingénieurs d'un domaine pour faire état de leurs avancées. Cela permet également à des collègues géographiquement éloignés de nouer et d'entretenir des contacts. Les congrès se répètent généralement avec une périodicité fixée, le plus souvent annuelle.\nLa collaboration est de mise au sein de la communauté scientifique, en dépit de guerres internes et transnationales. Ainsi, l'outil d'évaluation par les pairs (aussi appelée « arbitrage » dans certains domaines universitaires) consiste à soumettre l’ouvrage ou les idées d’un auteur à l’analyse de confrères experts en la matière, permettant par là aux chercheurs d’accéder au niveau requis par leur discipline en partageant leur travail avec une personne bénéficiant d’une maîtrise dans le domaine.\n\n\n=== Recherche ===\n\nLa recherche scientifique désigne en premier lieu l’ensemble des actions entreprises en vue de produire et de développer les connaissances scientifiques. Par extension métonymique, la recherche scientifique désigne également le cadre social, économique, institutionnel et juridique de ces actions. Dans la majorité des pays finançant la recherche, elle est une institution à part entière, voire une instance ministérielle (comme en France, où elle fait partie du Ministère de l'Éducation Nationale et de la Recherche) car elle constitue un avantage géopolitique et social important pour un pays. Le prix Nobel (il en existe un pour chaque discipline scientifique promue) récompense ainsi la personnalité scientifique qui a le plus contribué, par ses recherches et celles de son équipe, au développement des connaissances.\nLes Science studies sont un courant récent regroupant des études interdisciplinaires des sciences, au croisement de la sociologie, de l’anthropologie, de la philosophie ou de l’économie. Cette discipline s'occupe principalement de la science comme institution, orientant le débat vers une « épistémologie sociale ».\n\n\n=== Sociologie du champ scientifique ===\n\nLa sociologie des sciences vise à comprendre les logiques d'ordre sociologique à l'œuvre dans la production des connaissances scientifiques. Néanmoins, il s'agit d'une discipline encore récente et évoluant au sein de multiples positions épistémologiques ; Olivier Martin dit qu'« elle est loin de disposer d'un paradigme unique : c'est d'ailleurs une des raisons de sa vivacité ». Dans les années 1960 et 1970, une grande part de ces études s’inscrivait dans le courant structuraliste. Mais, depuis le début des années 1980, les sciences sociales cherchent à dépasser l’étude de l’institution « science » pour aborder l’analyse du contenu scientifique. La sociologie du « champ scientifique », concept créé par Pierre Bourdieu, porte ainsi une attention particulière aux institutions scientifiques, au travail concret des chercheurs, à la structuration des communautés scientifiques, aux normes et règles guidant l'activité scientifique surtout. Il ne faut cependant pas la confondre avec l'étude des relations entre science et société, quand bien même ces relations peuvent être un objet d'étude des sociologues des sciences. Elle est en effet plus proche de l'épistémologie.\nLe « père » de la sociologie des sciences est Robert K. Merton qui, le premier, vers 1940, considère la science comme une « structure sociale normée » formant un ensemble qu'il appelle l'« èthos de la science » (les principes moraux dirigeant le savant) et dont les règles sont censées guider les pratiques des individus et assurer à la communauté son autonomie (Merton la dit égalitaire, libérale et démocratique). Dans un article de 1942, intitulé The Normative Structure of Science, il cite quatre normes régissant la sociologie de la science : l'universalisme, le communalisme, le désintéressement, le scepticisme organisé. Ce que cherche Merton, c'est analyser les conditions de production de discours scientifiques, alors que d'autres sociologues, après lui, vont viser à expliquer sociologiquement le contenu de la science. Pierre Duhem s'attacha lui à analyser le champ scientifique du point de vue constructiviste. À la suite des travaux de Thomas Samuel Kuhn, les sociologues dénoncèrent la distinction portant sur la méthode mise en œuvre et firent porter leurs investigations sur le processus de production des connaissances lui-même.\nSi la philosophie des sciences se fonde en grande partie sur le discours et la démonstration scientifique d'une part, sur son historicité d'autre part, pour Ian Hacking, elle doit étudier aussi le style du laboratoire. Dans « Concevoir et expérimenter », il estime que la philosophie des sciences, loin de se cantonner aux théories qui représentent le monde, doit aussi analyser les pratiques scientifiques qui le transforment. Le sociologue américain Joseph Ben David a ainsi étudié la sociologie de la connaissance (« sociology of scientific knowledge ») dans ses « Éléments d'une sociologie historique des sciences » (1997).\n\n\n=== Applications, inventions, innovations et économie de la science ===\n\nL’« application » d’une science à une autre est l'usage qu’on fait des principes ou des procédés d’une science pour étendre et perfectionner une autre science. L'« invention » est d'abord une méthode, une technique, un moyen nouveau par lequel il est possible de résoudre un problème pratique donné. Le concept est très proche de celui d'une innovation. Par exemple, Alastair Pilkington a inventé le procédé de fabrication du verre plat sur bain d'étain dont on dit qu'il s'agit d'une innovation technologique majeure.\nUne « innovation » se distingue d'une invention ou d'une découverte dans la mesure où elle s'inscrit dans une perspective applicative. L'une et l'autre posent des enjeux majeurs à l'économie. Dans les pays développés, les guerres économiques reposent sur la capacité à prévoir, gérer, susciter et conserver les applications et les innovations, par le brevet notamment. Pour les économistes classiques, l'innovation est réputée être l'un des moyens d'acquérir un avantage compétitif en répondant aux besoins du marché et à la stratégie d'entreprise. Innover, c'est par exemple être plus efficient, et/ou créer de nouveaux produits ou service, ou de nouveaux moyens d'y accéder.\nCe sont tout d'abord les sociologues de la science Norman Storer et Warren Hagstrom, aux États-Unis, puis Gérard Lemaine et Benjamin Matalon en France, qui proposent une grille de lecture pour le champ économique des disciplines scientifiques. Ils envisagent en effet la science comme un système d'échange semblable à un marché sauf que la nature des biens échangés est du domaine du savoir et de la connaissance. Il y existe même une sorte de loi de la concurrence car si le scientifique ne publie pas, il ne peut prétendre voir ses fonds de recherche être reconduits l'année suivante. Cet esprit de compétition, selon Olivier Martin « stimule les chercheurs et constitue le moteur de la science ». Mais c'est surtout le sociologue Pierre Bourdieu qui a su analyser l'économie du champ scientifique. Dans son article intitulé « Le Champ scientifique », dans les Actes de la recherche en sciences sociales, il indique que la science obéit aux lois du marché économique sauf que le capital est dit « symbolique » (ce sont les titres, les diplômes, les postes ou les subventions par exemple). Par ailleurs, ce capital symbolique dépend de l'intérêt général et institutionnel : ainsi toutes les recherches se valent mais les plus en vue sont favorisées. Enfin, le milieu scientifique est dominé par des relations de pouvoir, politique et communautaire.\n\n\n== Notes et références ==\n\n\n=== Notes ===\n\n\n=== Références ===\n\n\n== Voir aussi ==\n\n\n=== Bibliographie ===\n \n\n : document utilisé comme source pour la rédaction de cet article.\n\nOuvrages utilisés\nJean-Pierre Verdet, Une Histoire de l'astronomie, Paris, Éditions du Seuil, coll. « Points », 1990, 380 p. (ISBN 2-02-011557-3)\nBernard Vidal, Histoire de la chimie, Paris, PUF, coll. « Que sais-je ? no 35 », 1985, 126 p. (ISBN 2-13-048353-4)\nSerge Hutin, L'alchimie, Paris, PUF, coll. « Que sais-je ? », 2005, 125 p. (ISBN 2-13-054917-9)\nMichel Dubois, Introduction à la sociologie des sciences, PUF, coll. « Premier Cycle », 1999, 329 p. (ISBN 978-2-13-048425-7)\nAurel David (préf. Louis Couffignal), La cybernétique et l'humain, Éditions Gallimard, coll. « Idées », 1965 (ISBN 978-2-07-035067-4)\nLouis Althusser, Philosophie et philosophie spontanée des savants, François Maspero, 1967, 160 p. (ASIN B000WI7YZ4)\nPaul Feyerabend, Contre la méthode, esquisse d’une théorie anarchiste de la connaissance, Éditions du Seuil, coll. « Points Sciences », 1988, 349 p. (ISBN 978-2-02-009995-0)\nDominique Pestre, Introduction aux Science Studies, La Découverte, coll. « Repère », 2006, 122 p. (ISBN 978-2-7071-4596-3)\nAmy Dahan et Dominique Pestre, Les sciences pour la guerre. 1940-1960, Paris, Éditions de l'École des hautes études en sciences sociales, coll. « Civilisations Et Sociétés, numéro 120 », 2004, 402 p. (ISBN 2-7132-2015-7)\nRenald Legendre, Dictionnaire actuel de l'éducation, Canada, Guérin, coll. « Le défi éducatif », 2006, 1584 p. (ISBN 978-2-7601-6851-0)\nJean-François Dortier, Une histoire des sciences humaines, Auxerre, Sciences Humaines Eds, 2006, 400 p. (ISBN 2-912601-36-3)\nEvelyne Barbin (dir.), Arts et sciences à la Renaissance, Paris, Éditions Ellipses, 2007, 318 p. (ISBN 978-2-7298-3676-4)\nBruno Jarrosson, Invitation à la philosophie des sciences, Paris, Éditions du Seuil, coll. « Points Sciences », 1992, 240 p. (ISBN 978-2-02-013315-9 et 2-02-013315-6)\nPlaton, Théétète, Paris, GF-Flammarion, 1995 (réimpr. 2e éd. corrigée)\nRobert Nadeau, Vocabulaire technique et analytique de l'épistémologie, PUF, coll. « Premier cycle », 1999, 904 p. (ISBN 978-2-13-049109-5)\nRaymond Chevallier, Sciences et techniques à Rome, Paris, PUF, coll. « Que sais-je ? », 1993, 128 p. (ISBN 2-13-045538-7)\nDominique Lecourt, Dictionnaire d'histoire et de philosophie des sciences, Paris, PUF, coll. « Quadrige », 1999, 1031 p. (ISBN 2-13-052866-X)\nLéna Soler, Introduction à l’épistémologie, Paris, Éditions Ellipses, 2000, 335 p. (ISBN 978-2-7298-4260-4)\nNoëlla Baraquin et Jacqueline Laffitte, Dictionnaire des philosophes, Paris, Armand Colin, 2008, 404 p. (ISBN 978-2-200-34647-8)\nMaurice Gagnon et Daniel Hébert, En quête de science. Introduction à l'épistémologie, Canada, Fides, 2000, 305 p. (ISBN 2-7621-2143-4, lire en ligne)\nDominique Lecourt, La philosophie des sciences, Paris, PUF, coll. « Que sais-je? », 2001, 127 p. (ISBN 2-13-052072-3)\nAhmed Djebbar, L'âge d'or des sciences arabes, Paris, Éditions du Seuil, coll. « Points sciences », 2001, 187 p. (ISBN 2-7465-0258-5)\nMichel Blay, Dictionnaire des concepts philosophiques, Paris, Larousse, coll. « CNRS éditions », 2005, 880 p. (ISBN 2-03-582657-8)\nGaston Bachelard, La Formation de l'esprit scientifique, Paris, Vrin, coll. « Biblio textes philosophiques », 1993, 256 p. (ISBN 2-7116-1150-7, lire en ligne)\nMichel Serres, Éléments d'histoire des sciences, Paris, Éditions Bordas, coll. « Référents », 2003, 890 p. (ISBN 2-04-729833-4)\nAndré Pichot, La Naissance de la science. Tome 1 : Mésopotamie, Égypte, Paris, Éditions Gallimard, coll. « Folio Essais », 1991, 474 p. (ISBN 2-07-032603-9)\nThomas Samuel Kuhn (trad. de l'anglais), La Structure des révolutions scientifiques, Paris, Flammarion, coll. « Champs », 1993, 284 p. (ISBN 2-08-081115-0)\nGilles-Gaston Granger, La science et les sciences, Paris, PUF, coll. « Que sais-je? », 1993 (ISBN 2-13-045077-6)\nGeoffrey Ernest Richard Lloyd (en), Une histoire de la science grecque, La Découverte, coll. « Points Science », 1990\nHervé Barreau, L'épistémologie, Paris, PUF, coll. « Que sais je? », 2008, 127 p. (ISBN 978-2-13-056648-9)\nRené Taton (dir.), Histoire générale des Sciences (t. 1 : La Science antique et médiévale ; t. II : La Science moderne), PUF, 1957\nAutres ouvrages\nLuc Brisson (dir.) (trad. du grec ancien), Timée : Platon, Œuvres complètes, Paris, Éditions Flammarion, 2008 (1re éd. 2006), 2204 p. (ISBN 978-2-08-121810-9) \nLuc Brisson (dir.) (trad. du grec ancien), Platon : Définitions, Paris, Éditions Gallimard, 2008 (1re éd. 2006), 2204 p. (ISBN 978-2-08-121810-9)\nGeorges Leroux (dir.) et Luc Brisson, La République, Paris, Éditions Gallimard, 2008 (1re éd. 2006), 2204 p. (ISBN 978-2-08-121810-9)\nLuc Brisson (dir.) et Monique Canto-Sperber (trad. du grec ancien par Monique Canto-Sperber), Ménon : Œuvres complètes, Paris, Éditions Flammarion, 2008 (1re éd. 2006), 2204 p. (ISBN 978-2-08-121810-9) \nAlan Chalmers, Qu'est-ce que la science ? Popper, Kuhn, Lakatos, Feyerabend, Le Livre de Poche, 1990, 286 p. (ISBN 978-2-253-05506-8)\nBruno Latour, La science en action : Introduction à la sociologie des sciences, La Découverte, 2005, 664 p. (ISBN 978-2-7071-4546-8)\nAhmed Moulaye et Salah Ould, De Thalès à Einstein, l'histoire de la science à travers ses grands hommes, Levallois-Perret, Studyrama, 2007, 159 p. (ISBN 978-2-7590-0251-1)\nJean-François Dortier (dir.), Une histoire des sciences humaines, Auxerre, Sciences Humaines éditions, 2006, 385 p. (ISBN 2-912601-36-3)\nJean-Marie Nicolle, Histoire des méthodes scientifiques : Du théorème de Thalès au clonage, Rosny, Bréal, 2006, 156 p. (ISBN 2-7495-0649-2)\nJean-Paul Charrier, Scientisme et Occident : Essais d'épistémologie critique, Paris, Connaissances et Savoirs, 2005, 400 p. (ISBN 2-7539-0061-2)\nMaurice Gagnon et Daniel Hébert, En quête de science : Introduction à l'épistémologie, Fides, 2000, 309 p. (ISBN 2-7621-2143-4, lire en ligne)\nJean-Pierre Mohen, L'art et la science : L'esprit des chefs-d'œuvre, Paris, Éditions Gallimard, coll. « Sciences », 1998, 160 p. (ISBN 2-07-059413-0)\nLe savant et le politique aujourd'hui : colloque de La Villette  (préf. François d'Aubert), Paris, Éditions Albin Michel, coll. « Bibliothèque des Idées », 7 juin 1996, 209 p. (ISBN 978-2-226-08825-3 et 2-226-08825-3)\nPatrice Flichy, L'innovation technique : récents développements en sciences sociales : vers une nouvelle théorie de l'innovation, Paris, La Découverte, coll. « Sciences et société », 2003, 250 p. (ISBN 2-7071-4000-7)\nNicholas Rescher (trad. de l'anglais), Le progrès scientifique : un essai philosophique sur l'économie de la recherche dans les sciences de la nature, Paris, PUF, coll. « Sciences, modernités, philosophie », 1993, 342 p. (ISBN 2-13-045764-9)\nRobert Oppenheimer, La science et le bon sens, Paris, Éditions Gallimard, coll. « NRF Idées », 1972, 200 p. (ISBN 2-7605-1243-6)\nPaul Feyerabend (trad. de l'anglais), Écrits philosophiques, volume 1 : Réalisme, rationalisme et méthode scientifique, Chennevières-sur-Marne/Paris, Dianoia, coll. « Fondements de la philosophie contemporaine des sciences », 2005, 447 p. (ISBN 2-913126-05-7)\nPatrick Tort, Dictionnaire du darwinisme et de l’évolution (dir.), Paris, PUF, 1996, 3 vol., 5000 p. Ouvrage couronné par l’Académie des sciences.\nScience et méthode d'Henri Poincaré (1908)\nLa Science et l'hypothèse d'Henri Poincaré (1902)\nLes Valeurs de la science d'Henri Poincaré (1902)\nDiscours de la méthode pour bien conduire sa raison et chercher la vérité dans les sciences (1637) de René Descartes\n\n\n=== Articles connexes ===\n\nGénéralités\n\nActualité en France\n\n\n=== Liens externes ===\n\nRessources relatives à la santé : Medical Subject Headings NCI Thesaurus \nRessource relative à la recherche : Stanford Encyclopedia of Philosophy \nRessource relative à l'audiovisuel : France 24 \n\n Portail de la philosophie   Portail de l’histoire des sciences   Portail des sciences"
        },
        {
            "pageid": 15894837,
            "ns": 2,
            "title": "Utilisateur:As9400htparis/Brouillon/SaskiaWalentowitz",
            "content": ""
        },
        {
            "pageid": 16034295,
            "ns": 0,
            "title": "Cell Worlds",
            "content": "Cell Worlds (« Mondes Cellulaires ») est un projet de médiation scientifique sur la thématique de la microscopie et de la biologie cellulaire. Il a été créé en 2022 par le collectif Exaltia (anciennement Les Explorers), composé de Renaud Pourpre, Terence Saulnier et Youenn Lerb. Il se décline sous divers formats tels que le documentaire, l'art immersif, ou encore le ciné-concert. Youenn Lerb est le compositeur de la bande originale de toutes les déclinaisons du projet.\n\n\n== Actions emblématiques ==\n\n\n=== Expositions ===\nDu 11 février 2022 au 2 janvier 2023, la première exposition immersive, nommée Cell Immersion,, a été présentée aux Bassins des Lumières de Culturespaces, dans une partie de la base sous-marine de Bordeaux. La fréquentation a dépassée les 600.000 personnes. L'exposition met en œuvre des images de plus de 25 équipes de recherche du monde entier, provenant d'instituts renommés tels que le Janelia Research Campus, le PariSanté Campus ou encore L'École polytechnique fédérale de Lausanne. L'exposition a été inaugurée le 14 mars 2022 en présence du CNRS, inscrivant ainsi cette première au sein de L'Année de la Biologie. L'exposition a été nommée dans la catégorie Immersive Science du Silbersalz Festival. Un clip musical a été filmé au sein de l'exposition.\nDu 25 au 29 octobre 2023, une exposition immersive éphémère a été présentée au Silbersalz Festival avec des images de L'Université de Poitiers ou encore du Max Planck Institute of Microstructure Physics.\n\n\n=== Documentaires ===\nLe 17 mars 2022, le premier documentaire, nommé Cell Worlds, Mondes Cellulaires, est publié sur YouTube. Il présente les mêmes images que l'exposition Cell Immersion, et utilise les codes des documentaires animaliers, grâce à une narration pédagogique et poétique à 4 voix (Lisa Bretzner, Mathilda Delecroix, Renaud Pourpre et Terence Saulnier). Ce premier volet a été lauréat de la commission Savoirs & Cultures du CNC et a été nommé au Festival Pariscience 2022 et au Festival Imagine Science 2022.\nLe 8 octobre 2023, le second documentaire, nommé Cell Worlds, Anomalies Cellulaires, est publié sur YouTube. Il est intégralement réalisé avec des images des scientifiques de L'Université de Poitiers. Les voix-off de ce second volet sont celles de Alice Thomas et Vincent Chabrette.\n\n\n=== Cinéma-expériences ===\nLe 29 octobre 2022, le premier cinéma-expérience Cell Worlds a été présenté au festival des Utopiales 2022, en partenariat avec l'Inserm. C'est un format de vulgarisation scientifique s'inspirant des codes du ciné-concert.\nUne tournée de cinéma-expérience s'est tenue en 2023, à travers les portes-ouvertes de l'EPFL, le festival Inscience, ou encore le festival Curiositas. Le cinéma-expérience de la fête de la science 2023, qui s'est tenu à Poitiers, a été le premier où le public s'est mis à danser au rythme de la musique, donnant ainsi le slogan de Cell Worlds : Célébrer le vivant.\n\n\n=== Scénographie ===\nLes 9 et 10 mars 2023, Cell Worlds et le Sensory Odyssey se sont associés pour penser la scénographie immersive du Hello Tomorrow Global Summit 2023.\n\n\n== Valeurs et objectifs ==\n\n\n=== Pour le grand public ===\nCell Worlds utilise les codes des expéditions scientifiques, en se présentant comme une mission d'exploration à la découverte de biodiversités cachées. La genèse du projet est de sortir les images de la recherche en biologie au plus loin des laboratoires, au plus proche du grand public. Cell Worlds utilise les émotions, à travers la musique, les couleurs ou encore une narration introspective, pour sensibiliser par l'émerveillement.\nL'utilisation des émotions et de la beauté a été démontré scientifiquement comme étant un excellent outil pour partager la science,. C'est l'opportunité pour le public de découvrir des thématiques complexes et parfois pesantes telles que les cancers, à travers un regard différent sur ces mondes inconnus, et pourtant logés en chacun et chacune d'entre nous.\n\n\n=== Pour les scientifiques ===\nCell Worlds s'adresse également aux scientifiques, en démystifiant les métiers de la recherche scientifique, et en valorisant les travaux de la recherche, dont les résultats négatifs non publiés. Le projet est ainsi une première mondiale scientifique, pédagogique et artistique, pour mettre en scène le domaine de la microscopie dans des proportions jamais tentées. Le projet est ainsi devenu un véritable showcase de la recherche scientifique sur ces thématiques, réunissant au total plus de 50 équipes de recherche à travers le monde entier.\n\n\n== Distinctions ==\n2022 : Lauréat de la commission CNC Savoirs & Cultures.\n2022 : Sélection Inserm au Festival des Utopiales.\n2022 : Nomination au Festival Imagine Science.\n2022 : Nomination au Festival Pariscience dans la catégorie court-métrages.\n2023 : Nomination au Festival Silbersalz dans la catégorie Immersive Science.\n\n\n== Discographie ==\n2022 : Cell Worlds - Mondes Cellulaires (Apoptose Records)\n2022 : Live at Bassins Des Lumières (Apoptose Records)\n2023 : Cell Worlds - Anomalies Cellulaire (Apoptose Records)\n\n\n== Notes et références ==\n\n Portail des sciences   Portail des neurosciences   Portail de la musique électronique"
        },
        {
            "pageid": 16479173,
            "ns": 2,
            "title": "Utilisatrice:Cognition24/Brouillon",
            "content": "Human Auditory Ecology (HAE) is a research program in hearing sciences aiming to study the interactions between human beings and their acoustic environments. This program, initially envisaged for urban environments, was later extended to natural environments.\n\n\n== History of the concept ==\nHAE studies the “relationship between the acoustic environments in which people live and their auditory needs in these environments” . This auditory ecology, a concept initially coined by Stuart Gatehouse, therefore refers to the auditory environments in which humans live and function, the tasks to be undertaken by humans in these complex acoustic environments and the importance of these tasks in daily life and daily routines. The use of this concept was initially restricted to the case of urban life. However, urban habitats are relatively recent in humankind history and evolution, and natural soundscapes have preceded the apparition of Homo sapiens, some 300,000 years ago. For this reason, the concept of auditory ecology was extended to study a different and evolutionary-based question, namely how humans perceive ecological processes at work in natural habitats through their peripheral and central auditory system.\n\n\n== Auditory perception of natural soundscapes ==\nNatural soundscapes correspond to the complex arrangements of biological (animal vocalizations) and geophysical (wind, rain, stream) sounds shaped by sound propagation through non-anthropogenic habitats    . According to its most recent definition, human auditory ecology (HAE) is a multidisciplinary research programme attempting to map and explain the ability of normal-hearing and hearing-impaired human listeners to perceive natural soundscapes   . In this specific case, HAE aims at characterizing how and to which extent humans perceive ecological processes underlying habitats marginally affected by human activity through their ears and their auditory brain, with a life-span perspective   . HAE could be considered as a field of auditory psychophysics, auditory neurosciences and audiology. More broadly, HAE aims to encourage hearing scientists who traditionally work on speech and music perception in urban settings to collaborate with soundscape ecologists, ecoacousticians and neuro-ethologists, and share expertise with environmental and architectural acousticians, anthropologists, philosophers and geographers   .\n\n\n== Extended definition of human auditory ecology ==\nHAE studies the (presumably ancestral) monitoring functions of the human auditory system. These monitory auditory functions are used by human auditory system to build a perceptual representation of the close environment, orient and navigate, assess resources (food, water, shelter) and danger (e.g., flooding, predators), opportunities for action, and the general health of the environment. These monitoring functions are assumed to help human listeners build a sense of place and time and operate in their accoustic environment   . \nFrom such a perspective, HAE is based on (i) concepts derived from soundscape ecology such as the acoutic adaptation and acoustic niche hypothesis    , and (ii) psychophysical and neuroscientific models and methods. HAE operates on the large acoustic databases of natural soundscapes collected by soundscape ecologists and eco-acousticians using standardized procedures and recording material   . HAE aims to characterize the monitoring functions of the human auditory system through ordinary listening behaviors (listening to animal vocalisations such as bird songs or insect stridulations, detecting presence of water or rain, assessing water discharge or wind strength …). Moreover, HAE investigates the extent to which these monitoring functions are adapted to specific information conveyed by natural soundscapes, whether they operate throughout the life span or whether they emerge through individual learning or cultural transmission .\nHAE aims to identify testable working hypotheses guided by computational models of the human auditory system, in order to (i) unveil low and high-level auditory mechanisms engaged in the auditory perception of soundscapes associated with natural habitats, green or blue species within or outside cities  , (ii) how they develop through life and (iii) the extent to which they are affected by exposure, learning and culture . Although HAE aims to improve fundamental knowledge on the human auditory system and how humans interact with natural environments, it also aims at providing novel solutions to screen and rehabilitate hearing loss via hearing aids and cochlear implants .\n\n\n== Acoustical aspects ==\n\n\n=== Contribution of soundscape ecology and ecoacoustics ===\nBecause most vertebrate species send and receive sound for essential life functions (e.g., navigation, courtship, foraging) , the collection of sounds perceived in an environment (i.e., soundscapes) can reflect ecological processes. Thus, by studying the structure of the soundscape over space and time, soundscape ecology and ecoacoustics aim to study the dynamics of ecological processes . Soundscape ecology is predicated on two key hypotheses: the acoustic niche hypothesis, where signals have evolved to partition in acoustic space to minimize overlap between speciesand the acoustic adaptation hypothesis, which states that species’ optimize transmission of vocalizations to overcome habitat constraints. Although, the acoustic adaptation hypothesis has received limited support in experimental studies . Recent developments in passive sound collection offer the opportunity to cost-effectively monitor soundscapes at enormous scales. For example, the Australian Acoustic Observatory comprises 360 permanent recording stations and the National Park Service’s Natural Sounds and Night Skies division have collected recordings at over 490 sites across the United States. Extracting relevant biological information from resulting enormous datasets remains challenging.  Species vocalizations of interest may be manually or automatically extracted, using listening, visualizations of spectrograms, or recognition algorithms. Alternatively, acoustic indices can be used to summarize the properties of the soundscape  .\n\n\n=== Contribution of computational auditory sciences ===\nThe pioneering work of Singh and Theunissen suggests that high spectral modulations (harmonicity) and slow amplitude modulations distinguish biological (animal vocalizations) from geophysical sounds (wind, rain, stream sounds) in natural scenes. Subsequent work by McDermott and Simoncelli showed that geophysical sounds such as wind, rain or stream sounds can be distinguished from other sounds by their textural properties. The latter are reflected in specific regularities or “statistics” of amplitude-modulation patterns computed by the human auditory system in response to these sounds (McDermott and Simoncelli, 2011). \nThe statistics of these sounds and scenes could be characterized further using the large, ecologically-valid databases collected by soundscape ecologists and ecoacousticians, allowing to test further efficient-coding principles positing that perceptual systems (e.g., the auditory system) have evolved to encode environmental stimuli in the most efficient way, and that the properties of auditory mechanisms  closely match the statistical properties of natural sounds and scenes    . The first studies based on this approach indicate that the central auditory system of humans has access to sufficient sensory information in the spectro-temporal domain to achieve accurate auditory discrimination of terrestrial biomes and their changes across moments of the day and seasons . \n\n\n== Psychoacoustical aspects ==\n\n\n=== Historical background ===\nHAE aims to understand human auditory perception of natural (i.e. biophonic and geophonic) acoustic environments.  In contrast, the majority of experimental studies of human auditory perception have utilized parametrically-generated stimuli which lack acoustic complexity of natural environmental sounds. The prevailing methodological paradigms of psychoacoustics have traditionally focused on investigating how sounds’ physical properties relate to the perception of abstract sound qualities such as pitch, loudness and duration without considering sounds’ semantic or referential aspects. Furthermore, traditional psychoacoustic methods have relied primarily on detection and discrimination tasks in which a specific acoustic parameter is manipulated (e.g. frequency or intensity) under conditions of low stimulus uncertainty and performed by trained listeners. Overall, traditional psychoacoustics has been highly successful in describing human auditory abilities in relationship to underlying anatomy and physiology, leading to tremendous breakthroughs and achievements in communication and audio technology. However, this approach has had limited utility for understanding the perception of meaningful acoustically complex environmental sounds in everyday life, which typically involves perception of sound producing objects and events along with associated materials and actions .\n\n\n=== Ecological psychoacoustics ===\nA growing awareness of these limitations to ecological validity in traditional psychoacoustics has led to the broadening of theoretical approaches and modifications of experimental procedures used in the studies of auditory perception to include tasks involving sound identification, categorization and comprehension.  Later efforts explored principles of perceptual organization of complex auditory scenes (under the general framework of auditory scene analysis) and formulated questions in terms of actionable and behaviorally relevant properties of specific sound (under the framework of ecological psychology).  A further attempt to bridge the gap between investigations of auditory perception utilizing traditional psychoacoustic paradigms and those pertaining to everyday listening (ordinary listening behaviors) has been proposed under the general heading of ecological psychoacoustics. Ecological psychoacoustics generally considers auditory perception in terms of the ecologically relevant behavioral goals of the listener in specific tasks, situational contexts and environments (e.g. cognition-perception-action loop), while employing psychoacoustic experimental methods to maintain high internal validity.\nFor example, numerous studies have investigated aspects of listeners’ perception of sound producing objects, materials, and actions solely based on associated environmental sounds. These studies consistently demonstrate remarkably accurate perception of sound sources based on acoustic signals alone. For instance, listeners can accurately judge the size and behavior of objects, such as predicting the timing of successive bounces of different types of balls, dropped from different heights, based on preceding sounds. Additionally, listeners can discern the sex and posture of a walker from the sound of their footsteps, estimate the volume of a container from the sound of liquid being poured into it, or infer the configuration of clapping hands from the sound produced.\n\n\n=== Ecologically-based taxonomies ===\nEfforts have been also made to develop comprehensive ecologically-based taxonomies that would apply to sounds of everyday listening environments   and integrate them into broader acoustic communication frameworks. However, due to the large inherent variability and complexity of everyday environmental sounds, no general all-inclusive taxonomy has been developed. Nevertheless, valuable classifications have emerged, notably distinguishing between the perception of actions/events and objects/materials  .  Experimental approaches so far have also mostly failed to distinguish among different types of listening experience such as active and focused listening versus background listening when listener is not actively seeking one or more specific sounds by monitors environment as a whole.  Furthermore, unlike HAE, previous approaches to the study of auditory perception have made no systematic distinction between natural (biophonic or geophonic) versus mechanically or electronically generated and technophonic sounds.\n\n\n=== Auditory perception of natural soundscapes, biophony and geophony ===\nTwo psychoacoustical studies have explored the ability of human listeners to discriminate natural soundscapes in a categorical way . Consistent with predictions of a modelling study, these behavioral studies reveal that human listeners are able to discriminate soundscapes recorded by soundscape ecologists in a nature reserve with their ears only. More precisely, these studies showed that naive (untrained) listeners hear changes in habitat (forest, meadow, grassland, chaparral), moment of the day (dawn, dusk, etc.) and season (summer, fall, etc.), although this capacity is not optimal. These first studies pave the way for further empirical studies aiming to assess the human ability to perceive natural soundscapes and their variations using the sound databases recorded by soundscape ecologists and ecoacousticians.\n\n\n== Developmental aspects ==\nIf human perceptual systems have evolved to efficiently encode environmental signals, as suggested by the efficient neural coding hypothesis , then these mechanisms may be evolutionarily ancestral, and as such may emerge early in ontogenetic development . Few studies to date have explicitly tested this hypothesis for natural sounds, and in general few studies have looked at how young children perceive environmental sounds other than speech and music. \nOne series of studies   investigated adults and young infants’ perception of water sounds. According to the efficient neural coding hypothesis, perceptual systems need to extract the statistical structure of environmental stimuli in order to achieve an information theoretical optimum, i.e. encoding the greatest amount of information at the lowest cost. One common feature of the statistical structure of many natural stimuli is scale-invariance, the property of exhibiting the same statistical structure at different spatial or temporal scales. The studies conducted with water sounds tested whether sounds produced by a generative model of gamma tone chirps obeying scale-invariance were perceived as instances of natural water sounds, while those having a variable-scale structure were not. Indeed, adults rated a wide range of scale-invariant, but not variable-scale, sounds as natural recordings of brooks and streams, and qualitatively described them as various forms of water. Five-month-old infants also showed categorical discrimination between scale-invariant and variable-scale sounds , and the newborn brain just 1-3 days after birth responded differentially to scale-invariant and variable-scale water sounds in the left inferior frontal and temporal areas . \nThese results suggest that the perception of natural sounds may indeed be efficient early on in human development, and pave the way for further studies of this hypothesis involving a much greater variety of natural sounds and soundscapes.\n\n\n== Emotional aspects and health benefits ==\n\n\n=== Psychological studies ===\nEmotion perception of environmental sounds can be described within the dimensional view of affect, specifically along a combination of several continua . Two dimensions often dominate, specifically valence (unpleasant to pleasant) and arousal (calming to exciting) , Although emotions could be high on either nor neither dimension, sounds that elicit emotions near the valence extremes (e.g., very pleasant or very unpleasant), also tend to be high in the arousal dimension (e.g., exciting or activating). The valence dimension reflects the motivational theory of emotion, where valence of an emotion supports a person’s motivation  . Unpleasant emotions (e.g., in response to a lion) facilitate attention and inspire action  . Pleasant emotions (e.g., in response to birdsong) encourage approach behavior and support well-being, such as through stress recovery  or creative thinking . Thus, both pleasant and unpleasant emotions serve important functions. Adults with permanent, sensorineural hearing loss do not report a full range of emotional responses to non-speech sounds; their range of ratings of valence is less extreme (less pleasant, less unpleasant) compared to their peers with normal hearing, even when they are similarly aged  .  This is true for natural and manmade sounds .\n\n\n=== Contribution of soundscape ecology and ecoacoustics ===\nExposure to nature provides a variety of health benefits . Soundscapes in particular provide crucial information, where sounds enable most species, including humans, to surveil their surroundings  . From an evolutionary perspective , a soundscape that is full of natural sounds, can be an indicator of an environment rich in resources needed for survival. Thus, a natural acoustic environment stimulates the parasympathetic nervous system, allowing mental recuperation and a reduction in stress-related behaviour. Two psychological theories explain the mechanistic basis of the restorative effects of exposure to natural soundscapes: Attention Restoration Theory, the ability of nature to replenish attention  and Stress Recovery theory, where nature is less arousing than fatigue-inducing urban environments, leading to recovery from stress . A synthesis of studies examining the evidence of health benefits of natural soundscapes revealed decreased stress and annoyance and improved health and positive affective outcomes . Examples of beneficial outcomes of listening to natural sounds included decreased pain, lower stress, improved mood, and enhanced cognitive performance . \n\n\n=== Soundscape studies ===\nSoundscape studies play an important role within the framework of HAE, focusing on human sensory and emotional auditory processing  . Defined by ISO 12913 as the acoustic environment perceived by humans within a contextual framework , soundscape studies diverge from traditional environmental noise research by emphasizing positive health and perceptual outcomes , particularly regarding the (measurable) restorative properties of natural sounds  . Contrary to assumptions within HAE, soundscape studies reveal cultural variations in individuals’ and communities’ responses to natural sounds . The ISO 12913 series, comprising theoretical frameworks , data collection  and data analysis methods , serves as a cornerstone in soundscape literature. Advancements in the field include innovative methods for visualizing and analyzing quantitative soundscape data , alongside prediction models that simulate human perceptions of present and future/hypothetical acoustic environments . These models utilize objective metrics as predictors and subjective metrics as descriptors , enabling a deeper understanding of how humans experience acoustic environments within context and driving progress in understanding and optimizing auditory experiences in natural habitats.\n\n\n== Audiological aspects ==\n\n\n=== Effects of aging and hearing loss ===\nOlder adults with normal hearing or with mild hearing losses appear to maintain generally robust source recognition of environmental sounds in quiet   .  However, when tested under more challenging conditions which closer approximate everyday listening environments, older adults with and without hearing loss require greater signal-to-noise ratio (SNR) to identify sounds in scenes  . Middle-aged and older adults with normal hearing and those with some degree of sensorineural hearing loss also perform poorer than younger adults in tasks which involve perception of multiple environmental sounds .  Furthermore, irrespective of age and severity of hearing loss, people with hearing loss exhibit great difficulties discriminating natural soundscapes that vary systematically in terms of place (forest, meadow, grassland, chaparral), moment of the day (dawn, midday, dusk and night) and season (autumn, winter, spring and summer) .\n\n\n=== Effects of conventional hearing aids ===\nThe primary complaint of people with hearing impairment is difficulty to understand speech in noise . As such, substantial knowledge has been acquired about mechanisms behind speech perception (2) and several speech-in-noise testing tools have been developed   . People with hearing impairment can benefit from the use of hearing aids . Research in auditory reality of hearing aid users has shown that people spend approximately 31% of time in situations involving speech communication, whereas they spend about 45% of time in situations where they are monitoring surroundings or passive listening . It has been proposed that HAE should not be limited to the case of urban settings and communication but should be extended to perception of wild or rural soundscapes .  The ability to experience sounds of nature can lead to feeling less pain, lower stress, enhanced mood and improved cognitive performance . People who use hearing aids report positive listening experiences that involve not only communication, but also perception of environmental and nature sounds (e.g., “the forest sounds completely different,” “nice to hear the wind howling,” “can hear traffic and the rain,” “heard rustling leaves and rustling of the trees,” “can hear the sparrow singing.”)  . \n\n\n=== Effects of cochlear implants ===\nFirst time users of sensory aids, hearing aids and cochlear implants often report new or renewed ability to recognize various natural, machine-produced or electronic sounds in their environments   .  Improvement in environmental sound perception is frequently cited by adults with hearing loss as an anticipated benefit of sensory aids     . Nevertheless, when explicitly tested, the ability of cochlear implant users to identify common environmental sounds shows considerable decrement compared to normal hearing peers .  Limited current research, further indicates no significant improvement in environmental sound perception following implantation   .   Given the ubiquity of nonlinguistic environmental sounds as well as their recognized importance for maintaining personal safety, well being and awareness of their surroundings   , environmental sound perception appears to be a fertile area for theoretical and applied auditory perception research . \n\n\n=== Effects of hearing loss and hearing aids on emotional processing ===\nAdults with permanent, sensorineural hearing loss do not experience a full range of emotional responses to non-speech sounds; they report emotional responses that are extreme (less pleasant and less unpleasant) than do their similarly aged peers with normal hearing  . The reason for this reduced range of emotional responses is unclear.  One explanation is audible bandwidth. Hearing loss in adults commonly causes changes in audible bandwidth, especially reduced audibility of high frequencies , such as those about 2000 Hz .  However, low- and high-frequency cues (<800 and >2000 Hz, respectively) are important for emotional responses to non-speech sounds .  Therefore, improving audibility of high frequency cues, such as with a hearing aid or cochlear implant, would be expected to expand the range of emotional responses to sound. However, even while using the current standard-of-care intervention for permanent hearing loss, including hearing aids, adults demonstrate this reduced range of pleasant responses . Therefore, other acoustic cues, such as amplitude modulations, might play an important role in emotional processing of non-speech sounds . Because modern hearing aids have amplitude compression, it is possible that assistive listening devices are reducing the amplitude modulation cues important for emotional responses to sounds  .\n\n\n== Auditory awareness of environmental changes ==\n\n\n=== Contribution of sounsdcape ecology and eco-acoustics ===\nSoundscape ecology and ecoacoustics study ecological questions using the analysis of environmental sound . Because of their unique climate, vegetation and animal communities, ecosystems are associated with environmental sounds showing unique patterns and dynamics . Changing of those soundscapes have been shown to reflect local disturbances within ecosystems. This is exemplified by the invasive ant Wasmannia auropunctata affecting the local fauna and therefore silencing the forest of New Caledonia , the soundscape’s diversity showing a flat response in burned area in comparison to unburned area, 3 years after a massive wildfire in the Chiricahua national monument in Arizona  or the increase in biological sounds in cities during the Covid-19 pandemic. \nEthnographic surveys in human geography highlight the awareness of human beings in the face of these changes . Altogether, these findings warrant detailed psychoacoustical investigations aiming to assess the ability of human listeners to hear changes reflecting local disturbances within ecosystems. \n\n\n== References: =="
        },
        {
            "pageid": 11338405,
            "ns": 0,
            "title": "Contrôle de santé intégré",
            "content": "Le contrôle de santé intégré (CSI) est le processus d'implémentation et de mise en œuvre d'une stratégie de détection et de caractérisation des dommages dans les structures d'ingénierie. Ici, les dommages sont définis comme les changements apportés aux propriétés physiques et / ou géométriques d'un système CSI y compris les changements dans les conditions aux limites et les connectivités du système, ce qui affecte négativement les performances de ce dernier.\nLe processus de CSI comprend l'observation d'un système au cours du temps en utilisant des mesures dynamiques et échantillonnées qui sont faites périodiquement à partir d'un réseau de capteurs, l'extraction des caractéristiques sensibles aux dommages à travers ces mesures et l'analyse statistique de ces caractéristiques afin de déterminer l'état d'intégrité actuel du système. Pour le CSI à long terme, les données de sortie de ce processus sont périodiquement mis à jour et elles apportent des informations concernant la capacité de la structure à remplir sa fonction prévue à la lumière de son vieillissement inévitable et de sa dégradation résultant des environnements opérationnels. Après des événements intenses, tels que les tremblements de terre ou les explosions, le CSI est utilisé pour le dépistage rapide de l'état de santé et vise à fournir, en temps quasi-réel, des informations fiables concernant l'intégrité de la structure.\n\n\n== Introduction ==\nLes méthodes qualitatives et discontinues ont longtemps été utilisées pour évaluer les structures et leur capacité à remplir leurs fonctions prévues. Depuis le début du XIXe siècle, les employés des chemins de fer ont utilisé le son d'un marteau frappant la roue du train pour savoir si des dommages sont présents. Dans les machineries tournantes, la surveillance vibratoires a été utilisée pendant des décennies comme une technique d'évaluation des performances. On trouve deux techniques dans le domaine du CSI d'une part les techniques Raghavan et Cesnik qui sont basées sur la propagation des ondes et d'autre part les techniques basées sur les vibrations,,. De façon générale, la littérature pour le CSI qui se base sur les vibrations peut être divisée en deux aspects : le premier où les modèles sont proposés pour le dommage pour déterminer les caractéristiques dynamiques, aussi connu comme le problème direct, par exemple se référer à cadre unifié (en) et le deuxième, dans lequel les caractéristiques dynamiques sont utilisés pour déterminer les caractéristiques des dommages, également connu sous le nom du problème inverse, par exemple se référer à. Au cours des dix à quinze dernières années, les technologies du CSI ont émergé tout en créant un nouveau domaine passionnant au sein de diverses branches de l'ingénierie. Des conférences académiques et des revues scientifiques qui se concentrent spécifiquement sur le CSI ont été mises en place durant cette période. Ces technologies deviennent actuellement de plus en plus courantes.\n\n\n== Reconnaissance de formes statistique ==\nLe problème de CSI peut être abordé dans le contexte du paradigme de la reconnaissance de formes statistique,. Ce paradigme peut être décomposé en quatre parties : \n\nÉvaluation Opérationnelle,\nAcquisition de Données et Nettoyage,\nExtraction des traits et Compression des Données,\nDéveloppement du Modèle statistique pour la discrimination des traits.\nLorsque l'on tente d'appliquer ce paradigme à des données provenant de structures du monde réel, il devient rapidement évident que la capacité de purifier, compresser, normaliser et fondre les données pour tenir en compte la variabilité opérationnelle et environnementale est un problème d'implémentation clé lorsque les parties 2 à 4 sont abordées de ce paradigme. Ces processus peuvent être implémentés à travers un support matériel ou logiciel et, en général, une combinaison de ces deux approches sera utilisée.\n\n\n== Évaluation de l'intégrité des structures des ponts, des bâtiments et d'autres infrastructures liées ==\nCommunément appelé évaluation de l'intégrité des structures ou CSI, ce concept est largement appliqué à diverses formes d'infrastructures, notamment lorsque des pays partout dans le monde entrent dans une plus grande période de construction de diverses infrastructures allant des ponts jusqu'aux Gratte-ciel. Il est important de noter qu'il existe des étapes de difficulté croissante nécessitant la connaissance des étapes précédentes surtout lorsque les dommages dans les structures sont concernés à savoir :\n\nDétecter l'existence du dommage dans la structure,\nLocaliser le dommage,\nIdentifier les types du dommage,\nQuantification de la gravité du dommage.\nIl est nécessaire d'utiliser le traitement du signal et la classification statistique pour convertir les données sur l'état de santé de l'infrastructure du capteur en une information sur les dommages pour évaluation.\n\n\n=== Évaluation opérationnelle ===\nL'évaluation opérationnelle tente de répondre à quatre questions concernant la mise en œuvre d'une aptitude d'identification des dommages :\n\nQuelles sont les raisons sécuritaires et/ou économiques pour effectuer le CSI ?\nComment le dommage est-il défini pour le système examiné et, dans le cas de plusieurs dommages, quels sont les cas les plus préoccupants ?\nQuelles sont les conditions opérationnelles et environnementales dans lesquelles le système à surveiller fonctionne ?\nQuelles sont les limites lors de l'acquisition de données dans l'environnement opérationnel ?\nL'évaluation opérationnelle commence à définir les limites de ce qui sera suivi et de la manière avec laquelle le suivi sera accompli. Cette évaluation commence à adapter le processus d'identification des dommages aux caractéristiques propres du système surveillé et tente d'exploiter les caractéristiques uniques des dommages qui doivent être détectés.\n\n\n=== Acquisition de données, normalisation et nettoyage ===\nLa partie d'acquisition de données du processus de CSI implique la sélection des méthodes d'excitation, les types des capteurs, leur nombre et leurs emplacements, et le matériel d'acquisition, de stockage et de transmission de données. Encore une fois, ce processus sera spécifique à l'application. Les considérations économiques joueront un rôle majeur dans la prise de ces décisions. Les intervalles auxquels les données doivent être recueillies sont également une considération à prendre en compte.\nVu que les données peuvent être mesurées dans des conditions variables, la capacité de normaliser les données devient très importante pour le processus d'identification des dommages. La normalisation des données est le processus qui consiste à séparer les changements de lecture du capteur causés par les dommages de ceux causés par les conditions opérationnelles et environnementales variables. L'une des procédures les plus courantes consiste à normaliser les réponses mesurées par les entrées mesurées. Lorsque la variabilité environnementale ou opérationnelle constitue un problème, il peut être nécessaire de normaliser les données de façon temporelle pour faciliter la comparaison des données mesurées à des moments similaires d'un cycle environnemental ou opérationnel. Les sources de variabilité dans le processus d'acquisition des données et dans le système surveillé doivent être identifiées et minimisées dans la mesure du possible. En général, on peut pas éliminer toutes les sources de variabilité. Par conséquent, il est nécessaire de faire les mesures appropriées tel que ces sources puissent être quantifiées statistiquement. La variabilité peut survenir de l'évolution des conditions environnementales et des conditions d'essai, des changements dans le processus de réduction des données et des incohérences d'unité à unité.\nLe nettoyage des données est le processus de choix sélective des données à transmettre ou à rejeter à partir du processus de sélection des traits caractéristiques. Le processus de nettoyage des données est généralement basé sur les connaissances acquises par les personnes directement impliquées dans l'acquisition des données. Par exemple, une inspection de la configuration de test peut révéler qu'un capteur est monté de manière lâche et par conséquent, sur la base du jugement des individus effectuant la mesure, cet ensemble de données ou les données de ce capteur particulier peuvent être particulièrement supprimées du processus de sélection des traits caractéristiques. Les techniques de traitement du signal telles que le filtrage et le rééchantillonnage peuvent également être considérées comme des procédures de nettoyage des données.\nEnfin, la portion d'acquisition de données, de normalisation et de nettoyage du processus de CSI ne doit pas être statique. Les informations obtenues grâce au processus de sélection des traits caractéristiques et au processus de développement du modèle statistique fourniront des informations sur les modifications susceptibles d'améliorer le processus d'acquisition des données.\n\n\n=== Extraction des traits caractéristiques et compression de données ===\nLa partie du processus de CSI qui reçoit le plus d'attention dans la littérature technique est l'identification des traits caractéristiques de données qui permettent de distinguer entre une structure saine et une structure endommagée. La condensation des données est inhérente à ce processus de sélection des traits caractéristiques. Les meilleures traits caractéristiques pour l'identification des dommages sont encore une fois spécifiques à l'application.\nL'une des méthodes d'extraction des traits caractéristiques les plus courantes repose sur la corrélation des grandeurs des réponses mesurées du système, tel que l'amplitude ou la fréquence de vibration, avec les observations de première main du système en cours de dégradation. Une autre méthode de développement des traits caractéristiques pour l'identification des dommages consiste à appliquer aux systèmes des défauts, similaires à ceux prévus dans les conditions réelles de fonctionnement, et à développer une compréhension initiale des paramètres qui sont sensibles aux dommages attendus. Le système défectueux peut également être utilisé pour valider que les mesures de diagnostic sont suffisamment sensibles pour distinguer les caractéristiques identifiées du système intact et du système endommagé. L'utilisation d'outils analytiques tels que les modèles d'éléments finis validés expérimentalement peut être un atout majeur dans ce processus. Dans de nombreux cas, les outils analytiques sont utilisés pour réaliser des expériences numériques où les défauts sont introduits dans une simulation informatique. Les essais d'accumulation des dommages, au cours desquels un nombre important de composants structuraux du système étudié sont dégradés en les soumettant à des conditions de charge réalistes, peuvent également être utilisés pour identifier les caractéristiques appropriées. Ce processus peut inclure des tests de dommages induits, des tests de fatigue, la croissance de la corrosion ou le cyclage de température pour accumuler certains types de dommages de manière accélérée. Un aperçu des caractéristiques appropriées peut être obtenu à partir de plusieurs types d'études analytiques et expérimentales comme décrit ci-dessus et il est généralement le résultat d'informations obtenues à partir d'une combinaison de ces études.\nLes techniques d'implémentation opérationnelle et de mesure diagnostique nécessaires à la réalisation du CSI produisent plus de données que les utilisations traditionnelles de l'information de dynamique structurelle. Une condensation des données est avantageuse et nécessaire lorsque des comparaisons de nombreux ensembles de traits caractéristiques obtenus au cours de la durée de vie de la structure sont envisagées. De plus, comme les données seront acquises à partir d'une structure sur une longue période et dans un environnement opérationnel, des techniques robustes de réduction des données doivent être développées pour conserver la sensibilité des traits caractéristiques aux changements structurels d'intérêt en présence d'une variabilité environnementale et opérationnelle. Afin de faciliter davantage l'extraction et l'enregistrement des données de qualité nécessaires pour effectuer le CSI, la signification statistique des traits caractéristiques doit être caractérisée et utilisée dans le processus de condensation.\n\n\n=== Développement du modèle statistique ===\nLa portion du processus de CSI qui a reçu le moins d'attention dans la littérature technique est le développement de modèles statistiques pour la discrimination entre les traits caractéristiques issues des structures intactes et des structures endommagées. Le développement de modèles statistiques est concerné par l'implémentation des algorithmes qui agissent sur les entités extraites afin de quantifier l'état d'endommagement de la structure. Les algorithmes utilisés dans le développement de modèles statistiques appartiennent d'une manière générale à trois catégories. Lorsque les données sont disponibles à la fois de la structure intacte et endommagée, les algorithmes de reconnaissance de formes statistiques se positionnent dans la classification générale appelée apprentissage supervisé. La classification de groupe et l'analyse de régression sont des catégories d'algorithmes d'apprentissage supervisé. L'apprentissage non supervisé fait référence à des algorithmes appliqués à des données ne contenant pas d'exemples de la structure endommagée. La détection des valeurs aberrantes ou de nouveauté est la classe primaire d'algorithmes utilisés dans des applications d'apprentissage non supervisées. Tous les algorithmes analysent les distributions statistiques des caractéristiques mesurées ou dérivées pour améliorer le processus d'identification des dommages.\n\n\n== Axiomes fondamentaux ==\nSur la base de la vaste littérature qui s'est développée sur le CSI au cours des deux dernières décennies, on peut affirmer que ce domaine a mûri au point où plusieurs axiomes fondamentaux, ou principes généraux, ont émergé. Ces axiomes sont listés comme suit :\n\nAxiome I : Tous les matériaux ont des défauts ou des imperfections inhérentes ;\nAxiome II : L'évaluation des dommages nécessite une comparaison entre deux états du système ;\nAxiome III : L'identification de l'existence et de l'emplacement des dommages peut se faire dans un mode d'apprentissage non supervisé, mais l'identification du type de dommage présent et de sa sévérité ne peut généralement être fait que dans un mode d'apprentissage supervisé ;\nAxiome IVa : Les capteurs ne peuvent pas mesurer les dégâts. L'extraction de caractéristiques à travers le traitement du signal et la classification statistique est nécessaire pour convertir les données du capteur en informations sur les dommages ;\nAxiome IVb : Sans une extraction intelligente des traits caractéristiques, plus une mesure est sensible à l'endommagement, plus elle est sensible aux conditions opérationnelles et environnementales changeantes ;\nAxiome V : Les échelles de longueur et de temps associées à l'initiation et à l'évolution des dommages dictent les propriétés requises du système de détection de CSI ;\nAxiome VI : Il y a un compromis entre la sensibilité aux dommages d'un algorithme et sa capacité de rejet de bruit ;\nAxiome VII : La taille des dommages qui peuvent être détectés à partir des changements dans la dynamique du système est inversement proportionnelle à la gamme de fréquence de l'excitation.\n\n\n== Composants ==\nLes éléments du système de CSI comprennent :\n\nLa structure\nLes capteurs\nLes systèmes d'acquisition des données\nLe transfert de données et le mécanisme de stockage\nLa gestion de données\nL'interprétation des données et le diagnostic :\nL'Identification du système\nLa mise à jour du modèle structurel\nL'évaluation de l'état structurel\nLa prédiction de la durée de vie restante.\nUn exemple de cette technologie est l'intégration de capteurs dans des structures telles que les ponts et les avions. Ces capteurs fournissent une surveillance en temps réel de divers changements structurels comme les contraintes et les déformations. Dans le cas des structures de génie civil, les données fournies par les capteurs sont généralement transmises à des centres d'acquisition de données distants. Avec l'aide de la technologie moderne, le contrôle en temps réel des structures (Le contrôle structurel actif) basé sur l'information des capteurs est possible.\n\n\n== Exemples ==\n\n\n=== Les ponts à Hong Kong ===\nLe système éolien et de contrôle de santé intégré est un système sophistiqué de surveillance des ponts, qui coûte 1,3 million de dollars américains, utilisé par le Département des autoroutes de Hong Kong pour assurer le confort et la sécurité des usagers des ponts Tsing Ma, Ting Kau, Kap Shui Mun et Stonecutters.\nAfin de surveiller l'intégrité, la durabilité et la fiabilité des ponts, le système éolien et de contrôle de santé intégré possède quatre niveaux d'opération différents: les systèmes sensoriels, les systèmes d'acquisition de données, les systèmes informatiques centralisés locaux et le système informatique central global.\nLe système sensoriel comprend environ 900 capteurs et leurs unités d'interfaçage pertinentes. Avec plus de 350 capteurs sur le pont Tsing Ma, 350 sur Ting Kau et 200 sur Kap Shui Mun, le comportement structurel des ponts est mesuré 24 heures par jour, sept jours par semaine.\nLes capteurs comprennent des accéléromètres, des jauges de contrainte, des transducteurs de déplacement, des stations de détection de niveau, des anémomètres, des capteurs de température et des capteurs dynamiques de poids en mouvement. Ils mesurent tout, de la température et des tensions de l'asphalte dans les éléments structuraux à la vitesse du vent et la déviation et la rotation des kilomètres de câbles et tout mouvement des ponts et des tours.\nCes capteurs sont le système d'alerte précoce pour les ponts, fournissant les informations essentielles qui aident le département des autoroutes à surveiller avec précision les conditions générales de santé des ponts.\nLes structures ont été construites pour résister jusqu'à une vitesse moyenne du vent de 95 m/s. En 1997, lorsque le Typhon Victor a frappé directement Hong Kong, les vitesses du vent de 110 à 120 km/h ont été enregistrés. Cependant, la vitesse de vent la plus élevée enregistrée a eu lieu pendant le typhon Wanda en 1962 quand un coup de vent de 3 secondes a été enregistrée à 78,8 mètres par seconde, 284 km/h.\nLes informations provenant de ces centaines de capteurs différents sont transmises aux unités d'acquisition de données. Il y a trois unités d'acquisition de données sur le pont de Tsing Ma, trois sur Ting Kau et deux sur le Kap Shui Mun.\nLa puissance de calcul pour ces systèmes est dans le bâtiment administratif utilisé par le département des autoroutes à Tsing Yi. Le système informatique central et local assure le contrôle de la collecte des données, le post-traitement, la transmission et le stockage.\nLe système global est utilisé pour l'acquisition et l'analyse des données, en évaluant les conditions physiques et les fonctions structurelles des ponts et pour l'intégration et la manipulation des processus d'acquisition, d'analyse et d'évaluation des données\n\n\n=== Autres exemples larges ===\nLes projets suivants sont actuellement connus parmi les plus grands projets de surveillance des ponts :\n\nLe pont de Rio–Antirrio, Grèce : possède plus de 100 capteurs surveillant la structure et le trafic en temps réel.\nLe Viaduc de Millau, France : a l'un des plus grands systèmes de fibres optiques du monde.\nLe Huey P pont Long, États-Unis : a plus de 800 jauges statiques et dynamiques de déformation conçues pour mesurer les effets de charges axiales et de flexion.\nLe Pont Fatih Sultan Mehmet, Turquie : a été contrôlé à l'aide d'un réseau de capteurs sans fil innovant avec des conditions de circulation normales.\nLe pont Sydney Harbour en Australie met actuellement en œuvre un système de surveillance impliquant plus de 2 400 capteurs. Les gestionnaires d'actifs et les inspecteurs de ponts disposent d'outils d'aide à la décision mobiles et de navigateurs webs basés sur l'analyse des données des capteurs.\nLe nouveau pont du Forth, actuellement[Quand ?] en construction dans le Firth of Forth, aura un système de surveillance comprenant plus de 2 000 capteurs dès son achèvement. Les gestionnaires d'actifs auront accès aux données de tous les capteurs à partir d'une interface de gestion de données basée sur le Web, y compris l'analyse de données automatisée[réf. nécessaire].\n\n\n== Pour les ponts ==\nLa surveillance de santé des grands ponts peut être effectuée en mesurant simultanément les charges sur le pont et les effets de ces charges. Elle comprend généralement la surveillance des :\n\nVents et météos\nCirculations\nPrécontraintes et des haubans\nPlates-formes\nPylônes\nSols\nMuni de cette connaissance, l'ingénieur peut :\n\nEstimer les charges et leurs effets ;\nEstimer l'état de fatigue ou autre état limite ;\nPrévoir l'évolution probable de l'état d'intégrité du pont/\nL'État de l'Oregon aux États-Unis, et plus précisément le département de l'ingénierie des ponts du département des transports a développé et a mis en œuvre un programme de contrôle de santé intégré (CSI) comme mentionné dans ce document technique par Steven Lovejoy, ingénieur sénior.\nDes références qui fournissent une introduction à l'application de capteurs à fibres optiques au contrôle de santé intégré sur les ponts sont disponibles.\n\n\n== Développements internationaux ==\n\n\n=== Asie ===\n\n\n==== Université nationale de Chonbuk (CBNU) ====\nDépartement de génie mécanique\n\n\n=== Amérique du Nord ===\n\n\n==== Canada ====\nBâtiment agricole et de génie civil, Université du Manitoba\n\n\n==== États-Unis ====\nSociété internationale pour le contrôle de santé intégré des infrastructures intelligentes (ISHMII).\n\n\n=== Europe ===\n\n\n==== Portugal ====\nLaboratoire de technologie du béton et des comportements structurels (LABEST) à Porto.\n\n\n==== Suisse ====\nInstitut fédéral de technologie de Lausanne.\nSR Technics Suisse SA\n\n\n=== Australie ===\nRéseau australien de Contrôle de Santé Intégré (CSI).\n\n\n== Notes et références ==\n\n\n=== Voir aussi ===\nSurveillance des déformations (en)\nCivionics (en)\nStructural Health Monitoring (en), une revue avec comité de lecture consacrée au sujet\n\n\n=== Bibliographie ===\nN. Bonessio, G. Lomiento, G. Benzoni, Damage identification procedure for seismically isolated bridges, Structural Control and Health Monitoring, vol. 19, no 5, 2012, p. 565–578. DOI 10.1002/stc.448.\nR. Ditommaso, S. Parolai, M. Mucciarelli, S. Eggert, M. Sobiesiak, J. Zschau, Monitoring the response and the back-radiated energy of a building subjected to ambient vibration and impulsive action: the Falkenhof Tower (Potsdam, Germany), in Bulletin of Earthquake Engineering, volume 8, no 3, 2010 DOI 10.1007/s10518-009-9151-4.\nDryver Huston, Structural Sensing, Health Monitoring, and Performance Evaluation, Taylor & Francis, 2010 (ISBN 978-0-7503-0919-6)\nY. Liu, S. Mohanty, A. Chattopadhyay, Condition Based Structural Health Monitoring and Prognosis of Composite Structures under Uniaxial and Biaxial Loading, 2010, Journal of Nondestructive Evaluation, volume 29, numéro 3, 181-188\nY. Liu, M. Yekani Fard, A. Chattopadhyay, D. Doyle, Damage assessment of CFRP composites using time-frequency approach, Journal of Intelligent Material Systems and Structures, vol. 23, no 4, p. 397 – 413, 2012.\nY. Liu, S.B. Kim, A. Chattopadhyay, D. Doyle, Application of system identification techniques to health monitoring of on-orbit satellite boom structures, Journal of Spacecraft and Rockets, vol.48, no 4, p. 589–598, 2011.\nS. Mohanty, A. Chattopadhyay, J. Wei, P. Peralta, Real time Damage State Estimation and Condition Based Residual Useful Life Estimation of a Metallic Specimen under Biaxial Loading, 2009, Structural Durability & Health Monitoring Journal, vol.5, no 1, p. 33–55.\nS. Mohanty, A. Chattopadhyay, J. Wei, P. Peralta, Unsupervised Time-Series Damage State Estimation of Complex Structure Using Ultrasound Broadband Based Active Sensing, 2010, Structural Durability & Health Monitoring Journal, vol. 130, no 1, p. 101–124.\nM. Picozzi, S. Parolai, M. Mucciarelli, C. Milkereit, D. Bindi, R. Ditommaso, M. Vona, M.R. Gallipoli, J. Zschau, Interferometric Analysis of Strong Ground Motion for Structural Health Monitoring: The Example of the L'Aquila, Italy, Seismic Sequence of 2009, Bulletin of the Seismological Society of America, vol. 101, no 2, avril 2011, p. 635–651 DOI 10.1785/0120100070.\n\n\n=== Liens externes ===\n\nInstitut d'ingénierie, Laboratoire national Los Alamos\nLaboratoire de nanotechnologie et de technologies des structures intelligentes (NESST), University de Californie, Davis\nUniversité de Siegen Allemagne\nLaboratoire des technologies structurelles intelligentes, Université de Michigan\nCentre pour l'évaluation non destructive IIT Madras, Inde\n Portail de l’architecture et de l’urbanisme   Portail des sciences des matériaux   Portail des ponts"
        },
        {
            "pageid": 16273129,
            "ns": 0,
            "title": "Critique du programme de la navette spatiale",
            "content": "Les critiques du programme de la navette spatiale viennent d'affirmations selon lesquelles le programme de la navette spatiale de la NASA n'avait pas atteint ses objectifs promis en matière de coût et d'utilité, ainsi que de conception, de coût, de gestion et de sécurité. Foncièrement, il n’a pas réussi à réduire le coût de l’accès à l’espace. Les coûts de lancement par kilogramme de la navette spatiale se sont finalement révélés considérablement plus élevés que ceux des lanceurs non réutilisables. En 2010, le coût par vol de la navette spatiale était de 380 millions € (409 millions de dollars), soit 13 154 € (14 186 dollars) par kilogramme en orbite terrestre basse (OTB). En revanche, le coût comparable du lanceur Proton était de 130 millions € (141 millions de dollars), soit 6 232 € (6 721 dollars) par kilogramme pour l'OTB, et celui du Soyouz 2.1 était de 51 millions € (55 millions de dollars), soit 6 180 € (6 665 dollars) par kilogramme, bien que ces lanceurs ne soient pas réutilisables.\nSi tous les coûts de conception et de maintenance sont pris en compte, le coût final du programme de la navette spatiale, moyenné sur toutes les missions et ajusté à l'inflation (de 2008), a été estimé à 1,4 milliard € (1,5 milliard de dollars) par lancement, soit 55 638 € (60 000 dollars) par kilogramme pour l'OTB. Cela doit être mis en contraste avec les coûts initialement envisagés de 241 € (260 dollars) par kilogramme de charge utile en 1972 (environ 1134 € par kilogramme (555 dollars par livre) en tenant compte de l'inflation jusqu'en 2019).\n\n« La navette spatiale fut designé pour être rentable à un taux de vol hebdomadaire, un but qui n'a jamais été crédible. »\n\n— Michael D. Griffin, administrateur de la NASA, 2007, Aviation Week.\nBien que la navette spatiale ait effectivement servi à entretenir les satellites et les stations spatiales en orbite, elle n’a pas atteint son objectif initial, à savoir permettre un accès régulier et fiable à l’espace, en partie à cause d’interruptions de plusieurs années dans les lancements à la suite des accidents de la navette spatiale. Elle ne fut jamais aussi économique que les fusées à usage unique pour le lancement de satellites. Les pressions budgétaires de la NASA, causées en partie par les coûts quotidiennement élevés du programme de la navette spatiale, éliminèrent les vols spatiaux avec équipage au-delà de l'orbite terrestre basse depuis Apollo et réduisirent considérablement l'utilisation de sondes sans équipage. La promotion et la dépendance de la NASA à l'égard de la navette ralentirent les programmes nationaux de lanceurs commerciaux à usage unique au moins jusqu'à la catastrophe de Challenger en 1986.\nDeux des cinq navettes spatiales furent détruites dans des accidents, tuant 14 astronautes : les accidents les plus mortels de la conquête spatiale.\n\n\n== Objectif du système ==\nLe « Système de Transport Spatial » (nom officiel donné par la NASA au programme global de la navette) a été créé pour transporter les membres d'équipage et les charges utiles sur des orbites terrestres basses. Il permettrait de mener des expériences scientifiques à bord de la navette pour étudier les effets du vol spatial sur les humains, les animaux et les plantes. D’autres expériences étudieraient comment des objets peuvent être fabriqués dans l’espace. La navette permettrait également aux astronautes de lancer des satellites depuis le vaisseau et même de réparer des satellites déjà dans l'espace. Elle était également destinée à la recherche sur les réponses du corps humain à l'environnement zéro-g.\nLa navette était initialement présentée comme un véhicule spatial capable de se lancer une fois par semaine et offrant de faibles coûts de lancement grâce à l'amortissement. Les coûts de développement devaient être compensés grâce à un accès fréquent à l'espace : affirmations qui furent faites dans le but d'obtenir un financement budgétaire du Congrès des États-Unis. À partir de 1981, la navette spatiale a commencé à être utilisée pour les voyages spatiaux. Cependant, au milieu des années 1980, l'idée de lancer autant de missions de navette s'est avéré irréaliste et les attentes de lancement furent réduites de 50 %. À la suite de l'accident de Challenger en 1986, les missions furent interrompues dans l'attente d'un examen de sécurité. Cette interruption devint longue et dura finalement près de trois ans alors que les disputes sur le financement et la sécurité du programme se poursuivaient. L'armée recommença en fin de compte à utiliser des lanceurs non réutilisables. Les missions furent de nouveau suspendues après la perte de Columbia en 2003. Au total, 135 missions furent lancées au cours des 30 années qui suivirent le premier vol orbital de Columbia, soit en moyenne environ une tous les 3 mois.\n\n\n== Coûts ==\nQuelques raisons expliquant les coûts opérationnels plus élevés que prévu sont :\n\nLa NASA obtenu un financement du budget de l'US Air Force en échange de sa contribution au processus de conception. Afin de remplir la mission de l'USAF consistant à lancer des charges utiles en orbite polaire, l'USAF insista et fut très exigeante concernant le fait que la navette devait avoir une portée de mise en orbite très large. Cela nécessita les énormes ailes delta de la navette, qui sont beaucoup plus grandes que les ailes tronquées de la conception originale. En plus d'ajouter de la traînée et du poids (près de 20 pour cent), le nombre excessif de tuiles chauffantes nécessaires pour protéger les ailes delta augmenta considérablement les coûts de maintenance, en plus d'augmenter les risques opérationnels tels que ceux qui aboutirent à la catastrophe de Columbia.\nL'USAF dupliqua toute l'infrastructure nécessaire au lancement et à l'entretien de la navette spatiale à la base aérienne de Vandenberg en Californie, pour un coût de plus de 3,71 milliards € (4 milliards de dollars). À la suite de l'explosion de Challenger, l'installation fut démantelée après n'avoir jamais lancé une seule mission de navette.\nL'ingénieur aérospatial Robert Zubrin décrivit la navette comme ayant été conçue « à l'envers » dans le sens où l'orbiteur, la partie la plus difficile à récupérer, est rendue récupérable, tandis qu'une partie du booster (le réservoir de carburant liquide) est jetée même si elle est plus facile à récupérer car elle ne vole pas aussi haut et aussi vite.\nLa maintenance des tuiles de protection thermique était un processus très laborieux et coûteux, avec quelque 35 000 tuiles devant être inspectées individuellement et chaque tuile étant spécialement fabriquée pour un emplacement spécifique de la navette.\nEn raison de la complexité des moteurs RS-25, ils devaient être retirés après chaque vol pour une inspection approfondie et un entretien méticuleux. Avant la livraison des moteurs Block II, le composant principal du moteur, la turbopompe, devait être retiré, démonté et révisé après chaque utilisation,.\nLes propulseurs toxiques utilisés pour les propulseurs OMS/RCS nécessitaient une manipulation particulière, pendant laquelle aucune autre activité ne pouvait être effectuée dans les zones partageant le même système de ventilation. Ce qui augmentait les délais de manœuvres.\nLe taux de lancement fut nettement inférieur à celui initialement prévu. Sans réduire les coûts d'exploitation absolus, un plus grand nombre de lancements par an entraîne un coût par lancement inférieur. Certaines premières études hypothétiques examinèrent la possibilité d'effectuer 55 lancements par an (voir ci-dessus), mais le taux de lancement maximum possible fut limité à 24 par an en fonction de la capacité de fabrication du centre d'assemblage de Michoud en Louisiane qui construisait le réservoir externe. Au début du développement de la navette, le taux de lancement prévu était d'environ 12 par an. Les taux de lancement atteignirent un sommet de 9 par an en 1985 mais étaient en moyenne de 4,5 pour l'ensemble du programme.\nLorsque la décision fut prise sur les principaux contracteurs de la navette en 1972, le travail fut réparti entre les entreprises pour rendre le programme plus attractif aux yeux du Congrès, comme le contrat pour les propulseurs d'appoint à poudre à Morton Thiokol dans l'Utah. Au cours du programme, cela augmenta les coûts opérationnels, même si la consolidation de l'industrie aérospatiale américaine dans les années 1990 signifiait que la majorité des dépenses du programme de navette étaient désormais confiées à une seule société : la United Space Alliance, une entreprise commune de Boeing et Lockheed Martin.\n\n\n== Questions et problèmes culturels ==\n\n« Pour qu'une technologie réussisse, la réalité doit passer avant les relations publiques, car la nature ne peut être berné. »\n\n— Richard Feynman, Rapport de la Commission Rogers sur l'accident de la navette spatiale Challenger.\nCertains chercheurs critiquèrent un changement généralisé dans la culture de la NASA, qui s'éloigne de la sécurité afin de garantir que les lancements aient lieu en temps régulier, ce qu'on appelle parfois la « go fever » (« fièvre du go », terme utilisé pour décrire le caractère imminent d'un lancement). Il semblerait que la haute direction de la NASA adopta cette priorité moindre en matière de sécurité dans les années 1980, tandis que certains ingénieurs restaient méfiants. Selon la sociologue Diane Vaughan, les programmes de lancement agressifs apparurent dans les années Reagan comme une tentative de réhabiliter le prestige américain de l'après-Vietnam.\nLe physicien Richard Feynman, chargé de l'enquête officielle sur la catastrophe de Challenger, écrivit dans son rapport que les ingénieurs de la NASA estimaient le risque d'échec de la mission « de l'ordre d'un pour cent », ajoutant : « D'un autre côté, les responsables prétendaient croire que la probabilité d'un échec était mille fois moindre ». L'une des raisons pourrait être une tentative d'assurer au gouvernement la perfection et le succès de la NASA afin d'assurer l'approvisionnement en fonds. L'autre pourrait être qu'ils y croyaient sincèrement, démontrant un manque considérable de communication entre eux et leurs ingénieurs en activité.\nMalgré les avertissements de Feynman, et malgré le fait que Vaughan ait siégé aux conseils et comités de sécurité de la NASA, la couverture médiatique qui suivi trouva des preuves que le relatif mépris de la NASA pour la sécurité persistait. Par exemple, avant la catastrophe de Columbia, la NASA écarté le risque de rupture de petits morceaux de mousse au lancement et supposa que l'absence de dommages causés par des collisions de mousse antérieures suggérait que le risque futur était faible.\n\n\n== Opérations techniques de la navette ==\n\nLa navette fut initialement conçue pour fonctionner comme une sorte d'avion de ligne. Après l'atterrissage, l'orbiteur serait vérifié et commencerait à être accouplé aux propulseurs du réservoir externe et serait prêt à être lancé en à peine deux semaines.\nEn pratique, avant la perte de Challenger, environ la moitié du temps d'exécution après une mission était constituée de tests et de modifications non planifiés basés sur des événements inattendus survenus pendant le vol. Le processus prenait généralement des mois ; Atlantis établit le record pré-Challenger en lançant deux fois en 54 jours, tandis que Columbia établit le record post-Challenger de 88 jours. L'objectif du programme de la navette, qui était de ramener son équipage sur Terre en toute sécurité, était en conflit avec l'objectif d'un lancement rapide et peu coûteux de la charge utile. De plus, comme dans de nombreux cas il n'existait aucun mode d'abandon permettant de survivre, de nombreux éléments matériels devaient fonctionner sans fautes et nécessitaient donc une inspection minutieuse avant chaque vol. Le résultat était un coût de main-d'œuvre élevé, avec environ 25 000 travailleurs dans les opérations de la navette et des coûts de main d'œuvre d'environ 1 milliard de dollars par an.\nCertaines fonctionnalités de la navette initialement présentées comme importantes pour le support d'une station spatiale se sont révélées superflues :\n\nComme les Soviétiques l’ont démontré, les capsules et les cargos de ravitaillement sans équipage suffisent à approvisionner une station spatiale.\nLa politique initiale de la NASA consistant à utiliser la navette pour lancer toutes les charges utiles sans équipage a décliné en pratique et a finalement été abandonnée. Les lanceurs à usage unique se révélèrent beaucoup moins chers et plus flexibles.\nÀ la suite de la catastrophe de Challenger, l'utilisation de la navette pour transporter les puissants étages supérieurs Centaur à ergols liquide prévus pour les sondes interplanétaires fut exclue pour des raisons de sécurité de la navette,.\nLes retards inattendus de la navette l'ont également amenée à rater des fenêtres de lancement étroites.\nLes progrès technologiques ont rendu les sondes plus petites et plus légères. En conséquence, de nombreuses sondes robotiques et satellites de communication peuvent désormais utiliser des lanceurs non réutilisables, tels que la Delta et l'Atlas V, qui sont moins chers et perçus comme plus fiables que la navette.\nLes progrès technologiques se produisent aujourd’hui beaucoup plus rapidement qu’au cours des années de développement de la navette. Ainsi, l’idée selon laquelle la navette serait utile pour récupérer des satellites coûteux en vue de leur retour sur Terre pour être remis à neuf et mis à jour avec de nouvelles technologies est obsolète ; les coûts ont baissé et les capacités ont tellement augmenté qu'il est désormais bien plus rentable d'abandonner les vieux satellites et de simplement en lancer de nouveaux.\n\n\n== Accidents ==\n\nMême si les détails techniques des accidents de Challenger et de Columbia sont différents, les problèmes organisationnels présentent des similitudes. Les préoccupations des ingénieurs de vol concernant d'éventuels problèmes n'ont pas été correctement communiquées ni comprises par les hauts responsables de la NASA. Le véhicule donnait suffisamment d'avertissement à l'avance en cas de problèmes anormaux. Une structure bureaucratique à plusieurs niveaux et axée sur les procédures entravait la communication et les actions nécessaires.\nAvec Challenger, un joint torique s'est érodé lors des lancements de navettes précédents, alors qu'il n'aurait jamais dû. Pourtant, les gestionnaires estimaient que, comme l'érosion n'était pas supérieure à 30 % auparavant, cela ne constituait pas un danger car il y avait « une marge de sécurité d'un facteur trois » (en réalité, la pièce était tombée en panne et il n'y avait aucun facteur de sécurité). Morton-Thiokol conçut et fabriqua les boosters, et lors d'une conférence téléphonique préalable au lancement avec la NASA, Roger Boisjoly, l'ingénieur Thiokol le plus expérimenté en matière de joints toriques, supplia à plusieurs reprises la direction d'annuler ou de reprogrammer le lancement. Il fit part de ses craintes que les températures inhabituellement basses ne durcissent les joints toriques, empêchant une étanchéité complète lors de la flexion des segments du moteur-fusée, ce qui était exactement ce qui se produisit lors du vol mortel. Cependant, les hauts responsables de Thiokol, sous la pression de la direction de la NASA, ont annulé sa décision et autorisé le lancement. Une semaine avant le lancement, le contrat de retraitement des propulseurs à poudre de Thiokol devait également être revu, et l'annulation du vol était une action que la direction de Thiokol voulait éviter. Les joints toriques de Challenger s'érodèrent entièrement comme prévu, entraînant la destruction complète du vaisseau spatial et la mort des sept astronautes à bord.\nColumbia fut détruite en raison d'une protection thermique endommagée par des débris de mousse qui se détachèrent du réservoir externe lors de l'ascension. La mousse n'avait pas été conçue ni prévue pour se briser, mais il fut observé par le passé qu'elle se détachait sans incident. Les spécifications techniques originales de la navette indiquaient que les tuiles de protection thermique de l’orbiteur n’étaient pas du tout conçues pour résister aux impacts de débris. Au fil du temps, les responsables de la NASA acceptèrent progressivement davantage de dommages aux tuiles, de la même manière que les dommages aux joints toriques étaient acceptés. La Commission d'enquête sur l'accident de Columbia appela cette tendance la « normalisation de la déviance » : une acceptation progressive d'événements en dehors des tolérances de conception de l'engin simplement parce qu'ils n'avaient pas été catastrophiques jusqu'à présent.\n\nLe sujet des tuiles thermiques manquantes ou endommagées sur la flotte des navettes n'est devenu un problème qu'après la perte du Columbia en 2003, celle-ci se désintégrant durant sa rentrée. En réalité, les navettes étaient auparavant revenues avec jusqu'à 20 tuiles manquantes sans aucun problème. STS-1 et STS-41 avaient tous deux volé avec des tuiles thermiques manquantes dans les modules du système de manœuvre orbital (visibles par l'équipage). Le problème sur Columbia était que les dommages provenaient d'un impact de mousse sur le panneau de bord d'attaque renforcé encarbone-carbone de l'aile, et non sur les tuiles thermiques. La première mission de navette, STS-1, comportait un dispositif de remplissage en saillie qui détournait les gaz chauds vers le passage de roue droit lors de la rentrée, entraînant une déformation de la trappe du train d'atterrissage principal droit.\n\n\n=== Facteurs à risques ===\n\nUn exemple d'analyse des risques techniques pour une mission STS est l'itération SPRA 3.1 des principaux contributeurs de risques pour STS-133, :\n\nFrappes de débris orbitaux de micro météoroïdes.\nDéfaillance catastrophique causé par les RS-25 (moteurs principaux de la navette spatiale).\nDes débris d'ascension frappant le TPS menant au LOCV en orbite ou à la rentrée.\nErreur de l'équipage lors de la rentrée.\nDéfaillance catastrophique du RSRM provoqué par le RSRM (moteurs fusées des boosters).\nPanne du COPV (réservoirs à l'intérieur de l'orbiteur qui contiennent du gaz à haute pression).\nJohn Young et Jerry L. Ross faisaient partie de ces astronautes qui pensaient que la navette a toujours été un engin expérimental, et non un véhicule opérationnel pour des vols spatiaux de routine, comme le déclara le président Ronald Reagan après STS-4. Rick Hauck dit en 2017 qu'avant STS-1, il avait vu une analyse estimant le risque de perte du véhicule à 1 sur 280, mais une étude interne d'évaluation des risques de la NASA (menée par le Shuttle Program Safety and Mission Assurance Office au Johnson Space Center) publié fin 2010 ou début 2011 concluait que l'agence avait sérieusement sous-estimé le niveau de risque lié aux opérations de la navette. Le rapport évalue qu'il y avait 1 chance sur 9 qu'une catastrophe se produise au cours des neuf premiers vols de la navette, mais que les améliorations en matière de sécurité ont ensuite amélioré ce rapport de risque à 1 sur 90. En 1984, Reagan signa une directive de sécurité nationale stipulant que la navette ne serait pas « pleinement opérationnelle » tant qu'elle ne pourrait pas effectuer 24 missions par an, peut-être d'ici 1988 ; la navette ne vola jamais plus souvent que les neuf missions de 1985, et effectua en moyenne environ six missions par an entre 1988 et 2003.\nBien que de nombreux astronautes de la NASA aient critiqué le programme de spécialiste de charge utile, en partie parce qu'ils ne pensaient pas que des étrangers moins formés étaient pleinement conscients des risques des vols spatiaux, les astronautes à plein temps ne l'étaient peut-être pas non plus. Charles Bolden fut étonné d'apprendre après la perte de Columbia que les bords d'attaque « impénétrables » des ailes du véhicule qu'il a piloté pendant 14 ans avaient moins d'un pouce d'épaisseur. En octobre 1982, la NASA prévoyait 37 vols de navette d'ici le début de 1986, mais la perte de Challenger était le 25e vol de navette. Hauck, avec beaucoup d'expérience dans le pilotage d'avions dangereux à la Naval Test Pilot School des États-Unis, déclara : « Si je savais à l'avance qu'un sur vingt-cinq échouerait, j'y réfléchirais probablement à deux fois avant de piloter trois fois (comme je l'ai fait) sur le premier des vingt-six vols ».\n\n\n== Rétrospectivement ==\nBien que le système ait été développé dans le cadre des estimations initiales de coût et de temps données au président Richard M. Nixon en 1971, les coûts opérationnels, le taux de vol, la capacité de charge utile et la fiabilité au moment de l'accident de Columbia en février 2003 se sont révélés bien pires qu'à l'origine anticipé. Un an avant le lancement de STS-1 en avril 1981, Gregg Easterbrook dans le Washington Monthly prévoyait de manière assez juste de nombreux problèmes de la navette. Notamment un calendrier de lancement trop ambitieux et, par conséquent, un coût marginal par vol plus élevé que prévu ; les risques de dépendre de la Navette pour toutes les charges utiles, civiles et militaires ; l'absence de scénario d'abandon permettant de survivre en cas de panne d'un booster ; et la fragilité du système de protection thermique de la navette,.\n\nAfin de faire approuver la navette, la NASA promit trop sur ses économies et son utilité. Pour justifier le coût fixe très élevé de son programme opérationnel, la NASA força initialement l'embarquement de toutes les charges utiles nationales, internes et du ministère de la Défense à utiliser sur la navette[réf. nécessaire]. Lorsque cela s'avéra impossible (après le désastre de Challenger ), la NASA utilisa la Station spatiale internationale (ISS) comme justification de la navette. L'administrateur de la NASA, Michael D. Griffin, déclara dans un article de 2007 que le programme Saturn, que s'il avait été poursuivi, aurait pu fournir six lancements en équipage par an, dont deux vers la Lune, au même coût que le programme de la navette, avec une capacité supplémentaire de charge utile et d'infrastructure pour d’autres missions :\n« Si nous avions fait tout cela, nous serions sur Mars aujourd’hui, et nous n’en parlerions pas pour \"les 50 prochaines années\". Nous aurions des décennies d’expérience dans l’exploitation de systèmes spatiaux de longue durée en orbite terrestre, et des décennies similaires d’expérience dans l’exploration et l’apprentissage de l’utilisation de la Lune. »\n\nCertains affirmèrent que le programme de navette présentait trop de défauts. La réalisation d'un véhicule réutilisable avec la technologie du début des années 1970 força des décisions de conception qui compromettaient la fiabilité opérationnelle et la sécurité. Les moteurs principaux réutilisables sont devenus une priorité. Cela nécessitait qu'ils ne brûlent pas lors de leur rentrée atmosphérique, ce qui faisait de leur montage sur l'orbiteur lui-même (la seule partie du système de la navette où la réutilisation était primordiale) une décision apparemment logique. Cependant, cela a eu les conséquences suivantes :[réf. nécessaire]\nUne conception de moteur dite « feuille propre » plus coûteuse était nécessaire, utilisant des matériaux plus coûteux, par opposition aux alternatives disponibles dans le commerce existantes et prouvées (telles que ceux de la Saturn V).\nUne augmentation des coûts de maintenance continue liés au fait de maintenir les SSME réutilisables en état de vol après chaque lancement, coûts qui au total pourraient avoir dépassé ceux de la construction de moteurs principaux jetables pour chaque lancement.\nUne préoccupation exprimée par la Commission Augustine en 1990 était que « le programme spatial civil dépend trop de la navette spatiale pour accéder à l'espace ». La commission souligna « qu'il était par exemple inapproprié, dans le cas de Challenger, de risquer la vie de sept astronautes et de près d'un quart des moyens de lancement de la NASA pour mettre en orbite un satellite de communication ».\nCertaines technologies dérivées de la NASA liées au programme de la navette spatiale furent développées avec succès en produits commerciaux, telles que l'utilisation de matériaux résistants à la chaleur développés pour protéger la navette lors de sa rentrée dans des combinaisons pour les pompiers municipaux et de sauvetage aérien.\n\n\n== Voir aussi ==\nBourane (vaisseau spatial)\nSkylab 4\n\n\n== Références ==\n\n\n== Liens externes ==\nQuand la physique, l’économie et la réalité se heurtent : le défi d’un accès orbital bon marché\nExamen du coût de mise en orbite des lanceurs à usage unqique par livre\nCoûts du transport spatial : tendances du prix par livre en orbite\nPopular Science novembre 1974, « Navette spatiale réutilisable » par Wernher von Braun Portail de l’astronautique"
        },
        {
            "pageid": 13269482,
            "ns": 0,
            "title": "Deep tech",
            "content": "Les jeunes pousses disruptives ou de rupture (disruptive startups ou deep tech startups en anglais), sont de jeunes entreprises développant des techniques considérées comme fortement novatrices.\n\n\n== Définition ==\nLes jeunes pousses disruptives sont de jeunes entreprises mettant au point des techniques considérées comme fortement novatrices.\nL'expression est employée pour qualifier une Start-up qui crée un produit exploitant une innovation technique importante, de rupture,.\nTrois grands critères distinguent une jeune deeptech startup des autres entreprises: elle propose un produit qui apporte une forte valeur ajoutée sur son marché, elle exploite une technologie disruptive souvent protégée par un brevet et elle se développe en lien étroit avec la recherche (collaboration, licence de brevet, chercheur entrepreneur...).\nElles requièrent de longs cycles de recherche et développement (R&D), voire de recherche scientifique, et de coûteux investissements avant d'atteindre un segment de marché et de pouvoir prétendre à un succès commercial. La propriété intellectuelle, la propriété industrielle (PI) ainsi que le savoir-faire qui s'y rapporte sont généralement très protégés et difficiles à reproduire, ce qui constitue un avantage compétitif important et une barrière pour la concurrence,,,.\n\n\n=== Un terme catégorisant ===\nDu point de vue commercial, les jeunes pousses disruptives se distinguent par trois caractéristiques : potentiel d'impact important, long temps de maturation et besoin substantiel de capitaux.\nLe temps nécessaire pour passer de la recherche fondamentale à une technologie commercialisable est beaucoup plus important que celui de développement des jeunes pousses exploitant des techniques préexistantes (shallow tech, ou techniques peu profondes) telles que les applications mobiles, les sites internet et les services de commerce en ligne,. Par exemple, le développement de l'intelligence artificielle a pris des décennies, alors que des entreprises basées sur l'IA se développent maintenant dans nombre de secteurs. En 2019, selon Hello Tomorrow, il faut en moyenne quatre ans pour atteindre le marché dans la biotechnologie et 2,4 ans dans celui de la blockchain.\nLes énormes besoins de financement dès les stades de R&D, de prototypage et la lenteur du développement de ces jeunes pousses les obligent à abandonner les circuits de financement de type amis/famille pour se tourner vers les circuits de financement de type Business angel, Capital d'amorçage ou des tours de table pour élargir leurs actionnaires pouvant mener jusqu'à une introduction en bourse ou une vente commerciale.\n\n\n=== Domaines d'actions des start-up deep tech ===\nSelon les recherches effectuées par le Boston Consulting Group et Hello Tomorrow, organisation française qui étudie les innovations de rupture, les sciences des matériaux, l'intelligence artificielle, les biotechnologies, la blockchain, la robotique, la photonique, l'électronique et l'information quantique sont les domaines où les jeunes pousses disruptives (Deep tech) sont les plus nombreuses.\nL'agriculture, les sciences de la vie, la chimie, l'aérospatiale et les énergies propres sont les domaines d'application les plus fréquents.\n\n\n== Histoire ==\nLe financement des jeunes entreprises disruptives a augmenté au fil des ans. Selon le Boston Consulting Group, l'investissement total dans les entreprises de biotechnologie est passé de 1,7 à 7,9 milliards de dollars entre 2011 et 2016.\nLes investissements sont principalement concentrés aux États-Unis et en Chine. À eux deux, ils totalisent 81 % des investissements privés entre 2015 et 2018 pour respectivement 32,8 et 14,6 milliards de dollars. La Chine est le principal moteur des investissements dans les technologies de rupture avec une croissance de 80 % chaque année, contre 10 % pour les États-Unis.\nLes investissements privés y ont augmenté de 20 % depuis 2015 et atteignent presque 18 milliards en 2018. On recense en France 200 nouvelles startups deep tech par an pour 1,5 milliard d'euros investis en 2019 et autour de 15 000 créés,.\nLes pays européens sont également actifs : en 2017, selon le Financial Times, les levées de fonds atteignaient trois milliards de dollars pour six cent transactions,.\nEn France, l'association French Deeptech a été créée en novembre 2023 en partenariat entre des startups, des fonds d'investissement, des laboratoires de recherche et les pouvoirs publics afin « de faire émerger des champions mondiaux du secteur ».\nDes entreprises telles que Google, Facebook, Amazon, IBM et Apple montrent un intérêt accru pour les entreprises disruptives dans les domaines de l'intelligence artificielle, de la réalité virtuelle, des drones et des véhicules autonomes. Les incubateurs de jeunes entreprises délaissent de plus en plus le domaine du numérique pour les innovations de rupture. En 2016, Y Combinator (un incubateur américain) comptait trente-deux jeunes pousses (start-up) disruptives, dont neuf en biotechnologie, quatre liées aux drones et trois touchant au matériel informatique.\n\n\n== Notes et références ==\n\n\n== Articles connexes ==\nCapital risque\nHello Tomorrow\n Portail des entreprises   Portail des sciences   Portail des technologies"
        },
        {
            "pageid": 10830501,
            "ns": 0,
            "title": "Dénialisme",
            "content": "Le dénialisme (denialism) est le choix de nier un fait ou un consensus, sans justification rationnelle. Le terme de déni peut renvoyer soit au dénialisme, lorsqu’une réalité est niée ou minimisée pour des raisons politique ou de cohérence idéologique, soit à la dénégation, lorsqu’on fait référence au besoin d'éviter une vérité psychologiquement inconfortable.\nFace à la science, le dénialisme est le rejet du consensus scientifique sur un sujet donné, en faveur d'idées radicales controversées à un niveau médiatique. Ce déni se caractérise donc par le rejet des preuves scientifiques patentes; la création de contenus faussement scientifiques ; les tentatives de générer une controverse, ; les tentatives de nier l'existence d'un consensus.\nEn sociologie, le dénialisme est aussi un comportement individuel ou collectif de malhonnêteté ou de laxisme qui consiste à ne pas prendre en compte un fait.\n\n\n== Description ==\nFace à la science, le dénialisme se caractérise par le rejet des preuves scientifiques ; la création de contenus faussement scientifiques ; les tentatives de générer une controverse, ; les tentatives de nier l'existence d'un consensus. Si cette posture est irrationnelle ; ses motivations ne le sont généralement pas.\nDénialisme et dénégation renvoient à des réalités distinctes. Le mot \"dénialisme\" évoque une tendance à nier ou à minimiser des faits établis, plutôt en raison de convictions idéologiques ou religieuses. Il est utilisé dans un contexte social ou politique pour désigner une forme de résistance au changement ou à l'acceptation d'une réalité qui tend à être jugée inacceptable. Le mot \"dénégation\" désigne lui le fait de nier ou de refuser de reconnaître un fait, une réalité ou une responsabilité. Il est d’abord utilisé dans un contexte psychologique ou psychiatrique pour désigner un mécanisme de défense qui permet à l'individu de se protéger d'une réalité qu'il ne peut pas accepter.\nLe déni de la science s’est accru considérablement ces dernières années. Un facteur majeur est la propagation rapide de la désinformation et des fausses nouvelles par les médias sociaux (comme Facebook), ainsi que la mise en évidence de cette désinformation dans les recherches Google.\nCependant, John Cook et ses collègues,, ont montré que la théorie de l’inoculation psychologique peut contrer le déni.\nLes motivations et les causes du déni comprennent les croyances religieuses, le machiavélisme (populisme) et l'intérêt personnel (économique, politique ou financier, etc.) ; mais aussi, les mécanismes de défense destinés à protéger la psyché contre des faits et des idées mentalement dérangeants,, souvent appelés dissonance cognitive en termes de psychologie.\nDesveaux distingue trois formes de déni : le dénialisme, le désaveu et la dénégation.\n\n\n== Dénialisme et négationnisme ==\nEn langue française, les termes « dénialisme » et « négationnisme » ont deux sens voisins mais sensiblement différents. Tous deux désignent le déni de faits établis, mais le dénialisme désigne plutôt le déni de faits scientifiques tel que le réchauffement climatique tandis que le négationnisme désigne le déni de faits historiques telle que la Shoah. Comme l'explique le philosophe Normand Baillargeon, « on pourra être tenté de traduire dénialisme par « négationnisme ». Cependant, ce serait oublier que ce mot désigne déjà, en français, le refus d’admettre la réalité de la Shoah qui est un fait historiquement prouvé. » Le terme négationnisme est également utilisé par certains pour désigner le déni de l'existence de communautés immigrantes en France, d'une deuxième religion comme l'Islam ou encore des guerres coloniales tel que la guerre d'Algérie[source insuffisante]. On peut considérer le négationalisme[à vérifier] comme un cas particulier de dénialisme, visant des faits établis par la science historique.\n\n\n== Exemples ==\nIl y a de nombreux cas de dénialisme, les principaux sont d'après Normand Baillargeon :\n\nle refus d’admettre l’efficacité et l’innocuité quasi totale des vaccins ;\nle refus d'admettre que le VIH cause le sida ;\nle refus d'admettre l'évolution biologique, et spécialement que l'espèce humaine soit le produit de l'évolution par sélection naturelle ».\nle refus d’admettre la réalité du réchauffement climatique anthropique (voir détail ci-dessous).\n\n\n=== Déni du réchauffement climatique ===\n\n« Déni du réchauffement climatique » est une expression qui désigne, de manière générale, une attitude de déni face au consensus scientifique relatif au réchauffement de la planète.\nCertaines personnes admettent qu'il y a un réel changement, allant dans le sens d'un réchauffement global, mais nient que ce changement ait une origine ou une part anthropique ; ils l'attribuent exclusivement aux variations naturelles du climat. D'autres nient que ce changement affecte déjà négativement les écosystèmes ou qu'il puisse affecter les sociétés humaines, estimant parfois que le CO2 ou le réchauffement est une chance pour le tourisme ou l'agriculture ; ces derniers jugent alors inutile toute démarche d'adaptation au changement climatique.\nCertains « négateurs » approuvent le terme de « déni ». D'autres préfèrent se dire « climatosceptiques », mais plusieurs scientifiques estiment que le mot « scepticisme » est désormais inexact pour qualifier l'attitude de négation du réchauffement climatique anthropique,.\nAu sens large, ce déni peut aussi être « implicite » : quand des individus ou des groupes sociaux acceptent les hypothèses et démonstrations scientifiques, mais sans parvenir à les traduire en action ou en changements de comportements. Plusieurs travaux de sciences sociales ont analysé ces attitudes, en les classant comme des formes de négationnisme, voire de pseudoscience. Toutes ces formes de déni alimentent la controverse sur le changement climatique, et inversement.\nDes campagnes visant à saper la confiance du public dans les sciences du climat ont été mises en évidence, en Amérique du Nord notamment. Elles ont été décrites comme une « machine à produire du déni », construite, financée et entretenue par des intérêts industriels, politiques et idéologiques, trouvant des relais dans les médias conservateurs et les « blogueurs sceptiques » afin de créer l'impression qu'il existe une grande incertitude autour des données montrant que la planète se réchauffe,.\nSelon des observateurs tels que Naomi Klein (2011), ces campagnes de déni sont soutenues par ceux qui prônent des politiques économiques conservatrices, et par des intérêts industriels opposés à la réglementation ou la taxation des émissions de CO2 (et équivalent CO2), en particulier les lobbies du charbon et plus généralement des énergies fossiles, les frères Koch, des groupes de défense de l'industrie ainsi que des think tanks conservateurs et libertariens, souvent américains,,,. Plus de 90 % des articles « sceptiques » sur le changement climatique proviennent de groupes de réflexion classés à droite.\nBien que, depuis la fin des années 1970, les sociétés pétrolières soient arrivées au cours de leurs recherches à des conclusions correspondant largement au consensus scientifique sur le réchauffement de la planète, elles ont fomenté une longue campagne de dénégation du changement climatique — durant plusieurs décennies — en s'appuyant sur une stratégie qui a été comparée au déni organisé sur les dangers du tabagisme par l'industrie du tabac,,.\nLe déni du changement climatique et la controverse politique sur le réchauffement ont eu une forte incidence sur les politiques en matière de réchauffement de la planète, sapant une partie des efforts déployés pour lutter contre le changement climatique ou pour s'y adapter,,.\nCeux qui encouragent ou créent ce déni utilisent couramment des tactiques et moyens rhétoriques donnant l’apparence d’une controverse scientifique là où il n’y en a pas,.\nLe « militantisme dénialiste » a évolué en se créant de nouveaux fronts, et il s'est encore accru en france en juillet 2022 dans certains réseaux sociaux (twitter notamment), alors même que le monde vivait des températures record, des événements extrêmes, et que la COP27 se déroulait avec environ 600 délégués envoyés par les lobbys des industries fossiles.)\n\n\n=== Déni de la théorie du germe ===\n\nLe déni de la théorie du germe est une croyance pseudoscientifique qui prétend que les bactéries et virus ne causent pas de maladies.\n\n\n=== SIDA ===\n\nLa négation du SIDA et de nombreuses théories du complot liées fleurissent notamment sur Internet. Thabo Mbeki, ancien président sud-africain, a freiné l'accès des malades sud-africains aux antirétroviraux en partie sous l'influence de ce dénialisme.\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\n\n\n=== Bibliographie ===\n(en) Janet Maslin, « Michael Specter Fires Bullets of Data at Cozy Antiscience in 'Denialism' », The New York Times,‎ 4 novembre 2009 (lire en ligne).\n(en) Paul O'Shea, A Cross Too Heavy: Eugenio Pacelli, Politics and the Jews of Europe 1917-1943, Kenthurst, Rosenberg Publishing, 2008, 392 p. (ISBN 978-1-877058-71-4, lire en ligne), p. 20.\n(en) Pascal Diethelm et Martin McKee (en), « Denialism: what is it and how should scientists respond? », European Journal of Public Health, vol. 19, no 1,‎ 2009, p. 2-4 (PMID 19158101, DOI 10.1093/eurpub/ckn139, lire en ligne).\n(en) Martin McKee (en) et Pascal Diethelm, « How the growth of denialism undermines public health », British Medical Journal, vol. 341,‎ 2010, p. 1309–1311 (PMID 21156741, DOI 10.1136/bmj.c6950, lire en ligne).\n(en) David Hambling, « Abominable 'No' Men », Fortean Times,‎ 1er septembre 2009 (lire en ligne).\n(en) George Monbiot, « The denial industry », Guardian Unlimited,‎ 19 septembre 2006 (lire en ligne).\n(en) M. Scudellari, « State of denial », Nature Medicine, vol. 16, no 3,‎ mars 2010, p. 248 (PMID 20208495, DOI 10.1038/nm0310-248a).\n\n\n=== Articles connexes ===\nDéni du réchauffement climatique\nControverse fabriquée\nBiais cognitif\nNégationnisme\nRésistance au changement\n\n\n=== Liens externes ===\n« Nouveaux charlatans : on touche le faux », La Science, CQFD, France Culture, 20 novembre 2024.\n Portail de la psychologie   Portail du scepticisme rationnel   Portail du droit   Portail du climat"
        },
        {
            "pageid": 1043494,
            "ns": 0,
            "title": "Discipline scientifique",
            "content": "Les disciplines scientifiques (aussi appelées « sciences », « domaines scientifiques », « disciplines académiques », ou « champs de recherche ») sont des subdivisions de la science où chaque discipline scientifique étudie un domaine particulier. La physique, la biologie, la chimie, la sociologie, la psychologie, l'histoire ou encore la linguistique ou les mathématiques (liste non exhaustive) sont des grands regroupements disciplinaires traditionnels. Les disciplines scientifiques se caractérisent par l'utilisation de méthodes d'administration de la preuve, consistant tester des hypothèses, recueillir des données, formuler des théories explicatives sur le fonctionnement et l'histoire du monde naturel ou social, etc. Chaque discipline scientifique possède ses propres objets d'étude, concepts, méthodes et normes de recherche, mais des objets d'étude, concepts, méthodes et normes de recherche peuvent être partagées par plusieurs disciplines. Il existe différents principes de classification des disciplines scientifique : catégorisation par objet, méthode, types de problèmes, paradigme, etc. et une multitude de proposition de classification, dont la célèbre classification des sciences d'Auguste Comte.\n\n\n== Identification des disciplines scientifiques ==\n\nUn système de disciplines scientifiques est une forme spécifique de division du travail qui structure la recherche et l'enseignement. Une discipline scientifique est « un conglomérat de problèmes et d’essais de solutions qui a été délimité artificiellement » organisées par traditions académiques,,. Comme le précise l'épistémologue Léna Soler, « les découpages considérés (inanimé/vivant ; psychique/matériel ; etc.) ne préexistent pas tels quels dans la nature. Ce sont justement les différentes sciences qui, au cours de leur évolution, spécifient [...] ce qui relève de leur domaine [...] ou ce qui en est exclu ». En résumé, les disciplines se développent dans des contextes historiques, sociaux, et culturels ; elles dépendent des contextes institutionnels et communautaires ; elles sont des conventions et elles se transforment au cours du temps. Ainsi, dresser un inventaire des différentes disciplines scientifiques est un exercice nécessairement partial, dépendant du contexte, notamment géographique et historique, il existe d'ailleurs de nombreux chevauchements entre disciplines. Ces précisions étant apportées, on se propose de présenter quelques grands regroupements traditionnels.\n\n\n=== L'histoire ===\n\nLa science historique, ou histoire, est une discipline des sciences sociales qui étudie les événements passés et les transformations qu'a connues l'humanité à travers les âges. Elle cherche à établir une connaissance rigoureuse et critique du passé à partir de la recherche de traces, d'archives et de témoignages. Les historiens cherchent à comprendre les causes et les conséquences des événements historiques en s'appuyant sur une méthodologie rigoureuse d'analyse des sources, de contextualisation et de comparaison. Ils cherchent également à mettre en évidence les continuités et les ruptures dans l'évolution de l'humanité, ainsi que les interactions entre les différentes cultures, sociétés et civilisations. La science historique a des liens étroits avec d'autres disciplines, telles que la géographie, la sociologie, l'anthropologie et la science politique. La science historique repose sur la collecte de sources historiques ou archéologiques (textes, images, archives, témoignages, objets, ruines) que l'historien se doit de contextualiser, de comparer, de critiquer et d'analyser afin de dégager des connaissances fiables et objectives. L'histoire est une discipline vaste qui se subdivise en plusieurs branches : histoire culturelle, histoire sociale, histoire économique, histoire des sciences, histoire des idées, histoire intellectuelle, histoire militaire, histoire politique, histoire du droit, histoire naturelle, archéologie, etc. L'histoire permet de mieux comprendre les enjeux et conflits actuels en prenant du recul et revenant sur les racines des problèmes contemporains. Elle permet aussi mettre en évidence les erreurs passées. Elle sert ainsi de guide pour les politiciens, les décideurs et les citoyens dans leur réflexion sur les politiques publiques, les relations internationales et les défis sociétaux.\n\n\n=== La géographie ===\n\nLa géographie est une discipline à la jonction des sciences sociales et naturelles qui étudie les phénomènes et les processus relatifs à la surface terrestre, ainsi que les interactions entre les êtres humains et leur environnement. La géographie allie des méthodes qualitatives et quantitatives. Elle se subdivise en plusieurs branches : la géographie physique, qui étudie les caractéristiques physiques de la Terre, telles que le relief, les climats, les sols, les rivières et les océans ; la géographie humaine, qui se concentre sur les activités humaines, comme les modes de vie, les migrations, les cultures, les économies et les sociétés ; la géographie environnementale, qui examine les interactions entre les êtres humains et leur environnement naturel, ainsi que les problèmes environnementaux tels que la pollution, le changement climatique et la gestion des ressources naturelles ; la géopolitique, qui étudie les relations entre l'espace géographique, les ressources naturelles et les enjeux politiques internationaux; etc. La géographie connaît de nombreuses applications au service de problématiques telles que l'aménagement du territoire, la protection de l'environnement, le tourisme, le commerce international, les conflits géopolitiques, la diversité culturelle.\n\n\n=== L'anthropologie ===\n\nL'anthropologie est une discipline à la jonction des sciences sociales et naturelles qui étudie l'humain dans toutes ses dimensions, passées et présentes, biologiques (anatomiques, biologiques, morphologiques, physiologiques, évolutives) et culturelles (sociales, religieuses, linguistiques, psychologiques, géographiques). Elle cherche à comprendre les cultures humaines, ainsi que leur histoire, leur évolution, leur diversité et leur impact sur les comportements individuels et collectifs. En cela, elle entretient de nombreuses connexions avec la sociologie. L'anthropologie constitue finalement une monographie sur le genre Homo, qui décrit et analyse les « faits anthropologiques », c'est-à-dire caractéristiques de l'hominisation et de l'humanité. L'anthropologie utilise une approche comparative. Elle utilise également des méthodes de recherche telles que les enquêtes ethnographiques, l'observation participante, les entretiens et les analyses de données pour collecter et analyser des informations sur les cultures humaines. Elle se divise en plusieurs branches : l'anthropologie sociale et culturelle, l'anthropologie physique, l'anthropologie économique, l'anthropologie politique, l'anthropologie juridique, l'anthropologie religieuse, l'anthropologie des violences, l'anthropologie des émotions, l'anthropologie des migrations, ou encore l'ethnologie et l'archéologie. L'anthropologie permet de mieux comprendre la diversité humaine, de remettre en question nos propres normes culturelles et de mieux comprendre les différences culturelles entre les peuples. Elle fournit également des outils pour comprendre les enjeux contemporains tels que la mondialisation, les conflits interculturels et les changements environnementaux, ainsi que pour concevoir des solutions pour répondre à ces défis.\n\n\n=== La sociologie ===\n\nLa sociologie est une discipline des sciences sociales qui étudie la structure, le fonctionnement et l'évolution de la société et des interactions sociales entre les individus, les groupes et les institutions. Elle s'intéresse à la façon dont les individus et les groupes s'adaptent aux changements et transformations de la société, à la manière dont les facteurs sociaux influencent les attitudes, les valeurs et les croyances des individus. La sociologie se préoccupe notamment des questions liées à la diversité culturelle, aux inégalités sociales et à la justice sociale, à la transformation des structures et des rapports sociaux, aux mouvements sociaux et aux conflits collectifs, et à la façon dont les normes et les valeurs sociales influencent les comportements individuels et collectifs. La sociologie utilise des méthodes qualitatives et quantitatives pour étudier les phénomènes sociaux. Les sociologues analysent les données recueillies sur la base d'enquêtes, d'observations, de sondages, d'entrevues et d'autres techniques de collecte de données. La sociologie se subdivise en plusieurs branches : la macrosociologie et la microsociologie, la sociologie de la famille, la sociologie de l'éducation, la sociologie politique, la sociologie du droit, la sociologie de la santé, la sociologie de la connaissance, la sociologie du travail, la sociologie économique, la sociologie des religions, la sociologie des médias, etc. Elle fournit des connaissances et des outils pour analyser et résoudre les problèmes sociaux tels que la pauvreté, la discrimination, l'injustice et la violence. En ce sens, elle constitue un socle de connaissances utiles pour l'élaboration des politiques publiques.\n\n\n=== La démographie ===\n\nLa démographie est une discipline des sciences sociales qui étudie la structure, la dynamique et les caractéristiques des populations humaines. Elle s'intéresse aux différents aspects de la population, tels que la taille, la composition, la croissance, la distribution géographique, les mouvements migratoires, la fécondité, la mortalité, l'âge, le sexe, l'éducation, l'emploi et les conditions socio-économiques. La démographie utilise des méthodes quantitatives pour analyser les données statistiques et tracer des tendances et des projections démographiques à l'échelle locale, nationale et mondiale. Elle fournit des informations cruciales pour comprendre les changements de population, les défis démographiques, les politiques publiques, la planification économique et sociale, et les enjeux de développement durable.\n\n\n=== L'économie ===\n\nLa science économique, ou économie, est une discipline des sciences sociales qui étudie la manière dont les ménages, les entreprises, les gouvernements et les autres organisations prennent des décisions concernant l'allocation des ressources rares au sein de la société. Elle se concentre sur la production, la répartition et la consommation de biens et de services, ainsi que sur les mécanismes qui régissent les échanges économiques. La science économique utilise des outils analytiques pour étudier les choix économiques et les comportements humains, en utilisant des modèles mathématiques, statistiques et théoriques pour comprendre les forces économiques qui influencent les marchés et les comportements des acteurs économiques. Elle se divise en plusieurs sous-disciplines : la microéconomie, qui étudie les décisions prises par des agents économiques individuels ; la macroéconomie, qui se concentre sur les grands agrégats économiques tels que le PIB, l'inflation et le chômage ; l'économétrie, qui se concentre sur les formules et les lois mathématico-économiques ; et des disciplines économiques plus spécialisées (économie politique, économie publique, économie du bien-être, économie de l'éducation, économie d'entreprise, économie de la culture, économie du travail, économie de l'environnement, économie de la santé, économie du développement, économie monétaire, économie internationale, la cliométrie, ou encore économie du droit). Il existe un grand nombre d'écoles de pensée en économie, les trois plus connues sont : L'école classique ; L'école néoclassique ; L'école keynésienne. Les économistes utilisent des données empiriques pour tester des hypothèses économiques, afin de mieux comprendre les mécanismes économiques et d'élaborer des politiques publiques efficaces pour répondre aux défis économiques contemporains. La science économique est souvent utilisée pour informer les politiques gouvernementales, les décisions d'entreprise et les décisions d'investissement.\n\n\n=== La science politique ===\n\nLa science politique, ou politologie, est une discipline des sciences sociales qui étudie les systèmes politiques, les institutions, les processus et les comportements politiques. Elle analyse les relations de pouvoir, les décisions politiques, les processus électoraux, la participation politique, les mouvements sociaux et les politiques publiques. La science politique utilise des méthodes qualitatives et quantitatives pour étudier les phénomènes politiques, telles que les enquêtes, les sondages, les observations et les analyses statistiques. Elle examine également les contextes historiques, économiques, sociaux et culturels qui influencent les comportements et les pratiques politiques. Elle se divise en plusieurs branches : la théorie politique, la sociologie politique, la politique comparée, la politique publique, l'économie politique, les relations internationales et la géopolitique, etc. La science politique est importante car elle fournit des connaissances et des outils pour comprendre et analyser les systèmes politiques, les conflits politiques et les enjeux de la gouvernance. Elle permet également d'évaluer les politiques et les programmes gouvernementaux, ainsi que les effets des décisions politiques sur la société et les individus.\n\n\n=== La linguistique ===\n\nLa linguistique, ou science du langage, est une discipline des sciences sociales qui étudie le langage humain sous toutes ses formes et dans tous ses aspects. Elle cherche à comprendre comment les langues sont construites, comment elles fonctionnent, comment elles sont utilisées dans la communication, et comment elles évoluent au fil du temps et dans différents contextes culturels. Les linguistes s'intéressent aux systèmes sonores, grammaticaux, sémantiques et pragmatiques des langues, ainsi qu'à leur acquisition, leur traitement par le cerveau, leur variation et leur diversité géographique et sociale. La linguistique est descriptive et non prescriptive : à la différence de la grammaire, elle n'a pas vocation à dire ce qui est jugé correct linguistiquement, mais se contente de décrire la langue telle qu'elle est. La linguistique se divise en plusieurs domaines: la phonétique, qui étudie les sons ou phones produits par l'appareil phonatoire humain ; la phonologie, qui étudie les sons ou phonèmes d'une langue donnée ; la morphologie, qui étudie les types et de la forme des lemmes ; la syntaxe, qui étudie la combinaison des monèmes pour former des énoncés et des phrases ; la sémantique, qui étudie le sens des lemmes, des phrases et des énoncés ; la stylistique, qui étudie le style d'un énoncé littéraire ou non ; la pragmatique, qui étudie l'utilisation (littérale, figurée ou autre) des énoncés dans les actes d'énonciation ; la cohérence, qui étudie les facteurs de cohérence dans le traitement du langage naturel. \nLa linguistique trouve des applications dans la traduction, l'interprétation et la compréhension des langues étrangères, dans la réalisation et l'analyse techniques de communication, dans l'étude des cultures étrangères, ou encore dans l'étude du cerveau, de la perception et de la cognition.\n\n\n=== La science juridique ===\n\nLa science juridique, ou science du droit, est une discipline des sciences sociales qui étudie le droit sous un angle théorique et systématique. Elle a pour objet d'analyser les règles et les principes juridiques, leur application, leur interprétation, leur évolution, ainsi que les institutions et les normes qui les produisent et les encadrent. La science juridique se distingue ainsi de la pratique du droit, qui consiste à appliquer les règles et les principes juridiques à des situations concrètes. Les méthodes utilisées en science juridique sont variées, allant de l'analyse de textes juridiques et de la jurisprudence, à la comparaison de systèmes juridiques, en passant par l'étude des doctrines et des théories juridiques. La science juridique englobe plusieurs branches du droit, telles que le droit constitutionnel, le droit civil, le droit pénal, le droit international, le droit administratif, le droit des affaires, etc. Elle est donc une discipline transversale, qui s'intéresse à l'ensemble des règles et des principes de nature juridique régissant la vie en société.\n\n\n=== La psychologie ===\n\nLa psychologie est une discipline des sciences sociales qui étudie le comportement humain et les processus mentaux. Elle vise à comprendre comment les gens pensent, ressentent, agissent et interagissent les uns avec les autres et avec leur environnement. Elle utilise des méthodes scientifiques pour étudier les différentes dimensions de l'expérience humaine, telles que la perception, l'apprentissage, la cognition, l'émotion, la personnalité, les relations interpersonnelles et les comportements anormaux. La psychologie utilise une variété de méthodes scientifiques pour étudier le comportement humain et les processus mentaux : des observations, des expérimentations, des statistiques, des enquêtes, des tests psychologiques, la neuroimagerie, etc. Elle comporte de nombreux champs : la psychologie générale, la psychologie clinique, la psychologie sociale, la psychologie cognitive, la psychologie de l'éducation, la psychologie de l'apprentissage, la psychologie du développement, la psychologie criminelle, la neuropsychologie, etc. La psychologie peut être utilisée pour aider les gens à mieux comprendre leur propre comportement et leurs pensées, ainsi que pour améliorer leur qualité de vie. La psychologie trouve des applications dans les domaines de la santé, de l'enseignement, de la gestion, des médias, du conseil, etc.\n\n\n=== La logique ===\n\nLa logique est une discipline des sciences formelles qui étudie la façon dont les idées et les raisonnements sont exprimés et évalués. Elle se concentre sur les principes formels de raisonnement et sur la manière dont les arguments peuvent être formulés de manière à être rigoureusement valides et cohérents. La logique est utilisée pour analyser les arguments et les propositions, afin de déterminer leur validité et leur vérité. Née durant l'Antiquité comme l'une des disciplines majeures de la philosophie, elle est désormais traitée avec une approche plus mathématique et informatique, et trouve des applications dans toutes les autres disciplines scientifiques (physique, chimie, économie, linguistique, droit, etc.). La logique peut être divisée en plusieurs domaines, comme la logique propositionnelle, la logique des prédicats, la logique modale, la logique temporelle, la logique floue, la logique mathématique, la logique philosophique, la logique syllogistique.\n\n\n=== Les mathématiques ===\n\nLes mathématiques sont une discipline des sciences formelles consistant en un ensemble de connaissances abstraites résultant de raisonnements logiques appliqués à des objets divers tels que les ensembles mathématiques, les nombres, les formes, les structures, les transformations, etc. Elles portent aussi sur les relations et opérations mathématiques qui existent entre ces objets. Les mathématiques se distinguent des autres sciences par un rapport particulier au réel car l'observation et l'expérience ne s'y portent pas sur des objets physiques ; les mathématiques ne sont pas une science empirique. Elles sont de nature entièrement intellectuelle, fondées sur des axiomes déclarés vrais ou sur des postulats provisoirement admis. Ces axiomes en constituent les fondements et ne dépendent donc d'aucune autre proposition. Les mathématiques, du fait qu'elles ne sont pas une science empirique, ne font pas appelle à l'observation ou l'expérimentation d'objets réels. Elles reposent sur le seul raisonnement logico-déductif à partir des axiomes et postulats à la base des objets abstraits étudiés.\nLes mathématiques regroupent plusieurs disciplines très diverses : l'algèbre, l'analyse, l'arithmétique, la géométrie, les probabilités, la statistique, la topologie, la théorie des nombres, la théorie des ensembles, ou encore la logique mathématique. Les mathématiques connaissent de très nombreuses applications. Les mathématiques appliquées sont utilisées dans la plupart des autres disciplines scientifiques : la physique, la chimie, l'économie, l'informatique, la biologie, la psychologie, tout particulièrement. Elles sont aussi utilisées dans des domaines techniques, afin de résoudre des problèmes concrets : la finance, la cryptographie, l'ingénierie, notamment.\n\n\n=== L'informatique ===\n\nLa science informatique, ou plus simplement informatique, est une discipline des sciences formelles qui s'intéresse à l'étude mathématique de la computation. Elle vise à comprendre les fondements logiques, les limites et les possibilités de l'algorithmique et des systèmes informatiques. La science informatique aborde des sujets tels que la complexité algorithmique, la théorie de la calculabilité, la théorie des langages formels et des automates, la théorie de la complexité descriptive, la théorie des graphes, la théorie de l'information, etc. L'informatique connaît, à l'image des mathématiques, d'innombrables applications, qui constituent l'informatique appliquée. Celles-ci permettent de perfectionner l'ingénierie informatique, par l'optimisation des algorithmes, le renforcement de la sécurité informatique, la conception de langages de programmation, la vérification de logiciels, la conception de réseaux et de protocoles, ou encore la cryptographie. L'informatique et ses applications trouvent des utilités variées dans les domaines des affaires et de la finance, de la santé, des médias, de l'éducation, des loisirs, des télécommunications, du télétravail. Les innovations en informatique ont également permis le développement de nouvelles industries, comme la technologie mobile, la réalité virtuelle, le commerce en ligne, l'intelligence artificielle et la blockchain.\n\n\n=== La chimie ===\n\nLa chimie est une discipline des sciences naturelles qui étudie la matière, ses propriétés, sa structure et ses transformations. La chimie s'intéresse aux atomes, aux molécules, aux ions et aux autres entités constitutives de la matière, ainsi qu'aux interactions entre ces entités. Elle examine également les réactions chimiques, qui impliquent la transformation de la matière d'une forme à une autre. La chimie est une science empirique qui peut être divisée en plusieurs sous-disciplines : la chimie analytique, qui intègre notamment l'astrochimie, la chimie des solutions, la chimie environnementale, la chimie numérique, la chimie supramoléculaire, la chimie théorique, la chimie verte ; la chimie minérale ou inorganique, qui inclut notamment la chimie bioinorganique et la sciences matériaux ; la chimie nucléaire ; la chimie organique, qui inclut notamment la biochimie, la chimie bioorganique, la chimie macromoléculaire et des polymères, la chimie organique physique, la chimie organométallique ; la chimie physique, qui regroupe notamment la chimie quantique, la chimie cinétique, l'électrochimie, la femtochimie, la géochimie, la physico-chimie, la photochimie, la science des surfaces, la spectroscopie, la stéréochimie et la thermochimie. La chimie a des applications dans de nombreux domaines, tels que la médecine et la pharmacologie (mise au point de médicaments), l'industrie et l'ingénierie (découverte de nouveaux matériaux ou de nouvelles propriétés chimiques), l'agriculture (meilleure compréhension de la fertilité), la protection de l'environnement (découverte de technologies plus durables, meilleure compréhension des problèmes environnementaux tels que la pollution).\n\n\n=== La physique ===\n\nLa physique est une discipline des sciences naturelles qui étudie la nature fondamentale de l'Univers, ses composants, ses lois et ses interactions. La physique cherche à comprendre les phénomènes naturels en utilisant des concepts mathématiques et en développant des modèles pour décrire les comportements des objets et des systèmes observés ou théorisés. La physique développe pour ce faire des représentations du monde expérimentalement vérifiables dans un domaine de définition donné. La modélisation des systèmes physiques peut inclure ou non les processus chimiques et biologiques. La physique est une science empirique, elle repose sur des observations et expérimentations et cherche à modéliser leurs résultats. Ce travail de modélisation emprunte de nombreuses techniques aux mathématiques, surtout en physique théorique. La physique est une discipline vaste qui peut se diviser en plusieurs sous-disciplines : l'acoustique, la biophysique, la cryogénie, l'électromagnétisme, la géophysique, la mécanique (mécanique classique, mécanique des fluides, mécanique des milieux continus, mécanique quantique, mécanique statique), l'optique, la physico-chimie, la physique atmosphérique, la physique de la matière condensée, la physique des particules, la physique des plasmas, la physique des polymères, la physique nucléaire, la physique du solide, ou encore la physique théorique. La physique est une discipline fondamentale qui a de nombreuses applications et utilités pratiques dans les domaines des technologies (ordinateurs, télécommunications, navigation, capteurs, dispositifs de stockage de données, par exemple), de l'ingénierie (confection des avions, voitures, ponts, bâtiments, équipements électromécaniques, entre autres), de l'énergie (énergie nucléaire, énergie solaire, énergie éolienne, énergie hydraulique, énergie thermique, etc.) ou encore de la médecine (par exemple les techniques d'imagerie médicale).\n\n\n=== Les biosciences ===\n\nLes biosciences, ou sciences de la vie, ou biologie au sens large, sont une discipline des sciences naturelles, qui étudie les êtres vivants et les processus biologiques qui les animent. Elle s'intéresse à tous les niveaux d'organisation du vivant, depuis les molécules et les cellules jusqu'aux organismes, aux populations et aux écosystèmes. Elles cherche à appréhender les mécanismes de la vie, les interactions entre les êtres vivants et leur environnement, et les processus évolutifs qui ont conduit à la diversité des formes de vie sur Terre. La biologie utilise des méthodes d'observation, d'expérimentation et d'analyse. Elle regroupe de nombreuses disciplines qui s'intéressent au vivant selon plusieurs échelles : moléculaire (la chimie organique, la biochimie, la biologie moléculaire), microscopique (la biologie cellulaire, la cytologie, la microbiologie, l'histologie, la physiologie), macroscopique (la biologie des organismes, l'anatomie, l'éthologie), populationnelle (la biologie des populations, la génétique des populations), spécifique (la taxinomie, la phylogéographie) ou supra-spécifique (la systématique, l'écologie, la phylogénie). Les sciences de la vie ont des applications dans l'environnement, l'agriculture, la médecine et l'industrie et de l'alimentation.\n\n\n=== Les géosciences ===\n\nLes géosciences, ou sciences de la Terre, sont une discipline des sciences naturelles étudiant la Terre, ses strates (lithosphère, hydrosphère, atmosphère et biosphère), ses composantes (roches, minéraux, eaux, sols, air), ses processus, et ses interactions avec l'environnement et le vivant. Les géosciences entretiennent des relations privilégiées avec les biosciences et la géographie physique, mais aussi avec les sciences de la matière (physique, chimie). Reposant sur des techniques d'observation, d'analyse et de modélisation pour étudier la Terre, les géosciences sont nombreuses : la géologie évidemment, mais également la géomorphologie, la géophysique, l'aéronomie, la climatologie, la météorologie, la glaciologie, l'hydrogéologie, l'hydrologie, la minéralogie, l'océanographie, la paléogéographie, la paléontologie, la pétrologie, la limnologie, la sismologie, la pédologie, la tectonique, la topographie ou encore la volcanologie. Les sciences de la Terre trouvent de nombreuses application dans la gestion des risques naturels (séismes et tsunamis, éruptions volcaniques, inondations, tempêtes, sécheresses), la protection de l'environnement, l'industrie et l'agriculture, la construction et l'aménagement du territoire.\n\n\n=== Les sciences de l'Univers ===\n\nLes sciences de l'Univers, ou sciences astronomiques, ou encore sciences spatiales, sont une discipline des sciences naturelles qui étudient tout ce qui, dans l'Univers, est extérieur à la Terre. Elles s'intéressent aux propriétés, à la composition, à la structure, à l'origine, à l'évolution et au comportement des objets dans l'Univers : les étoiles, les galaxies, les planètes, les comètes, les astéroïdes, les trous noirs, les rayons cosmiques, la matière noire, etc. Les scientifiques de l'Univers utilisent une variété de méthodes et d'outils, tels que les télescopes, les satellites, les sondes spatiales, les ordinateurs et les modèles théoriques pour étudier ces objets et comprendre leur fonctionnement. Les sciences de l'Univers comprennent plusieurs disciplines, notamment l'astronomie (l'astrobiologie, l'astrochimie, l'astrodynamique, l'astrométrie, la planétologie), l'astrophysique (la physique stellaire, l'astronomie galactique, l'astronomie extragalactique, la mécanique spatiale), la cosmologie, l'exobiologie, la géologie planétaire, la météorologie spatiale. Les sciences de l'Univers trouvent de nombreuses applications dans les domaines de la navigation et de la cartographie, des télécommunications, de la médecine spatiale, de l'aéronautique, de la sécurité nationale, de la conquête spatiale.\n\n\n== Classification des disciplines scientifiques ==\n\nLa classification des sciences est un processus historique, social et conventionnel, produisant des catégories multiples et évolutives, encadrées institutionnellement,,. Dans la mesure où il existe une multitude de disciplines réunies sous la catégorie de science, il peut s'avérer être utile d'établir des classifications. Une classification des sciences catégorise les disciplines considérées comme scientifiques sur la base de principes et critères divers : par objet, méthode, types de problèmes, de leur soubassement paradigmatique, ...,. Ainsi, il existe une diversité de classification des sciences.\nMais il n'y a pas une classification des sciences qui soit vraiment satisfaisante dans la mesure où notamment : il existe des cas limites ; les sciences évoluent sans tenir compte des cadres classificatoire ; il existe des enjeux à affirmer qu'une discipline est une science ou non. Ainsi, toutes les classifications ne sont pas toujours justifiables : les critères ou les règles établis pour organiser ou catégoriser des éléments ne sont pas toujours logiques ou appropriés, même lorsqu'on les évalue selon leurs propres normes.\nIl existe différentes types épistémologiques de classifications (non exhaustif) : \n\nsciences formelles et sciences empiriques ;\nsciences de la nature et sciences de l'homme et de la société ;\nsciences dures et sciences molles ;\nsciences expérimentales et sciences de l'observation ;\nsciences empiriques mathématisées et sciences empiriques non mathématisées ;\nsciences explicatives et sciences compréhensive ;\nsciences empiriques prédictives et sciences empiriques non prédictives ;\nsciences et métasciences.\nUn regroupement disciplinaire peut être plus ou moins englobant, et est constitué de sous-champs, eux-mêmes constitués de disciplines, elles-mêmes constituées de sous-disciplines. Par exemple, le philosophe Michel J. F. Dubois et l'épistémologue Nicolas Brault, propose une première grande catégorie métascience qui comprend trois sous-champ disciplinaire, à savoir la mathématique, l’épistémologie et l’éthique et une seconde grande catégorie qui comprend quatre sous-champ disciplinaire, à savoir les sciences de la matière (comme la physique), les sciences de la vie (comme la biologie), les sciences humaines en extériorité (comme la sociologie), et les sciences humaines en intériorité (comme la psychologie). Rappelons qu'une classification n'est jamais vraiment satisfaisante : ici par exemple, la psychologie est comprise dans les « sciences humaines en intériorité », alors que la psychologie comprend la psychologie animale (aussi appelée psychologie comparée) qui s'intéresse aux différentes espèces animales, ou encore la psychopharmacologie par exemple. Autre exemple, Auguste Comte est connu pour avoir proposé une classification comprenant cinq sciences fondamentales : « astronomie, physique, chimie, biologie, physique sociale (pour laquelle Comte inventera, après 1841, le nom de sociologie) » ordonnées selon la logique des « plus éloignées de l’humanité » vers celles qui sont « les plus directement intéressantes pour l’homme ».\n\n\n=== Classification selon leur objet d'étude ===\n\nEn prenant comme critère leur objet d'étude, il est possible de distinguer les sciences formelles des sciences naturelles et des sciences sociales. Tandis que les premières décrivent des mondes abstraits reposant sur des axiomes arbitrairement définis, les sciences naturelles et sociales s'attaquent à décrire le monde réel par la réalisation de modèles et théories selon une approche empirique.\n\n\n==== Les sciences formelles ====\n\nEn sciences formelles, ou sciences logico-formelles, les « mondes » observés sont fabriqués (abstraits) et les règles qui les régissent sont choisies comme point de départ « admis » (appelés axiomes). Les sciences formelles se caractérisent par leur rigueur et leur abstraction. Elles sont fondées sur des déductions logiques à partir de principes de base et de règles formelles. Les sciences formelles ne cherchent donc pas, à la différences des sciences naturelles et sociales, qui sont des sciences empiriques, à décrire le monde réel en reposant sur l'observation et l'expérimentation : elles ont une approche purement théorique et conceptuelle, indépendante de l'expérience. Cependant les « mondes » fabriqués en sciences formelles peuvent connaître de nombreuses applications afin de bâtir des théories et modèles au service des sciences naturelles et sociales. Les mathématiques sont la discipline formelle la plus connue. Elles permettent de formaliser et de modéliser des phénomènes abstraits, tels que les relations géométriques, les structures algébriques, les fonctions et les probabilités. Les mathématiques sont utilisées dans de nombreuses disciplines scientifiques pour modéliser des phénomènes complexes et pour établir des théories scientifiques. La logique est une autre discipline formelle importante. Elle étudie les principes du raisonnement valide et les règles pour manipuler les symboles et les propositions. La logique est utilisée dans de nombreuses applications, telles que l'informatique, la philosophie, les mathématiques et la linguistique.\n\n\n==== Les sciences naturelles ====\n\nLes sciences naturelles, ou sciences de la nature, sont un ensemble de disciplines qui cherchent à décrire le monde naturel. Contrairement aux sciences formelles et à l'image des sciences sociales, ce sont des sciences empiriques, qui procèdent par l'élaboration de théories, de lois ou de modèles qui reposent sur des expérimentations et observations du réel. Les sciences naturelles décrivent le monde indépendamment des créations de l'Homme, ce qui les oppose quant à leur objet aux sciences sociales, qui étudient ce qui relève du « choix » des humains (les systèmes politiques, économiques, linguistiques, etc.) et non pas de ce qui est « imposé » par le monde naturel. Les sciences naturelles sont classiquement rassemblées en trois grands ensembles : les sciences de la vie et de l'environnement, ou biosciences (biologie, écologie, anthropologie) ; les sciences de la matière, ou sciences physiques lato sensu (physique, chimie) ; les sciences de la Terre et de l'Univers (géosciences, sciences de l'Univers, géographie physique). Elles connaissent de très nombreuses applications dans les domaines de l'éducation, de la santé, de l'ingénierie, de l'agriculture, de l'industrie, de l'aéronautique, notamment.\n\n\n==== Les sciences sociales ====\n\nLes sciences sociales, ou sciences humaines et sociales, sont des sciences empiriques qui cherchent à décrire par la réalisation de théories ou de modèles les comportements humains, les structures sociales et les processus culturels. Tandis que les sciences naturelles s'intéressent à ce qui existe indépendamment des Hommes, ce qui est inhérent à la Nature, les sciences sociales s'intéressent à l'Homme et à la Société : les comportements humains, la matière dont les êtres humains interagissent au sein de la société, les systèmes politiques, sociaux, juridiques, linguistiques, économiques, qu'ils établissent. Elles portent donc sur ce qui relèvent du « choix », plus ou moins conscient ou individuel, des humains. Au nombre des sciences sociales, on trouve notamment la sociologie, la psychologie, l'anthropologie, l'économie, le droit, la politologie, la géographie, la linguistique, la démographie, l'histoire, la criminologie, et leurs très nombreuses applications concrètes dans les domaines de l'éducation, de la gestion, de l'ingénierie, de la santé, de la politique, du commerce et de l'industrie, notamment.\n\n\n=== Classification selon leur méthode ===\n\nEn prenant comme critère la méthode de recherche employée, il est possible de distinguer les sciences formelles et les sciences empiriques (sciences naturelles et sciences sociales). Tandis que les premières, dans la mesure où elles manipulent des constructions abstraites, ne partent pas de l'expérience pour créer de la connaissance, les secondes ne peuvent se passer de l'observation et de l'expérimentation pour valider leurs théories.\n\n\n==== Les sciences formelles ====\nLes sciences formelles, telles que les mathématiques et la logique, sont basées sur des systèmes de règles et de principes abstraits, qui sont manipulés de manière logique et déductive pour arriver à des conclusions. Les sciences formelles n'ont pas besoin d'observations empiriques pour valider leurs théories et ne dépendent pas de la réalité matérielle ou physique, elles utilisent exclusivement des raisonnements logiques et déductifs pour atteindre des conclusions. Par exemple, les mathématiques étudient les propriétés des nombres, des formes et des structures, mais sans avoir besoin de se référer à des objets physiques qu'il conviendrait d'observer.\n\n\n==== Les sciences empiriques ====\nLes sciences empiriques, telles que la biologie, la physique, l'économie, ou encore la psychologie, se basent sur l'observation et l'expérimentation pour tester leurs hypothèses et construire des théories. Elles cherchent à comprendre les phénomènes naturels, humains ou sociaux en utilisant des méthodes scientifiques rigoureuses, telles que l'observation, la mesure, l'expérimentation, la modélisation et la simulation. Le raisonnement empirique se déroule en différentes étapes qui se répètent de manière cyclique jusqu'à aboutir à une théorie, un modèle, qui décrit le plus parfaitement possible l'objet naturel ou social étudié :\n\nL'observation du phénomène, qui permet au scientifique de collecter des données et informations pour préparer les hypothèses qu'il va émettre ;\nL'induction, où le scientifique émet des hypothèses le permettant d'expliquer ce qu'il a observé ;\nLa déduction, ou le scientifique réfléchit aux conséquences des hypothèses émises ;\nLa phase de test, où le scientifique va vérifier ses hypothèses ;\nL'évaluation, où il confirme ou informe le modèle qu'il a créé et les conséquences qu'il a induit.\nLes théories qui naissent du raisonnement empirique passent ensuite le feu de l'évaluation par les pairs (peer review). Il s'agit d'une étape indispensable de la recherche empirique qui désigne l'activité collective des chercheurs qui jugent de façon critique les travaux d'autres chercheurs (leurs « pairs ») dans le but de ne retenir, au fil du temps, que les théories les plus solides.\n\n\n=== Classification selon leur finalité ===\n\nSelon leur finalité, on peut distinguer les sciences fondamentales, qui sont l'ensemble des connaissances rationnelles sur le fonctionnement et l'histoire du monde indépendamment des considérations pratiques pouvant en résulter, et les sciences appliquées, qui sont l'ensemble des connaissances rationnelles permettant la réalisation d'objectifs pratiques.\n\n\n==== Les sciences fondamentales ====\n\nLes sciences fondamentales (ou sciences pures) sont des disciplines scientifiques qui étudient les lois et les principes fondamentaux de notre monde voire de mondes abstraits, sans nécessairement chercher à résoudre des problèmes pratiques ou à créer des applications concrètes. Les sciences pures vont permettre d'acquérir de nouvelles connaissances sur les fondements des phénomènes et des faits observables, par la réalisation de théories générales, sans envisager une application ou une utilisation particulière. Les sciences fondamentales sont importantes car elles fournissent une base solide pour les sciences appliquées, qui cherchent à appliquer ces connaissances pour résoudre des problèmes pratiques dans des domaines tels que la médecine, l'ingénierie, la gestion, l'éducation, la technologie. Ainsi, même si les sciences fondamentales n'ont pas pour objet premier de fournir des solutions aux problèmes concrets, elles n'en demeurent pas moins essentielles à cette fin, car elles permettent le e développement de nouvelles techniques ou technologies, permettent de concevoir et construire des dispositifs et des systèmes complexes. Cette interdépendance entre les sciences pures et les sciences appliquées se retrouve aussi au niveau de la recherche. En effet, si le corpus de connaissances fondamentales s'amplifie principalement grâce à la recherche fondamentale, il profite aussi régulièrement de la recherche appliquée par sérendipité. Et inversement, il arrive régulièrement que la recherche appliquée permettre de dégager des connaissances fondamentales.\n\n\n==== Les sciences appliquées ====\n\nLes sciences appliquées sont l'ensemble des connaissances rationnelles qui permettent de réaliser un objectif pratique. Elles sont l'application des savoirs apportés par la science fondamentale afin d'atteinte un but précis (soigner des personnes, gérer au mieux une entreprise, confectionner une machine, par exemple). Cette volonté pratique assimile ainsi souvent les sciences appliquées à la technique. La plupart (pour ne pas dire la totalité) des disciplines scientifiques trouvent des applications. Ainsi, on peut par exemple parler de mathématiques appliquées, d'économie appliquée, de physique appliquée, d'informatique appliquée. Il existe également des disciplines scientifiques qui sont fondamentalement appliquées, qui sont entièrement dirigées vers la réalisation d'un objectif pratique, et qui reposent sur d'autres disciplines plus théoriques. Par exemple, la médecine est entièrement tournée vers la réalisation d'un objectif pratique, soigner les personnes, et puise pour ce faire dans les connaissances apportées par la biologie et la chimie, entre autres. De la même manière, les sciences de gestion appliquent les connaissances apportées par l'économie, la sociologie, ou encore les mathématiques, afin d'éclairer l’action conduite de façon collective par des groupes humains organisés. On pourrait encore citer les sciences de l'ingénieur, les sciences de l'éducation, les sciences de l'information, les sciences du sport, l'électronique, l'agronomie, l'aéronautique, l'architecture, ou encore la robotique. Des disciplines elles aussi fondamentalement tournées vers la réalisation d'objectifs pratiques.\n\n\n== Notes et références ==\n\n\n=== Notes ===\n\n\n=== Citations ===\n\n\n=== Références ===\n\n\n=== Bibliographie ===\n : document utilisé comme source pour la rédaction de cet article.\n\n\n==== Manuels d'épistémologie ====\n[Barreau 2021] Hervé Barreau, L'épistémologie, Paris, PUF, coll. « Que sais-je ? », 2021, 8e éd., 127 p. (ISBN 978-2-13-062607-7, lire en ligne ) – accès gratuit par la bibliothèque Wikipédia\n[Bitbol et Gayon 2015] Michel Bitbol et Jean Gayon, L’épistémologie française, 1830-1970, Editions Matériologiques, 2015 (ISBN 978-2-919694-91-4, lire en ligne)\n[Dhombres et Kremer-Marietti 2006] Jean Dhombres et Angèle Kremer-Marietti, L'épistémologie : état des lieux et positions, Paris, Éditions Ellipses, coll. « Philo », 2006, 111 p. (ISBN 978-2-7298-2837-0, OCLC 300408225) - Intro, sommaire\n[Dubois et Brault 2021] Michel J.F. Dubois et Nicolas Brault, Manuel d’épistémologie pour l’ingénieur.e, Éditions Matériologiques, coll. « Essais », 2021 (ISBN 9782373612769, DOI 10.3917/edmat.duboi.2021.01, lire en ligne ).  – accès gratuit par la bibliothèque Wikipédia\n[Soler 2019] Léna Soler, Introduction à l'épistémologie, Paris, Ellipse, 2019, 3e éd., 334 p. (ISBN 9782340029385, présentation en ligne).  - Intro, sommaire\n[Macherey 1989] Pierre Macherey, « La seconde leçon du « Cours » : la classification des sciences », dans Comte : la philosophie et les sciences, Paris, Presses Universitaires de France, 1989, 73-120 p. (ISBN 9782130426219, lire en ligne ).  – accès gratuit par la bibliothèque Wikipédia\n\n\n==== Références concernant les disciplines scientifiques ====\n[Boutier, Passeron et Revel 2006] Jean Boutier (dir.), Jean-Claude Passeron (dir.) et Jacques Revel (dir.), Qu’est-ce qu’une discipline ?, Paris, Éditions de l’École des hautes études en sciences sociales, coll. « Enquête » (no 5), 2006, 260 p. (ISBN 978-2-7132-2103-3 et 978-2-7132-3111-7, DOI 10.4000/books.editionsehess.20041, lire en ligne ).\n[Fabiani 2006] Jean-Louis Fabiani (dir.), « À quoi sert la notion de discipline ? », dans Qu’est-ce qu’une discipline ?, Paris, Éditions de l’École des hautes études en sciences sociales, 2006 (ISBN 978-2-7132-2103-3, DOI 10.4000/books.editionsehess.20081, lire en ligne ), p. 11-34. \n[Lenclud 2006] Gérard Lenclud (dir.), « L’anthropologie et sa discipline éditeur=Éditions de l’École des hautes études en sciences sociales », dans Qu’est-ce qu’une discipline ?, Paris, 2006 (ISBN 978-2-7132-2103-3, DOI 10.4000/books.editionsehess.20091, lire en ligne ), p. 69-93. \nBailhache Patrice, 2005, « Sciences historiques et classification des sciences », Cahiers François Viète, I-9/10, p. 9‑32.\nBourdieu Pierre, 1975, « La spécificité du champ scientifique et les conditions sociales du progrès de la raison », Sociologie et sociétés, 7(1), p. 91‑118.\nBourion Christian, Antoine Alain, Bournois Frank, 2014, « La gestion des connaissances scientifiques par les classifications les Ranking et les listes. Assiste-t-on au développement de prophéties auto réalisatrices sources de processus magiques de rédemption au sein de la gestion des connaissances scientifiques ? », Revue internationale de psychosociologie et de gestion des comportements organisationnels, XX(49), p. 263‑286.\nBraverman Charles, 2015, « La classification scientifique chez Ampère : entre Bacon et les naturalistes », Revue philosophique de la France et de l’étranger, 140(3), p. 307‑324.\nDelattre Nicole, 2010, « Scientisme et guerre des sciences », Psychotropes, 16(3‑4), p. 77‑88.\nDubois Michel J. F., Brault Nicolas, 2021, « Chapitre 8. Proposition de classification des sciences », Essais, p. 165‑188.\nFabiani Jean-Louis, 2012, « Du chaos des disciplines à la fin de l’ordre disciplinaire ? », Pratiques. Linguistique, littérature, didactique, 153‑154, p. 129‑140.\nFabiani Jean-Louis, 2013, « Vers la fin du modèle disciplinaire ? », Hermès, La Revue, 67(3), p. 90‑94.\nHauchecorne Mathieu, 2007, « ABBOTT (Andrew), Department and Discipline. Chicago Sociology at One Hundred , Chicago, The University of Chicago Press, 1999, 249 pages, bibliographie, index. A BBOTT (Andrew), Chaos of Disciplines , Chicago, The University of Chicago Press, 2001,259 pages, bibliographie, index. »:, Politix, nº 79(3), p. 224‑230.\nHeilbron Johan, Bokobza Anaïs, 2015, « Transgresser les frontières en sciences humaines et sociales en France », Actes de la recherche en sciences sociales, 210(5), p. 108‑121.\nHeilbron Johan, Gingras Yves, 2015, « La résilience des disciplines », Actes de la recherche en sciences sociales, 210(5), p. 4‑9.\nHocquard Dominique, 2001, « La subjectivité dans l’objectivité scientifique des classifications », La lettre de l’enfance et de l’adolescence, 43(1), p. 19‑26.\nLouvel Séverine, 2015, « Ce que l’interdisciplinarité fait aux disciplines. Une enquête sur la nanomédecine en France et en Californie », Revue française de sociologie, 56(1), p. 75‑103.\nMonteil Lucas, Romerio Alice, 2017, « Des disciplines aux “studies”. Savoirs, trajectoires, politiques », Revue d’anthropologie des connaissances, 11, 3(3), p. 231‑244.\nPrud’homme Julien, Gingras Yves, 2015, « Les collaborations interdisciplinaires : raisons et obstacles », Actes de la recherche en sciences sociales, 210(5), p. 40‑49.\nValade Bernard, 2013, « Edmond Goblot : classification des sciences et système des savoirs », Hermès, La Revue, 66(2), p. 54‑57.\nVinck Dominique, 1999, « Les objets intermédiaires dans les réseaux de coopération scientifique: Contribution à la prise en compte des objets dans les dynamiques sociales », Revue française de sociologie, 40(2), p. 385‑414.\nAbbott Andrew, 2006, « Le chaos des disciplines », in Boutier Jean, Passeron Jean-Claude, Revel Jacques (dir.), Qu’est-ce qu’une discipline ?, Paris, Éditions de l’École des hautes études en sciences sociales, Enquête, p. 35‑67.\nMandosio Jean-Marc, 2017, « Tensions et transformations dans la classification des sciences et des arts au xve siècle », in Science et technique au Moyen Âge (XIIe – XVe siècles), Saint-Denis, Presses universitaires de Vincennes, Temps & Espaces, p. 65‑94.\nRobin Léon, 1967, « La classification des sciences chez Platon », in La pensée hellénique, Paris cedex 14, Presses Universitaires de France, Bibliothèque de philosophie contemporaine, p. 361‑367.\n\n\n==== Références concernant un champ disciplinaire ====\nÉlisabeth Demont, La psychologie : histoire, concepts, méthodes, expériences, Auxerre, Sciences humaines, coll. « La petite bibliothèque des sciences humaines », 2009, 256 p. (ISBN 978-2-912601-76-6)\n\n\n==== Ouvrages publiés de 1800 à 1900 ====\nCharma Antoine, 1859, Une nouvelle classification des sciences: résumé de quelques leçons professées à la Faculté des lettres de Caen, L. Hachette, 50 p.\nDelavaud Charles Édouard, 1876, Classification des sciences: aperçu général des sciences du monde matériel et de leur filiation, J.-B. *Baillière et fils, 78 p.\nGoblot Edmond, 1898, Essai sur la classification des sciences, F. Alcan, 306 p.\nMariétan Joseph, 1901, Problème de la classification des sciences d’Aristote à St. Thomas, Alcan, 224 p.\nNaville Adrien, 1888, De la classification des sciences: étude logique, H. Georg, 56 p.\nNaville Adrien, 1901, Nouvelle classification des sciences: étude philosophique, F. Alcan, 202 p.\nSpencer Herbert, 1872, Classification des sciences, Librairie Germer Baillière et Cie, 192 p.\n\n\n==== Auguste Comte (source primaire) ====\n« Auguste Comte. Œuvres. III-VIII Cours de philosophie positive. III 1re - 9e leçons. » Disponible sur: https://gallica.bnf.fr/ark:/12148/btv1b6000549n\n« Auguste Comte. Œuvres. III-VIII Cours de philosophie positive. IV 19e-34e leçons. » Disponible sur: https://gallica.bnf.fr/ark:/12148/btv1b60005509\n« Auguste Comte. Œuvres. III-VIII Cours de philosophie positive. V 35e-45e leçons. » Disponible sur: https://gallica.bnf.fr/ark:/12148/btv1b6000551q\n« Auguste Comte. Œuvres. III-VIII Cours de philosophie positive. VI 46e-51e leçons. » Disponible sur: https://gallica.bnf.fr/ark:/12148/btv1b60005524\n« Auguste Comte. Œuvres. III-VIII Cours de philosophie positive. VII 52e-55e leçons. » Disponible sur: https://gallica.bnf.fr/ark:/12148/btv1b6000553j\n\n\n== Voir aussi ==\n\n Portail des sciences"
        },
        {
            "pageid": 6181,
            "ns": 0,
            "title": "Épistémologie",
            "content": "L'épistémologie (du grec ancien ἐπιστήμη / epistếmê, « connaissance vraie, science » et λόγος / lógos / « discours ») est un terme polysémique et  recouvre des domaines d'études et des approches philosophiques différentes selon les contextes linguistiques et culturels :\n\nDans le monde anglo-saxon, epistemology désigne la théorie de la connaissance. L'epistemology anglophone a connu plusieurs courants et se focalise depuis la fin du positivisme logique sur les conditions et les critères de la connaissance ordinaire. On retrouve parfois le terme « épistémologie » en français dans ce sens.\nEn français, ce même terme d'« épistémologie » a plusieurs acceptions et peut désigner : (1) la philosophie des sciences elle-même ; (2) une tradition spécifiquement française mêlant philosophie des sciences, histoire des sciences et théorie de la connaissance et (3) le courant de l'épistémologie historique, qui étudie l'évolution des fondements et méthodes scientifiques à travers le temps.\nLes frontières entre épistémologie, philosophie des sciences, histoire et sociologie des sciences peuvent varier selon les traditions académiques et les auteurs. Ces disciplines, bien que distinctes, présentent des recouvrements et des emprunts interdisciplinaires.\nL'introduction en 1901 du mot « épistémologie » en français résulte d'un emprunt à l'anglais epistemology à l'occasion de la traduction de l' Essai sur les fondements de la géométrie de Bertrand Russell, le mot anglais ayant été lui-même formé pour traduire l'allemand Wissenschaftslehre, désignation par le philosophe postkantien Johann Gottlieb Fichte de sa propre philosophie comme Doctrine de la science.\nEn plus d'avoir adopté une approche historique, la tradition française a mis l'accent sur le lien entre la connaissance scientifique et la connaissance commune, cette dernière étant vue comme étant fondamentalement distincte de la première en même temps que sa base.  Selon Hervé Barreau, Kant a établi la base pour cette distinction en proposant une connaissance a priori de nature scientifique. Auguste Comte dit que la connaissance scientifique s'élabore au contraire à partir de la connaissance commune. Gaston Bachelard introduit la notion d'épistémologie régionale: non seulement il y a une coupure entre la connaissance commune et la science, mais il y a aussi des coupures entre les diverses sciences (de même que des coupures à l'intérieur de la connaissance commune).\n\n\n== Histoire du mot ==\nLe terme « épistémologie » vient du grec ancien ἐπιστήμη / epistếmê « connaissance, capacité à faire, art, habileté, compétence professionnelle, science » et λόγος / lógos « discours ». Le terme se traduit donc littéralement par discours sur la science.\nL'introduction du mot « épistémologie » en français est relativement récente (1901) : elle résulte de la traduction de l' Essai sur les fondements de la géométrie de Bertrand Russell. Il s'agit d'un « emprunt à l'anglais epistemology, formé pour traduire l'allemand Wissenschaftslehre […], avec le grec epistémé “science, connaissance”, dérivé de epistanaï “savoir”, proprement “se tenir au-dessus de” et -logy ». Son introduction en français vise à désigner « l'étude critique des sciences », afin de « déterminer leur valeur, leur origine et leur portée ».\n\n\n=== Introduction dans la philosophie anglophone ===\n\n\n==== Première occurrence du mot en 1847 ====\nLa première occurrence connue du néologisme epistemology date de 1847 : elle se trouve dans un article anonyme sur l'écrivain allemand Jean Paul, paru dans la revue « The English Review ». Le mot epistemology est alors forgé pour traduire celui de Wissenschaftslehre (Doctrine de la science), du titre de la philosophie de Fichte à laquelle il est fait allusion dans le roman Titan (de) de Jean Paul,, :\nLe lien ou l'influence entre cette première introduction anonyme et la suivante qui sera faite par James Frederick Ferrier, ne sont pas connus.\n\n\n==== Introduction d'Epistemology en remplacement de la Wissenschaftslehre de Fichte ====\nLe mot epistemology est ensuite proprement introduit dans la littérature philosophique anglophone par James Frederick Ferrier en 1854, qui l'a utilisé dans ses « Instituts de métaphysique », :\n\nDans cet ouvrage, Ferrier présente l’immatérialisme de George Berkeley en le reformulant dans le vocabulaire de l’idéalisme allemand. Ferrier introduit alors le terme pour transposer le terme Wissenschaftslehre de Fichte.\nSelon Dominique Lecourt, « epistemology est resté pendant plusieurs décennies un mot de diffusion très restreinte. Ironie de l’histoire, il s’est répandu sous la plume de penseurs qui ont fait profession de rejeter Friedrich Hegel (1770-1831) et la philosophie romantique allemande ».\n\n\n=== L'introduction du mot « épistémologie » en français ===\nC'est Bertrand Russell qui sera à l'origine de l'introduction du mot « épistémologie » en français. Russel emploie epistemology, dans son Essai sur les fondements de la géométrie en 1901, sous la définition d'analyse rigoureuse des discours scientifiques, pour examiner les modes de raisonnement qu'ils mettent en œuvre et décrire la structure formelle de leurs théories.\nC'est comme simple remplacement pour epistemology que épistémologie a été utilisé par Russell et Couturat dans les années 1890 dans leur correspondance. Russell écrivait ses lettres en français, car Couturat ne maîtrisait pas bien l'anglais. C'est ensuite dans la traduction de l’Essai sur les fondements de la géométrie de Russell qu'épistémologie apparaît officiellement pour la première fois en français en 1901,.\n\n\n== L'émergence de différentes épistémologies ==\nL’épistémologie n’a pas toujours eu une seule et même signification. À travers les siècles, le terme a évolué, couvrant à certaines époques l’étude de la connaissance en général, tandis qu’à d’autres, il se focalisait spécifiquement sur la science. Cette évolution reflète la diversité des approches philosophiques, où chaque courant a proposé sa propre conception de ce que l’épistémologie devait englober.\n\n\n=== De la Connaissance Commune à la Connaissance Scientifique ===\nLa distinction entre connaissance commune et connaissance scientifique n'était pas une préoccupation majeure avant le XVIIIe siècle et a gagné en importance avec les travaux de Kant sur les formes a priori de la sensibilité et de l'entendement,.\nDepuis lors, diverses approches théoriques ont tenté d'expliquer le passage de la connaissance commune à la connaissance scientifique,. Hervé Barreau évoque David Hume, mais retient surtout la psychologie du XIXe siècle comme seule capable d'expliquer ce passage avec « des résultats acceptables ». « Husserl qui est le fondateur du mouvement phénoménologique […] a dénoncé [le fondement idéaliste] de la connaissance scientifique par la psychologie [c'est-à-dire par la subjectivité de l'apprenant] ». \nAu cours du XXe siècle, on assiste ainsi des variations dans les contenus de l'épistémologie : en essayant de répondre à la question « qu'est-ce que la science ? », l'épistémologie se heurte en effet au « problème de l'unité scientifique et à celui des formes de la connaissance ». Autrement dit, la question pourrait se formuler ainsi : « y a-t-il rupture ou continuité entre la connaissance commune et la science ? ».\nAinsi à la traduction de l'œuvre de Russell est annexé un Lexique philosophique rédigé par Louis Couturat, qui à l'entrée Épistémologie donne la définition d'une « théorie de la connaissance appuyée sur l'étude critique des Sciences, ou d'un mot, la Critique telle que Kant l'a définie et fondée »,,. Selon Pierre Wagner, Couturat semblera pour le lecteur moderne mélanger théorie de la connaissance et philosophie des sciences.\n\n\n=== Divergences sémantiques au début du XXe siècle ===\nLe début du XXe siècle marque un tournant important dans le développement des épistémologies, avec l’apparition de divergences nettes entre les traditions philosophiques et linguistiques. Catherine Chevalley pointe ainsi une rupture significative entre l’Erkenntnistheorie allemande, centrée sur la connaissance en général, et l’epistemology anglo-saxonne, qui privilégie l’analyse logique du langage et des théories scientifiques. Ces deux traditions, bien que traitant de questions similaires, ont donné lieu à des trajectoires épistémologiques distinctes, chacune influencée par ses contextes culturels et intellectuels propres.\nEn France des philosophes comme Gaston Bachelard, Georges Canguilhem, Michel Foucault introduisent des théories qui s’appuient sur l’histoire des sciences ainsi que sur une « réflexion autour des notions de valeur et de pouvoir ». L'épistémologie a alors épousé un courant « historique » avec l’avènement de la méthode historico-critique comme méthode directrice. « Les scientifiques commencent à produire des travaux en histoire [des sciences] et en philosophie des sciences [= l'épistémologie] »,.\nEn Allemagne, à la même époque, Wilhelm Dilthey ou Max Weber distinguent au contraire les sciences de la nature des sciences de l'esprit, ce qui « les conduit à mobiliser également des ressources philosophiques, empruntées à la tradition herméneutique développée depuis Schleiermacher, qui trouve sa postérité chez Hans Georg Gadamer ».\nD'après Anheim, le philosophe français Bergson exerça une « influence souterraine sur la conception du temps  [dans le corpus] des Annales ». Et L'introduction à la philosophie de l'histoire de Raymond Aron, paru en 1938 « peut être considéré comme un écho français à la discussion épistémologique allemande sur le statut de la connaissance historique dans une perspective herméneutique ». D'autres exemples dans l'actualité de la réflexion historique sur le temps (François Hartog, Georges Didi-Huberman) « s'articulent à la tradition philosophique allemande, de Walter Benjamin à Reinhart Koselleck ».\n\nPour Catherine Chevalley, la différence linguistique ne signifie toutefois qu'« une différence de perspective » qui n'interdit pas la communication : depuis le premier tiers du XXe siècle. Étienne Anheim montre ainsi comment :\n« dans le domaine épistémologique, des traditions historiques et philosophiques se lient les unes aux autres pour produire des théories souvent concurrentes de la connaissance. Cet effort se déroule sur la toile de fond de l'essor d'une épistémologie des sciences fortement liées à la philosophie et à l'histoire des disciplines scientifiques. »\n\n\n=== Classification des sciences : tradition française et tradition allemande ===\n\nSelon Anastasios Brenner, « la tradition française a longtemps été dominée par l’idée de l’unité du savoir », idée dont les origines remonteraient sûrement à Descartes. Même si Ampère a proposé de distinguer « entre sciences cosmologiques et sciences noologiques », en quoi il préfigurait Dilthey, Auguste Comte s'y est opposé, de sorte que c'est la classification comtienne des sciences qui l'a emporté, parce qu'elle est apparue « bien supérieure à celle de son adversaire ». Dès lors, « l’influence profonde du positivisme en France au XIXe siècle a eu pour effet d’enraciner une conception unitaire de la science ».\nDans la tradition allemande, c'est au contraire « l’idée d’une dichotomie entre Naturwissenschaften et Geiteswissenschaften » qui domine. Et l’influence allemande va conduire à un « renversement de perspective en France », où l'idée d'une dichotomie s'installe au cours du XXe siècle. Brenner constate là un « chassé-croisé curieux », entre la montée en puissance du positivisme et « l’affirmation de l’unité des sciences en Autriche par le Cercle de Vienne ». \n\n\n== Définitions ==\nDans son cours de Culture scientifique, Jean-Claude Simard avertit son auditoire en ces termes : « lorsque l’on aborde l’épistémologie pour la première fois, il faut prendre acte des variations du terme » : chez les Anglo-Saxons, dit-il, le mot epistemology « évoque en général une branche spécialisée de la philosophie, la théorie de la connaissance », tandis qu'en France, « il fait plutôt référence à l’étude des théories scientifiques ».\nPour Léna Soler : « Les usages concrets du terme « épistémologie » sont en effet multiples et évolutifs. Car différentes manières de concevoir et de pratiquer l’épistémologie coexistent, souvent hétérogènes et parfois antagonistes. Aussi est-il impossible de donner une définition de l’épistémologie qui permette de saisir immédiatement ce dont il est question et de décider sans ambiguïté, en présence d’un discours donné, s’il appartient ou non à l’épistémologie ».\nCette vision est corroboré par Pierre Wagner, pour qui le terme français épistémologie est usité tantôt comme synonyme de « philosophie des sciences », tantôt pour désigner la philosophie des sciences « de style français », tantôt pour traduire epistemology.\n\n\n=== Épistémologie et philosophie des sciences ===\n\nSelon Hervé Barreau, l'épistémologie est l'étude des sciences et vient « remplacer l'expression antérieure de philosophie des sciences qu'avaient employée Auguste Comte et Augustin Cournot […] ». Barreau ajoute : « L'épistémologie se distingue surtout de la théorie de la connaissance, telle qu'elle était entendue par les philosophes des XVIIe et XVIIIe siècles, qui s'étaient préoccupés déjà d'élargir, au contact de la science moderne, les anciennes doctrines sur la connaissance humaine ».\nÀ l'entrée « épistémologie » du Vocabulaire technique et critique de la philosophie, une citation d'Émile Meyerson (dans Identité et réalité) reconnaît que cet ouvrage réfère, par la méthode, au domaine de la philosophie des sciences, ou épistémologie, « suivant un terme suffisamment approché et qui tend à devenir courant ». Le Dictionnaire historique de la langue française  précise effectivement que le terme « épistémologie » « est donné comme équivalent de philosophie des sciences par Meyerson (1907) »,.\nC'est également le sens retenu dans l'ouvrage Philosophies de la connaissance : « en français la « philosophie des sciences » a toujours constitué, depuis le début du XXe siècle, le cœur du domaine de recherche que l’on désigne sous le nom d’«épistémologie». »\n\n\n==== Variations ====\nCertains auteurs privilégient la définition de l'épistémologie comme l'étude critique des sciences,.\nDans le Vocabulaire technique et critique de la philosophie d'André Lalande, il est dit de l'épistémologie qu'elle désigne « la philosophie des sciences, mais avec un sens plus précis » Ce n'est ni l'étude des méthodes scientifiques, ni « une synthèse ou une anticipation conjecturale des lois scientifiques », c'est « plus précisément l'étude critique des principes, des hypothèses et des résultats de diverses sciences, destinée à déterminer leur origine logique (non psychologique), leur valeur et leur portée objective ». Pour cette raison, il faut « distinguer l'épistémologie de la théorie de la connaissance », même si elle est « l'introduction et l'auxiliaire indispensable » de cette dernière : elle étudie en effet « la connaissance en détail et a posteriori, dans la diversité des sciences et des objets plutôt que dans l'unité de l'esprit ».\n\n\n=== Dans le sens de théorie de la connaissance ===\n\nLe terme epistemology anglais a toujours eu le sens de theory of knowledge (la théorie de la connaissance), mais le sens du terme a changé au cours du 20e siècle. Par exemple, Rorty a interprété les travaux de Russell dans la philosophie analytique comme une philosophie des sciences. De même, avec Quine en 1969, naturalized epistemology a pour objectif de décrire la science par la science,,.\nL'épistémologie naturalisée de Quine, dans la ligné de Karl Popper, a encouragé les philosophes du Cercle de Vienne à abandonner la tentative de justifier la connaissance scientifique uniquement par l’observation,, marquant ainsi la fin du positivisme logique, qui se concentrait exclusivement sur la science. Pour Catherine Chevalley, cela fut la dernière étape avant la naissance de l'epistemology contemporaine anglaise avec son intérêt pour les problèmes de Gettier, une épistémologie qui se fait en analysant la connaissance ordinaire.\nEn français, on utilisera certaines fois épistémologie pour couvrir un sens étendu qui inclut le sens original orienté vers la connaissance scientifique (et les variations de ce sens en français) et le nouveau sens anglais orienté vers la connaissance ordinaire. On retrouve le sens étendu d'épistémologie dans le titre de champs de recherche récent de la théorie de la connaissance, comme par exemple : épistémologie sociale ou l'épistémologie féministe. Cependant, dans des références plus officielle comme dans le Dictionnaire de philosophie (2004) de Christian Godin, le sens étendu est celui de théorie de la connaissance et le terme épistémologie est réservé au sens original orienté vers la science.\n\n\n=== Une tradition philosophique spécifique française ===\n\nL'épistémologie française ne se présente pas comme une école de pensée unifiée, mais plutôt comme une approche regroupant divers penseurs ayant contribué de manière significative à la réflexion sur les sciences, sans pour autant former un ensemble cohérent. Parmi les traditions rattaché à cette approche, on trouve des positivistes, représenté par des figures telles que Henri Poincaré, Pierre Duhem, Gaston Milhaud, Édouard Le Roy, Otto Neurath, Émile Meyerson et Louis Rougier,. Une autre tradition importante est celle de l'histoire et de la philosophie des sciences, avec des représentants comme Hélène Metzger, Alexandre Koyré et Abel Rey.\n\n\n=== Un courant français : l'épistémologie historique ===\n\nL'épistémologie historique est une discipline située à l'intersection de l'histoire et de la philosophie des sciences. Elle se consacre à l'étude des fondements, des méthodes, et du développement des sciences à travers le temps. Cette approche se distingue par son attention particulière aux conditions historiques et contextuelles qui influencent la production et la validation des connaissances scientifiques. Les épistémologues historiques scrutent les changements dans les paradigmes scientifiques, les révolutions conceptuelles, et les fluctuations des normes et des pratiques scientifiques. Ils rejettent l'idée d'un progrès scientifique linéaire et ininterrompu, mettant plutôt en lumière les discontinuités et ruptures qui caractérisent l'histoire des sciences.\nCette approche, initié par Auguste Comte qui a selon Pietro Redondi « une histoire des sciences avec des fondements philosophiques »,. Parmi les figures prééminentes de cette discipline, Gaston Bachelard est souvent cité comme le père de l'épistémologie historique, avec ses concepts de \"ruptures épistémologiques\". Georges Canguilhem et Michel Foucault ont ensuite élargi ces idées, explorant respectivement les rôles des normes dans les sciences et les relations entre pouvoir et savoir. Au tournant du XXIe siècle, une \"nouvelle école\" d'épistémologie historique a émergé, notamment dans le monde anglo-saxon, avec des auteurs tels que Ian Hacking, Lorraine Daston, Peter Galison, et Hans-Jörg Rheinberger, qui ont apporté de nouvelles perspectives sur les styles de raisonnement scientifique et l'impact des outils expérimentaux. Cette renaissance souligne l'importance continue et la pertinence de l'épistémologie historique dans l'analyse contemporaine des sciences.\n\n\n=== Épistémologies disciplinaires ===\nLorsqu'elle penche sur connaissance scientifique, l'épistémologie peut s'appliquer a l'étude d'une discipline scientifique particulière, on parle alors d'épistémologie régionale ou d'épistémologies disciplinaires. Il peut s'agir :\n\nd'un thème général qui a été particularisé par la science spéciale ;\nde l'émergence sur le devant de la scène d'un nouveau thème lié spécifiquement à la science spéciale et qui ne se généralise pas aux autres disciplines.\nPar exemple, le thème de l'éthique qui est posée à l'économie dont on ne peut accepter que la science qui la prend pour objet ne s'inquiète pas du sort de populations fragiles.\n\n\n== Épistémologie dans l'histoire des sciences et de la philosophie ==\n\nL'histoire des sciences et de la philosophie a produit de nombreuses théories quant à la nature de la connaissance et à la manière comment nous obtenons et validons notre connaissance. Elle remonte aux philosophes de l’Antiquité comme Platon et Aristote qui interrogaient sur la vérité et la réalité, Platon mettant l’accent sur des idées parfaites, et Aristote sur l’observation du monde réel. Cette tradition antique nous parvient à travers le Moyen Âge, par le biais des penseurs catholiques qui ont mêlé ces idées anciennes à la religion chrétienne, formant une nouvelle approche de la connaissance. À la Renaissance, le regain d’intérêt pour les textes anciens et l’importance donnée à l’observation directe ont ouvert la voie à des scientifiques comme Francis Bacon et René Descartes. Bacon a encouragé l’utilisation d’expériences pour découvrir des vérités, tandis que Descartes a cherché des certitudes absolues en questionnant tout ce qui pouvait être mis en doute. Ces idées ont jeté les bases de la science moderne, transformant la façon dont nous comprenons et validons la connaissance.\nSelon Hervé Barreau, l'épistémologie moderne tire son origine du criticisme de Kant au XVIIIe siècle et du positivisme de Comte aux XIXe et XXe siècles. ant a révolutionné la pensée de la connaissance en affirmant que notre compréhension du monde est façonnée par la manière dont notre esprit structure les expériences. Selon lui, nous ne connaissons pas les choses telles qu’elles sont en elles-mêmes, mais seulement telles qu’elles apparaissent à travers les filtres de notre esprit. Cette a ouvert de nouvelles voies pour explorer comment et pourquoi nous pensons comme nous le faisons, influençant profondément les débats les épistémologie future.\nLe XXe siècle a marqué un tournant radical. Très schématiquement, aux premières réflexions purement philosophiques et souvent normatives sont venus s’ajouter des réflexions plus sociologiques et psychologiques, puis des approches sociologiques et anthropologiques dans les années 1980, et enfin des approches fondamentalement hétérogènes à partir des années 1990 avec les Science studies. Le discours sera également interrogé par la psychologie avec le courant du constructivisme.\n\n\n=== La révolution Kantienne ===\n\nKant offre un changement de perspective radical vis-à-vis de l'empirisme : c'est une véritable révolution épistémologique, qu'il qualifie lui-même par l'expression célèbre de « révolution copernicienne ». Hume avait déjà placé le sujet au centre de la connaissance. Kant, lui, va jusqu'à affirmer que la véritable origine de la connaissance est dans le sujet et non dans une réalité vis-à-vis de laquelle nous serions passifs[réf. nécessaire].\nIl reprend certains principes des empiristes : « Ainsi, dans le temps, aucune connaissance ne précède l'expérience, et toutes commencent avec elle », explique-t-il dans Critique de la raison pure.\nAinsi pour Kant, note l'économiste Claude Mouchot, « l'objet en soi, le noumène, est et restera inconnu » et « nous ne connaitrons jamais que les phénomènes », et en cela Kant reste très actuel. Selon les termes de Kant (Critique de la raison pure) « il n’y a que les objets des sens qui puissent nous être donnés […] ils ne peuvent l’être que dans le contexte d’une expérience possible ».\n\nRussel fait remonter la filiation de l'épistémologie à Kant :\n« Ce fut seulement de Kant, le créateur de l'Épistémologie, que le problème géométrique reçut sa forme actuelle ».\n\nOn a considéré à l'époque — peut-être à tort — que la problématique de Fichte était éloignée de la problématique kantienne et l'on a attribué le concept d'épistémologie à Eduard Zeller, lequel utilise le mot allemand Erkenntnistheorie (« théorie de la connaissance ») dans un sens kantien[pertinence contestée], c'est-à-dire portant sur la possibilité de la connaissance et les fondements des sciences particulière.\n\n\n=== Subjectivité postkantienne : Idéalisme allemand, « Sciences de l'esprit » ===\n\n\n==== De Kant à Hegel ====\n\nAlors qu'on a souvent évoqué l'influence qu'auraient exercée Descartes et Rousseau dans la philosophie allemande, notamment dans « l’idéalisme multiforme qui s’est développé de Kant à Hegel, à savoir Descartes pour la philosophie théorique et Rousseau pour la philosophie pratique », Bernard Bourgeois considère qu'« en ce qui concerne Descartes, son influence a été fort limitée » : les problématiques de Descartes d'une part, de Kant et de ses successeurs d'autre part, sont à ses yeux très différentes. Dans le Cogito, la problématique cartésienne est ontologique, elle « s’interroge sur ce qui est véritablement » : la problématique de la réflexion y est provisoire, pour laisser la place peu après « à une problématique d’un tout autre type, privilégiant dans son objet essentiellement la déduction, et une déduction à portée ontologique ». Chez Kant et les postkantiens, « la problématique philosophique se déploie sur la base d’une science constituée, une science qui réussit, à savoir la physique newtonienne notamment. Elle ne présente pas une interrogation ontologique, mais une interrogation sur l’ontologie ». Pour le Je pense transcendantal, la question devient celle de savoir « comment un discours scientifiquement assuré est possible ».\nDans l'idéalisme allemand, « tout le champ du savoir s’étend d’une réflexivité première – celle du Je pense – à une réflexivité dernière – la réflexivité essentiellement éthique ». Chez Fichte, « la conscience de soi la plus concrète, la représentation de soi la plus totale, est celle du Je éthique ». Cette « présence à soi de la pensée », qui n'est en fait pas une « réflexion proprement dite », Fichte la désigne du terme « intuition » : « il s’agit de l’intuition intellectuelle, de l’intuition de l’intellect, en tant que l’intellect, à la différence de la sensibilité, réceptivité ou passivité, est une activité. L’intuition intellectuelle est la présence à soi immédiate de l’agir qu’est la pensée ».\nSelon Alexis Philonenko, Fichte, qui trouvait la philosophie de Kant « inachevée », peut être considéré comme « une marche dans l'escalier menant via Schelling de Kant à Hegel ».\nEn reprenant l'idée kantienne que la subjectivité est un des fondements de toute philosophie, la subjectivité étant le fondement de la « philosophie transcendantale » (cf. §16 de la Critique de la raison pure), Hegel re-développe l'idée d'une subjectivité absolue, au travers du concept de Moi chez Fichte, pour en faire une phénoménologie de l'esprit. Il s'agit aussi pour les penseurs de cette époque de défendre la primauté de l'Esprit sur la nature.\n\n\n==== D'une « conception encyclopédique de la science » aux sciences de l'esprit (Geisteswissenschaften) ====\n\nDans les premières années du XIXe siècle, Schelling et surtout Hegel « conçoivent l'idée d'une philosophie ou “science de l'esprit” ». Cette « science philosophique » est exposée dans la troisième et dernière partie de l' Encyclopédie des sciences philosophiques (1817, 1820-1830) qui représente « le système achevé de Hegel » (la première partie traite de la logique et la seconde partie de la philosophie de la nature).\nSelon Myriam Bienenstock, c'est à cette « conception encyclopédique de la science, héritée du XVIIIe siècle », que s'oppose, dans la première moitié du XIXe siècle, ce que Heinrich Heine a dénommé l'« école romantique ». Cette dernière reprend, en la transformant, l'opposition aristotélicienne de la « science » à l'« histoire ». Pour ses adeptes en effet, « l'histoire relève d'autres démarches que celles réservées à la “science” proprement dite parce qu'elle traite, par définition, de ce qui est singulier, unique — individuel ». À cette raison s'ajoute celle « du changement, de l'historicité fondamentale de l'être humain » que, dans la seconde moitié du XIXe siècle, vont mettre en avant les partisans de l'historicisme. Ou s'y ajoute encore « l'idée selon laquelle le comportement humain, précisément parce qu'il est l'expression d'une individualité singulière, située dans l'histoire, devrait être compris plutôt que simplement expliqué, comme le veulent les sciences de la nature ».\n\nIsabelle Kalinowski rapporte la « formule demeurée fameuse » de Wilhelm Dilthey :\n\n« Nous expliquons la nature, nous comprenons la vie psychique »\n\n— Wilhelm Dilthey\nL'opposition de « l’expliquer » (erklären) et du « comprendre » (verstehen) correspond donc au « dualisme des types de sciences » : les sciences de la nature s'opposent aux sciences de l’esprit chez Dilthey. Ou autrement dit, « les “sciences de l'esprit” ou Geisteswissenschaften sont fondamentalement distinctes des sciences de la nature ».\nD'après Sylvie Mesure, « le philosophe allemand Wilhelm Dilthey, dès son Introduction » aux sciences de l'esprit (1883), désigna son entreprise comme une « critique de la raison historique » : ce qu'avait fait la Critique de la raison pure à l'égard des sciences de la nature, il s'est agi pour lui de le transposer aux sciences historiques, en posant le problème de leur objectivité et de ses limites ». L'entreprise de Dilthey visait d'une part à « isoler les sciences historiques des sciences physiques, en dégageant leurs principes propres », d'autre part, et c'est épistémologiquement le plus important, « à établir l'autonomie de ces sciences de la réalité sociale, culturelle et politique qui rassemble l'historicité de leurs objets ». C'est ici que  son œuvre est à l'évidence « en rupture avec l'épistémologie positiviste alors dominante ».\nPour Myriam Bienenstock, si la « philosophie de la vie » (Lebensphilosophie) fonde la « science de l'esprit » de Dilthey en rendant compte de ses catégories de base, elle est « étrangère à la « philosophie morale » en honneur en France, tout au long du XIXe siècle ». Mais en Allemagne, l'influence de Dilthey s'est avérée considérable ; elle s'est exercée notamment en histoire de la littérature et en philosophie, a joué un rôle important dans la formation de la pensée de Heidegger ainsi que dans l'herméneutique de Gadamer, c'est-à-dire « bien au-delà des frontières de l'Allemagne ».\n\n\n=== Tournant positiviste et positivisme logique en France ===\n\nAuguste Comte distingue trois états historiques :\n\ndans l'état théologique, l'esprit de l'homme cherche à expliquer les phénomènes naturels par des agents surnaturels.\ndans l'état métaphysique, l'explication se fonde sur des forces naturelles mais encore personnifiées (la théorie de l'éther par exemple).\navec l'état positif, l'esprit ne cherche plus à expliquer les phénomènes par leurs causes, mais il s'édifie sur des faits constatables et mesurables.\nLe personnage de Newton est, pour Comte, révélateur de cette « marche progressive de l'esprit humain ».\nLa science doit ainsi mettre en œuvre des hypothèses, permettant de se passer de l'expérience, et aboutissant à la formation de lois non contradictoires. Comte cite ainsi, comme exemple, la théorie de la chaleur de Joseph Fourier, qu'il a bâtie sans avoir à observer la nature du phénomène. Le positivisme met en avant la qualité prédictive de la science, qui permet de « voir pour prévoir » selon les mots de Comte, dans ses Discours sur l'ensemble du positivisme (1843). Néanmoins, pour lui, la méthode scientifique culmine dans la mise en pratique, dans l'action : ce que le discours moderne appellera l'application scientifique. L'ingénierie est ainsi la main de la science, caractérisée par le savoir-faire. La science est avec Comte indissociable de l'action :\n\n« Science, d'où prévoyance ; prévoyance d'où action »\n\nDans la philosophie de Comte, l'esprit se limite au « comment », et renonce à la recherche du « pourquoi ultime » des choses.\n\n\n=== Philosophie contemporaine ===\n\n\n==== Cercle de Vienne ====\n\nLe « cercle de Vienne » (Wiener Kreis) qui se forme à partir de 1923 autour de la personnalité de Moritz Schlick, projette de « développer une nouvelle philosophie de la science dans un esprit de rigueur, et en excluant toute considération métaphysique ». Les thèmes principaux élaborés de concert avec une autre association fondée à Berlin par Hans Reichenbach (cercle de Berlin) vont donner naissance au « néo-positivisme, ou positivisme logique ». Si l'avènement du nazisme a contraint le groupe viennois à la diaspora vers l'Amérique et l'Angleterre, la plupart de ses membres y ont poursuivi leur carrière, tandis que « leurs travaux  se sont imposés à l'ensemble du monde philosophique ».\nPour le philosophe américain Sydney Hook, auquel se réfère Gilles Gaston Granger, le monde philosophique germanique entre les deux guerres se caractériserait en 1930 « par l'influence dominante d'une tradition idéaliste purement autochtone » et ferait preuve d'une « “étonnante indifférence” à l'égard des résultats et des méthodes de la physique moderne » : à l'occasion, on y entendrait dire par exemple « qu'une activité scientifique spécialisée fait obstacle à une intuition philosophique supérieure… » (S. Hook, Journal of American Philosophy, vol. XXVII, no 6, 1930). Selon Granger, « c'est sans doute contre cette situation que réagissent les fondateurs du cercle de Vienne, qui invoquent le plus souvent comme inspirateurs immédiats Mach, Poincaré, Einstein, Frege, Russell et Wittgenstein ».\nKarl Popper, « d'après ses propres dires » entend parler du « Cercle de Schlick » surtout vers 1926-1927, rapporte la philosophe Mélika Ouelbani : « il avait fréquenté le séminaire de Carnap et avait eu des discussions privées avec presque tous les membres du Cercle – à savoir, Schlick, Carnap, Waismann, Hahn, Frank, Von Mises, Reichenbach et Neurath ». Selon Ouelbani, Popper aurait en réalité « critiqué le positivisme logique à travers Carnap ».\n\n\n==== « Science normale » de Thomas Kuhn ====\nLes travaux de Thomas Samuel Kuhn vont marquer une rupture fondamentale en philosophie, en histoire et en sociologie des sciences. Il va historiciser la science et rejeter une conception fixiste de la science. Son ouvrage principal en la matière, La Structure des révolutions scientifiques (1962) pose qu'« il est ainsi difficile de considérer le développement scientifique comme un processus d’accumulation, car il est difficile d’isoler les découvertes et les inventions individuelles ».\n« Lorsque les scientifiques ne peuvent plus ignorer plus longtemps des anomalies qui renversent la situation établie dans la pratique scientifique, alors commencent les investigations extraordinaires qui les conduisent finalement à un nouvel ensemble de convictions, sur une nouvelle base pour la pratique de la science » ajoute-t-il, qualifiant ces bases pratiques de paradigmes scientifiques (comme la lumière considérée comme un corpuscule, puis comme une onde, puis enfin comme une particule). Ces « épisodes extraordinaires » sont comme des « révolutions scientifiques » (ainsi celles apportées par Isaac Newton, Nicolas Copernic, Lavoisier, ou encore Einstein) : toutes viennent renverser un paradigme dominant. L'état d'une science, des connaissances et du paradigme, à une période donnée, constitue la « science normale » qui est selon Kuhn « une recherche fermement accréditée par une ou plusieurs découvertes scientifiques passées, découvertes que tel ou tel groupe scientifique [a considérées] comme suffisantes pour devenir le point de départ d’autres travaux. »\n\n\n==== Quine et « l'épistémologie naturalisée » ====\n\nAvec l'article Deux dogmes de l'empirisme, Willard Van Orman Quine critique deux aspects centraux du positivisme logique. Le premier est la distinction entre vérités analytiques et vérités synthétiques : il y aurait des propositions vraies indépendamment des faits, qui seraient vraies en vertu de leur seule signification. Le second dogme, le réductionnisme, est la théorie selon laquelle les énoncés doués de sens peuvent être reformulés en énoncés portant sur des données de l'expérience immédiate (dans ce cas un énoncé analytique serait un énoncé confirmé par l'expérience dans tous les cas).\nCe texte constitue une attaque en règle contre l'héritage théorique du positivisme logique. Comme le précise Quine lui-même, « Another effect is a shift toward pragmatism » : « Les deux dogmes de l'empirisme » marque le grand retour du pragmatisme dans la philosophie américaine, au sein même du mouvement intellectuel qui l'avait évincé de la scène intellectuelle : la philosophie analytique (sous sa forme empiriste).\nAvec l'« épistémologie naturalisée », Quine, dans un point de vue naturaliste, affirme que la philosophie de la connaissance et des sciences constitue elle-même une activité scientifique, corrigée par les autres sciences, et non pas une « philosophie première » fondée sur une métaphysique.\n\n\n==== Critique de l'induction de Mach ====\n\nInventeur de la mesure de la vitesse de propagation du son, Ernst Mach développa une pensée épistémologique qui influença notamment Albert Einstein. Dans La Mécanique, exposé historique et critique de son développement, Mach dévoile la conception mythologique qui sous-tend les représentations mécanistes de son époque et qui aboutissent au conflit des spiritualistes et des matérialistes. Mais la critique de Mach porte surtout sur la méthode de l'induction, pendant de la déduction. Dans La Connaissance et l'erreur (1905), Mach explique que le travail du savant porte avant tout sur les relations des objets étudiés entre eux, et non sur leur classement. La démarche de recherche est avant tout mentale conclut Mach : « Avant de comprendre la nature, il faut l'appréhender dans l'imagination, pour donner aux concepts un contenu intuitif vivant ». Par ailleurs, Mach défend l'idée que la science est symbolique, thèse qu'il reprend chez Karl Pearson dans La Grammaire de la science (1892) et qui explique que la science est « une sténographie conceptuelle ». Mach annonce que seule la méthode empirique est scientifique : \n« Nous devons limiter notre science physique à l'expression des faits observables, sans construire d'hypothèses derrière ces faits, où plus rien n'existe qui puisse être conçu ou prouvé »\n\n\n==== Réfutabilité de Karl Popper ====\n\nLe philosophe autrichien Karl Popper (1902-1994) bouleverse l'épistémologie classique en proposant une nouvelle théorie de la connaissance, dès 1934 avec la Logique de la découverte scientifique. Il donne à l'épistémologie de nouveaux concepts et outils d'examen, comme la réfutabilité (capacité d'une théorie scientifique de se soumettre à une méthode critique sévère) ou l'infaillibilité (qui définit a contrario les théories métaphysiques, psychanalytiques, marxistes, astrologiques). Il propose ainsi de voir dans la réfutabilité le critère permettant de distinguer la science de la non-science. Un énoncé est ainsi « empiriquement informatif, si et seulement s'il est testable ou réfutable, c'est-à-dire s'il est possible, au moins en principe, que certains faits puissent le contredire ». Néanmoins, Popper admet que les énoncés non réfutables peuvent être heuristiques et avoir un sens (c'est le cas des sciences humaines).\nPopper émet par ailleurs une critique de la thèse de l'unicité de la science, notamment dans son ouvrage La Logique de la découverte scientifique. L'idée d'un système de connaissance est futile selon lui : « nous ne savons pas, nous ne faisons que conjecturer. » L’idéal d’une connaissance absolument certaine et démontrable s’est révélé être une idole. Selon lui, enfin, l'induction n'a aucune valeur scientifique :\n\n« Il n'y a pas d'induction parce que les théories universelles ne sont pas déductibles d'énoncés singuliers. »\n\n\n==== « Programmes de recherche scientifique » de Imre Lakatos ====\n\nLa pensée d'Imre Lakatos (1922-1974) est en droite file de celle de Popper. Il est le créateur de la notion de « programmes de recherche scientifique » (P.R.S) qui est un corpus d'hypothèses théoriques lié à un plan de recherche au sein d'un domaine particulier (un « paradigme ») comme la métaphysique cartésienne par exemple. Lakatos, bien qu'étant l'élève de Karl Popper, s'oppose à lui sur le point de la réfutabilité. Un programme de recherche est selon lui caractérisé à la fois par une heuristique positive (qui définit ce qu'il faut chercher et quelle méthode utiliser) et une heuristique négative (les hypothèses sont inviolables).\n\n\n==== Holisme épistémologique ====\n\nOpposé à toute interprétation matérialiste et réaliste de la chimie et de la physique, Pierre Duhem proposa une conception qu'on qualifiera ensuite d'« instrumentaliste » de la science dans La Théorie physique. Son objet et sa structure (1906). Selon l'instrumentalisme, la science ne décrit pas la réalité au-delà des phénomènes mais n'est qu'un instrument le plus commode de prédiction.\nLe holisme épistémologique de Quine ne se limite pas à la physique comme celui de Duhem, ni même aux sciences expérimentales comme celui de Carnap mais s'étend à toute la science, logique et mathématique comprise.\n\n\n==== Phénoménologie de Husserl ====\n\nPour Edmund Husserl, la phénoménologie prend pour point de départ l'expérience en tant qu'intuition sensible des phénomènes afin d'essayer d'en extraire les dispositions essentielles des expériences ainsi que l'essence de ce dont on fait l'expérience.\n\n\n==== Constructivisme et systémique ====\n\nLe terme constructivisme est né au début du XXe siècle avec le mathématicien néerlandais Brouwer qui l'utilisa pour caractériser sa position sur la question des fondements en mathématiques comme discipline maîtresse. Mais c'est surtout Jean Piaget qui a su apporter au constructivisme ses lettres de noblesse : avec la publication en 1967 de l'encyclopédie de la Pléiade et notamment du volume XXII : Logique et connaissance scientifique, il opère selon Jean-Louis Le Moigne une « renaissance du constructivisme épistémologique, notamment à partir des travaux de Bachelard ». Toutefois, selon Ian Hacking, c'est Kant qui fut le « grand pionnier de la construction ».\nL'école constructiviste n'accepte comme vrai que ce que le scientifique peut construire, à partir d'idées et d'hypothèses que l'intuition (comme fondement des mathématiques) accepte comme vraies, et qui sont représentables. Le psychologue et épistémologue Jean Piaget expliquera ainsi que le « fait est […] toujours le produit de la composition, entre une part fournie par les objets, et une autre construite par le sujet ». L'expérimentation ne sert alors qu'à vérifier la cohérence interne de la construction[réf. nécessaire] (c'est la notion de modèle épistémologique).\nPiaget étendra cependant le cadre constructiviste à ce qu'il nomme l'« épistémologie génétique » qui étudie les conditions de la connaissance et les lois de son accroissement, en lien avec le développement neurologique de l'intelligence. Pour lui, l'épistémologie englobe la théorie de la connaissance et la philosophie des sciences (ce qu'il nomme le « cercle des sciences » : chaque science renforce l'édifice des autres sciences). Autrement dit, « la succession des sciences dans l'histoire obéit à la même logique que l'ontogenèse des connaissances ». Sans parler de ressemblance totale, les mécanismes, de l'individu au groupe de chercheurs et donc, aux disciplines scientifiques, sont communs (Piaget cite ainsi l'« abstraction réfléchissante »).\nRefusant l'empirisme, l'épistémologie constructiviste pose que la connaissance se fait au moyen d'une dialectique, du sujet à l'objet et de l'objet au sujet, par un aller et retour expérimental.\nJean Piaget proposait de définir l’épistémologie « en première approximation comme l’étude de la constitution des connaissances valables », dénomination qui, selon Jean-Louis Le Moigne, permet de poser les trois grandes questions de la discipline :\n\nQu’est ce que la connaissance et quel est son mode d'investigation (c'est la question « gnoséologique ») ?\nComment la connaissance est-elle constituée ou engendrée (c'est la question méthodologique) ?\nComment apprécier sa valeur ou sa validité (question de sa scientificité) ?\nCes travaux vont inspirer plusieurs auteurs. Certains, liés à la systémique, sont publiés par Paul Watzlawick en 1980 dans l’ouvrage L’invention de la réalité – Contributions au constructivisme. Edgar Morin offre au constructivisme son « discours de la méthode » avec La Méthode. Herbert Simon renouvelle la classification des sciences avec Les sciences de l’artificiel.\n\n\n==== Structuralisme ====\n\nLe structuralisme est un ensemble de courants holistes en épistémologie apparus principalement en sciences humaines et sociales au milieu du XXe siècle, ayant en commun l'utilisation du terme de structure entendue comme modèle théorique (inconscient, ou non empiriquement perceptible) organisant la forme de l'objet étudié pris comme un système, l'accent étant mis moins sur les unités élémentaires de ce système que sur les relations qui les unissent. La référence explicite au terme structure, dont la définition n'est pas unifiée entre les différents courants de pensée concernés, se systématise progressivement avec la construction institutionnelle des sciences humaines et sociales à partir de la seconde moitié du XIXe siècle dans la filiation positiviste ; cependant certains auteurs font remonter bien antérieurement (jusqu'à Aristote) la généalogie du structuralisme.\nLa définition du structuralisme et de ses frontières disciplinaires est devenue un champ de recherche à part entière, complexe et en évolution rapide. Actuellement, le terme en français tend à désigner deux types de phénomènes :\n\ndans le sens le plus connu (structuralisme généralisé), une période particulière de l'histoire des idées scientifiques, un phénomène transitoire de mode intellectuelle à caractère contestataire ayant eu cours entre la fin des années 1950 et le début des années 1970, essentiellement en France, débordant largement les frontières universitaires pour envahir le champ littéraire, médiatique et politique ; ce « moment structuraliste », inspiré essentiellement de la linguistique saussurienne et très marqué par son formalisme, s'est organisé autour d'un petit nombre de personnalités-phares : Roland Barthes en littérature, Jacques Lacan en psychanalyse, Michel Foucault et Louis Althusser en philosophie ;\ndans son acception épistémologique plus spécialisée,, un paradigme scientifique proche de la systémique où la notion de structure est centrée sur la genèse dynamique des systèmes de l'esprit et du sens, entendus au sens de la philosophie de la forme, avec une généalogie remontant jusqu'à Aristote ; c'est dans cette lignée naturaliste du structuralisme que s'est situé l'ethnologue Claude Lévi-Strauss, en développant à partir des années 1950 l'anthropologie structurale en rupture avec les courants de l'anthropologie anglo-saxonne de l'époque (évolutionnisme, diffusionnisme, culturalisme, fonctionnalisme).\n\n\n===== Michel Foucault =====\nPour Hervé Barreau, « on a désigné [dans le passé] en France par épistémologie l'étude de l’épistémè, c'est-à-dire de ce que Michel Foucault considérait comme un corps de principes, analogues aux “paradigmes” de T. S. Kuhn, qui sont à l’œuvre simultanément dans plusieurs disciplines, et qui varient dans le temps de façon discontinue ». […] C'est pourquoi la conception foucaldienne de l'épistémologie, que son auteur avait bornée du reste aux sciences de la vie et aux sciences de l'homme, ne peut prétendre occuper le terrain de ce qu'on entendait jadis par la philosophie des sciences.\nCette épistémologie foucaldienne est incluse dans l'épistémologie actuelle.\n\n\n===== Épistémologie comparative de Gilles Gaston Granger =====\n\nIntroduite par Gilles Gaston Granger, l'Épistémologie comparative a pour objet la comparaison de théories ou de systèmes scientifiques en vue de dégager « l'homologie formelle du fonctionnement de différents concepts dans ces structures ».\nUne chaire d'Épistémologie comparative a été créée au Collège de France en 1987.\n\n\n==== Épistémologie complexe ====\n\nDans ce courant de pensée, l'objet à étudier est considéré comme un système complexe, c'est-à-dire qu'il est fonction d'une multitude de paramètres et inclut des inerties, des non-linéarités, des rétroactions, des récursivités, des seuils, des jeux de fonctionnement, des influences mutuelles de variables, des effets retard, des hystérésis, des émergences, de l'auto-organisation, etc. Il est en relation avec son milieu, qui l'alimente en entrées (par ex. énergie et commandes) et à qui il donne des sorties (par ex. production et déchets).\nEn France, Henri Poincaré est un précurseur de cette approche. Edgar Morin et Jean-Louis Le Moigne l'ont développé par leurs travaux, écrits et conférences.\n\n\n== Questions de l'épistémologie dans le sens philosophie des sciences ==\n\nLorsqu'elle penche sur connaissance scientifique, l'épistémologie est axée sur l'analyse de la spécificité et des conditions d'existence de la connaissance scientifique. Autrement dit, elle se penche sur des questions telles que : qu'est-ce qu'une science et comment la reconnaît-on ? Quelles sont les différences entre une connaissance scientifique et un savoir qui ne l'est pas ?\n\n\n=== Production des connaissances scientifiques ===\n\nLes questions épistémologiques portent par exemple sur :\n\nQuelle place accorder à l'intuition, à la créativité, à l'imagination, à l'analogie entre disciplines, à la sérendipité ?\nQuelles méthodes ? La question de la déduction, de l'induction…\nQuelles formes de validations ?… On trouve ici la question de l'explication, de la validation…\nIl y a aussi la question de l'unité de la science ou de production de science dans un contexte pluridisciplinaire/interdisciplinaire.\nUn exemple volontiers cité est l'étonnement des mathématiciens grecs devant le fait que la diagonale du carré ne puisse correspondre à aucune fraction irréductible p/q, à une époque où on n'imaginait de nombres que rationnels (l'irrationalité de pi était encore inconnue). En effet, on aurait eu alors (p/q)² = 2, soit p² = 2 q². Cela aurait impliqué que p² soit pair, soit p = 2k ; mais en ce cas p² aurait valu 4k² et la fraction p/q n'aurait pas été irréductible, ce qui était contraire à l'hypothèse.\n\n\n==== Rationalité ====\nJean Ladrière donne une définition de la rationalité scientifique : « Une démarche rationnelle, dans l'ordre cognitif comme dans l'ordre de l'action, est une démarche qui s'accompagne de la monstration de sa validité ou de sa légitimité, conformément à des critères qui peuvent eux-mêmes être reconnus comme acceptables au regard d'une critique éventuelle ». L'exigence fondatrice de la rationalité c'est la nécessité de justifier le pourquoi de ses jugements.\n« La recherche de rationalité est une démarche atemporelle, mais les formes de la raison sont […] historiques et donc contingentes », nous dit Michel Morange.\n\n\n==== Déduction ====\n\nLa méthode hypothético-déductive est régulièrement considérée comme la production scientifique par excellence, surtout depuis que la science s'inscrit dans le paradigme de la recherche appliquée, qui consiste à travailler à résoudre des problèmes identifiés d'avance, selon la méthode du problem-solving. Cependant, la démarche mise en œuvre par les découvreurs échappe régulièrement à cette approche, très rationaliste.\n\n\n==== Induction ====\n\nL'induction consiste à se fonder sur l'observation de cas singuliers pour justifier une théorie générale ; c'est l'opération qui consiste à passer du particulier au général. Le problème est de savoir s'il peut être épistémologiquement valide de croire que les théories universelles sont justifiées voire vérifiées par la seule prise en compte d'un grand nombre d'observations singulières passées. Par exemple, nous avons observé que le soleil, jusqu'ici, se lève le matin. Mais rien ne semble justifier notre croyance au fait qu'il se lèvera encore demain. Ce problème avait été jugé insoluble par Hume, pour lequel notre croyance relevait de l'habitude consistant à voir telle cause susciter tel effet, ce qui ne présume pas que ce soit le cas dans la réalité. Cette position non réaliste fut critiquée par Emmanuel Kant, Karl Popper et Ernest Mach bien que le concept d'induction, tout comme celui de réfutation, regroupent aujourd'hui une variété de théories allant des plus naïves aux plus sophistiquées.\n\n\n=== Validation des connaissances scientifiques ===\n\n\n==== Vérification ====\n\nC'est le problème des fondements de la connaissance scientifique :\n\nla nature de la connaissance : connaissance scientifique ou générale, exclusion de la métaphysique de la Science… C'est notamment la question de la démarcation,\nla validation de la connaissance, de la question du réalisme/antiréalisme, et bien sûr la question du rapport au vrai.\nCe qui mène également à la question du relativisme.\n\n\n==== Nature des connaissances ====\nHistoriquement, cette question épistémologique concerne plus directement la question de savoir comment identifier ou démarquer les théories scientifiques des théories métaphysiques. Au XXIe siècle, il y a aussi le tri entre la connaissance en général et la connaissance véritablement scientifique.\nLes philosophes positivistes fondateurs du Cercle de Vienne, pensaient que le seul critère de démarcation qui puisse être valide, (afin d'éliminer la métaphysique), était la vérifiabilité des énoncés singuliers, seules données des sens capables de permettre la vérification des théories générales de la science, à la condition qu'elles soient suffisamment nombreuses et bien observées.\nPour Karl Popper, philosophe des sciences du XXe siècle et adversaire des thèses et du projet du Cercle de Vienne, aucune théorie scientifique générale n'a jamais pu être établie par une quelconque forme d'induction, donc être vérifiée. Il critique le raisonnement par induction : ce dernier a pour lui une valeur psychologique mais pas une valeur logique. De nombreuses observations cohérentes ne suffisent pas à prouver que la théorie qu'on cherche à démontrer soit vraie. A contrario, une seule observation inattendue suffit à réfuter une théorie. Ainsi, mille cygnes blancs ne suffisent pas à prouver que tous les cygnes sont blancs ; mais un seul cygne noir suffit à prouver que tous les cygnes ne sont pas blancs. Voir Paradoxe de Hempel.\nKarl Popper pense que les théories scientifiques ne peuvent pas être justifiées, même sur la base d'un très grand nombre d'observations empiriques, elles peuvent seulement être évaluées à partir de tests dont la logique consiste à tenter de mettre à l'épreuve les connaissances scientifiques (la réfutation). Il en résulte qu'une théorie ne peut être « prouvée » mais seulement considérée comme non invalidée jusqu'à preuve du contraire. Partant de là, on peut distinguer :\n\nles théories impossibles à réfuter (par l'observation ou l'expérience)\nles théories qui peuvent être invalidées.\nD'autre part il pense qu'aucune théorie scientifique n'est logiquement ou même empiriquement vérifiable si l'on admet sous ce terme la notion de certitude ou de vérification avec certitude. Karl Popper soutient même qu'une théorie ne peut être scientifique que si elle est potentiellement fausse (réfutable), et même fausse en comparaison de la vérité certaine à laquelle elle prétendrait se rapprocher. Seules les théories potentiellement réfutables (celles associables à des expériences dont l'échec prouverait l'erreur de la théorie) font partie du domaine scientifique; c'est le « critère de démarcation des sciences ».\nLe problème de la démarcation (identifié comme étant le problème de Kant par Karl Popper) s'articule à celui de la justification des théories :\n\nsoit selon une méthode inductive,\nsoit par une méthode hypothético-déductive.\nDans le domaine de la science empirique, la vérification devrait plutôt être assimilable à la corroboration (Karl Popper), c'est-à-dire à une forme relative et non absolue de vérité, toujours dépendante des tests scientifiques qui ont pu être réalisés par une communauté de chercheurs. Ainsi, en science, la vérification des théories seraient donc toujours relative à des tests eux-mêmes relatifs à d'autres tests précédents et toujours améliorables, et jamais absolus.\n\n\n==== Réfutation ====\n\nRendu célèbre par l'œuvre de Karl Popper, ce terme implique la possibilité d'évaluer empiriquement les énoncés généraux de la science par l'intermédiaire de tests. Seules les théories formulées de manière à pouvoir permettre la déduction logique d'un énoncé particulier capable potentiellement de les réfuter, peuvent, pour Karl Popper, être considérées comme scientifiques et non métaphysiques.\nMais Popper propose qu'il existe deux niveaux de réfutabilité. La réfutabilité « logique » et la réfutabilité « empirique » ; sachant qu'un énoncé réfutable d'un point de vue logique ne l'est peut-être pas d'un point de vue empirique. Par exemple, l'énoncé « tous les hommes sont mortels » est logiquement réfutable, mais empiriquement irréfutable puisque aucun être humain ne pourrait vivre assez vieux pour vérifier qu'un homme est immortel.\nKarl Popper a toujours soutenu qu'aucune réfutation empirique ne pouvait être certaine, car il est toujours possible de sauver une théorie d'une réfutation par l'adoption de stratagèmes ad hoc. En conséquence, pour Popper, le critère de démarcation reposant sur la réfutation, doit avant tout être un critère méthodologique puisque tout reposerait, en dernier ressort, sur les décisions de la communauté scientifique, pour accepter ou rejeter la valeur d'un test, d'une réfutation ou d'une corroboration.\n\n\n==== Relativisme ====\n\nPaul Feyerabend observait à l'exemple de la naissance de la mécanique quantique que souvent l'avancement scientifique ne suit pas de règles strictes. Ainsi, selon lui, le seul principe qui n'empêche pas l'avancement de la science est « a priori tout peut être bon » (ce qui définit l'anarchisme épistémologique - à distinguer de « tout est bon » (anything goes), que Feyerabend lui-même récusait). Il critique donc l'aspect réducteur de la théorie de la réfutabilité et défend le pluralisme méthodologique. Il existe selon lui une très grande variété de méthodes différentes adaptées à des contextes scientifiques et sociaux toujours différents.\nDe plus, il remet en question la place que la théorie de la réfutabilité accorde à la science, en faisant d'elle l'unique source de savoir légitime et le fondement d'une connaissance universelle qui dépasse les clivages culturels et communautaires. Enfin, Feyerabend critique le manque de pertinence pour décrire correctement la réalité du monde scientifique et des évolutions des discours et pratiques scientifiques.\nSon œuvre principale, Contre la méthode. Esquisse d'une théorie anarchiste de la connaissance, fut reçue très négativement par la communauté scientifique, car elle accusait la méthode scientifique d'être un dogme et soulevait la question de savoir si la communauté doit être aussi critique par rapport à la méthode scientifique que par rapport aux théories qui en résultent.\n\n\n==== Contexte de découverte ====\nContexte de découverte et contexte de justification : pendant longtemps, la question de la découverte ne relève pas de l'épistémologie, mais au mieux de la psychologie (recherche des intentions, des pré-pensés… du chercheur).\nLes choses ont changé progressivement : l'épistémologie moderne ré-interroge les corpus de connaissances scientifiques acquises et questionne les contextes de découverte, de validation, de communication et d'enseignement de la Science et de la recherche en train de se faire.\n\n\n=== Évolution et dynamique des connaissances ===\n\nLa question épistémologique concerne la nature du processus dynamique du changement scientifique :\n\nLa science avance-t-elle par sauts ? Continuisme et discontinuisme,\nLa science progresse-t-elle que de l'intérieur ? ou bien est-ce que les non scientifiques font progresser la Science ? internaliste et externalisme.\nCe qui renvoie à nouveau au problème du relativisme.\n\n\n==== Continuisme et discontinuisme ====\nBachelard et l'« obstacle épistémologique » : Gaston Bachelard définit ce dernier, en 1934, dans un article intitulé La formation de l'esprit scientifique, comme étant « la rectification du savoir, l'élargissement des cadres de la connaissance ». Pour lui, le scientifique doit se dépouiller de tout ce qui constitue les « obstacles épistémologiques internes », en se soumettant à une préparation intérieure afin que sa recherche progresse vers la vérité. La notion d’« obstacle épistémologique » est ce qui permet de poser le problème de la connaissance scientifique : c'est à partir du moment où celui-ci est surmonté, donnant lieu à une « rupture épistémologique », que l'on atteint le but recherché. Les obstacles sont, pour Bachelard, non seulement inévitables, mais aussi indispensables pour connaître la vérité. Celle-ci en effet n'apparaît jamais par une illumination subite, mais au contraire, après de longs tâtonnements, « une longue histoire d'erreurs et d'errances surmontées ».\nBachelard dénonce l'opinion que laisse l'expérience empirique et son influence sur la connaissance scientifique : « le réel n'est jamais ce que l'on pourrait croire, il est toujours ce qu'on aurait dû penser », dit-il. « La science s'oppose formellement à l'opinion : l'opinion ne pense pas, elle traduit des besoins en connaissances ». La connaissance scientifique consistera à revenir sans arrêt sur le déjà découvert.\nMettant l'accent sur la discontinuité dans le processus de la construction scientifique, Thomas Samuel Kuhn discerne des périodes relativement longues pendant lesquelles la recherche est qualifiée de « normale », c'est-à-dire qu'elle s'inscrit dans la lignée des paradigmes théoriques dominants, périodes pendant lesquelles de brefs et inexplicables changements constituent une véritable « révolution scientifique ». Le choix entre les paradigmes n'est pas fondé rationnellement. Cette posture implique que chaque paradigme permet de résoudre certains problèmes et, de là, les paradigmes seraient incommensurables.\n\n\n==== Internalisme et externalisme ====\nLa vision internaliste ne prend en compte que l’histoire des idées scientifiques, de découverte en découverte, indépendamment de tout contexte : les savants sont un monde à part, qui progresse indépendamment du reste. La science se nourrit d’elle-même. Il est ainsi possible de comprendre l’histoire des sciences sans se référer au contexte historique, social, culturel. Dans cette vision l’important, ce sont les étapes de progression de l’histoire scientifique.\n\nLa vision externaliste rend au contraire la science dépendante de l’économie, de la psychologie, etc. Cela amène à des conséquences différentes suivant le contexte\n\n\n== Institutions ==\n\n\n=== En France ===\nEn France, l'épistémologie a le statut institutionnel d'une discipline à part, distincte de la philosophie et de l'histoire : elle constitue ainsi la section 72 du CNU. Elle y occupe plusieurs dizaines de laboratoires, dont notamment l'IHPST, le Centre de recherche en épistémologie appliquée, REHSEIS, le Centre François Viete, les Archives Henri Poincaré, le Centre Georges Canguilhem, l'Institut Jean-Nicod, le Centre Gilles Gaston Granger, l'IRIST, l'unité Savoirs et Textes, le GRS (Groupe de recherche sur les savoirs), qui regroupent des centaines de chercheurs, le CREA (Centre de recherche en épistémologie appliquée), le CEP (Centre d'épistémologie et de physique) ou le Centre de recherches Alexandre-Koyré. Elle intéresse plus d'une vingtaine d'écoles doctorales et des sociétés savantes comme la Société de philosophie des sciences (dépendant de l'ENS Ulm) ou la SFHST ou des listes de diffusion comme Theuth. De 1986 à 1991, une chaire d'Épistémologie comparative est créée au Collège de France pour Gilles Gaston Granger.\n\n\n== Notes et références ==\nCet article est partiellement ou en totalité issu de l'article intitulé « Science » (voir la liste des auteurs).\n\n\n=== Notes ===\n\n\n=== Références ===\n\n\n== Voir aussi ==\n\n\n=== Bibliographie ===\n : document utilisé comme source pour la rédaction de cet article.\n(Chaque liste est dans l'ordre alphabétique des noms d'auteurs :)\n\n\n==== Sur une forme d'épistémologie ====\nHervé Barreau, L'épistémologie, Paris, PUF, coll. « Que sais-je ? », 2013, 8e éd., 127 p. (ISBN 978-2-13-062607-7, lire en ligne). \nMario Bunge (trad. de l'anglais par Hélène Donadieu), Épistémologie, Maloine, coll. « Recherches interdisciplinaires », 1983, 285 p. (ISBN 978-2-224-00900-7)\nJean-Michel Besnier, Les théories de la connaissance, Paris, Presses universitaires de France, coll. « Que sais-je ? » (no 3752), 2005, 126 p. (ISBN 978-2-13-055442-4, OCLC 696019861)\nMichel Bitbol et Jean Gayon, L'épistémologie française 1830-1970, Paris, PUF, 2006, 466 p. (ISBN 978-2-13-050112-1)\nAnastasios Brenner, « Le statut de l'épistémologie selon Meyerson: », Archives de Philosophie, vol. Tome 70, no 3,‎ 1er septembre 2007, p. 375–384 (ISSN 0003-9632, DOI 10.3917/aphi.703.0375, lire en ligne, consulté le 30 août 2024). \nAnastasios Brenner, Les textes fondateurs de l'épistémologie française : Duhem, Poincaré, Brunschvicg et autres philosophes - textes choisis, Paris, Hermann, 2015, 295 p. (ISBN 978-2-7056-9090-8, lire en ligne).\nA. F. Chalmers (trad. de l'anglais par Michel Bieczunski), Qu'est-ce que la science ? : récents développements en philosophie des sciences : Popper, Kuhn, Lakatos, Feyerabend [« What is this thing called science »], Paris, La Découverte, coll. « Sciences et société », 1987, 237 p. (ISBN 978-2-7071-1713-7, OCLC 1088484020).\nJean-Marie Chevalier,\n« Aux origines de l’épistémologie sociale : principe communautaire de l’enquête et sciences sociales », dans : Alexandre Guay et Stéphanie Ruphy (éd.),  Science, philosophie, société, Presses universitaires de Franche-Comté, 2017, [lire en ligne]\n« Les deux sources de l’épistémologie sociale. Épistémologie analytique et épistémologie « proactive » : les enjeux d’une compétition », Cahiers philosophiques, 2015/3 (n° 142), p. 73-91. DOI : 10.3917/caph.142.0073. [lire en ligne]\nAndré Stanguennec, Bernard Bourgeois, Massimo Ferrari et Jean-Marie Lardic, « Première Table Ronde. La réflexion dans la philosophie allemande et française aux XIXe et XXe siècles: », Revue des sciences philosophiques et théologiques, vol. Tome 90, no 1,‎ 1er mars 2006, p. 67–77 (ISSN 0035-2209, DOI 10.3917/rspt.901.0067, lire en ligne, consulté le 30 août 2024). \nGuillaume Decauwert (préf. Denis Vernant), Apprendre à philosopher : L'épistémologie, Paris, Éditions Ellipses, 2018, 224 p. (ISBN 978-2-340-02166-2).\nDans Christian Delacroix, François Dosse, Patrick Garcia et Nicolas Offenstadt (dir.), Historiographies. Concepts et débats, 2 volumes, Gallimard, coll. « Folio histoire », 2010, vol. I :  (ISBN 978-2-07-043927-0) et vol. II :  (ISBN 978-2-07-043928-7)\nDans vol. I : Étienne Anheim, « Philosophie et histoire », passage : p. 567-571 (Épistémologie). \nDans vol. II : Enrico Castelli Gattinara, « Vérité », passage : « Épistémologie : Vérité et vérification », p. 927-927-929 ; « Épistémologie : Horizons divers, Vérités différentes », p. 929-932.\nJean Dhombres et Angèle Kremer-Marietti, L'épistémologie : état des lieux et positions, Paris, Éditions Ellipses, coll. « Philo », 2006, 111 p. (ISBN 978-2-7298-2837-0, OCLC 300408225) .\nJean-François Dortier, « Les philosophes face à la science », dans : Thomas Lepeltier (éd.), Histoire et philosophie des sciences, Auxerre, Éditions Sciences Humaines, « Petite bibliothèque », 2013, p. 153-156. DOI : 10.3917/sh.lepel.2013.01.0153. [lire en ligne]\nThierry Dugnolle, Précis d'épistémologie, \nAnne Fagot-Largeault, « Les sciences et la réflexion philosophique », Revue des sciences philosophiques et théologiques, 2006/1 (Tome 90), p. 51-65. DOI : 10.3917/rspt.901.0051. [lire en ligne]\n(en) J. F. Ferrier, Institutes of Metaphysic : The Theory of Knowing and Being, William Blackwood and Sons, 1854 (lire en ligne)\nMichel Fichant, « L'épistémologie en France », dans François Châtelet, Histoire de la Philosophie, tome=VIII : Le XXe siècle, Paris, Hachette Littératures, 2000 (ISBN 2012790046), p. 135-178.\nAlvin I. Goldman, « A Causal Theory of Knowing », The Journal of Philosophy, vol. 64, no 12,‎ 1967, p. 357–372 (ISSN 0022-362X, DOI 10.2307/2024268, JSTOR 2024268)\nCédric Grimoult, Histoire de l’histoire des sciences. Historiographie de l’évolutionnisme dans le monde francophone, Librairie Droz, « Travaux de Sciences Sociales », 2003,  (ISBN 9782600008280). DOI : 10.3917/droz.grimo.2003.01. [lire en ligne]\nGeorges Gusdorf, « Le savoir romantique de la nature », sur Les classiques des sciences sociales, 1985, p. 395.\nCarl Hempel, Éléments d'épistémologie, Paris, A. Colin, 1972.\nLaurent Jaffro, « « Reid said the business, but Berkeley did it. ». Ferrier interprète de l'immatérialisme », Revue philosophique de la France et de l'étranger, 2010/1 (Tome 135), p. 135-149. DOI : 10.3917/rphi.101.0135. [lire en ligne] \nIsabelle Kalinowski, « Compréhension (sociologie) », sur www.universalis.fr (consulté le 17 janvier 2023) \nAngèle Kremer-Marietti, Philosophie des sciences de la nature, Paris, Éditions L'Harmattan, coll. « Epistémologie et philosophie des sciences », 2007, 2e éd. (1re éd. 1999), 279 p. (ISBN 978-2-296-03548-5, OCLC 181080544, lire en ligne).\nSylvain Lavelle,  « Les actes de connaissance. La pragmatique de la cognition et le problème épistémique de la justification », Revue Philosophique de Louvain. Quatrième série, tome 102, n°3, 2004, p. 477-504, [lire en ligne]. \nDominique Lecourt, L’Épistémologie historique de Gaston Bachelard (1969), 11e éd. augmentée, Paris, Vrin, 2002.\nDominique Lecourt, La Philosophie des sciences, PUF, coll. « Que sais-je ? », 2018, 5e éd. (lire en ligne) \nJean-Louis Le Moigne, Les Épistémologies constructivistes, Paris, PUF, coll. « Que sais-je ? », 1995, 128 p. (ISBN 978-2-13-060681-9).\nJean-Louis Le Moigne, Modélisation des systèmes complexes, Éditions Dunod, 1999, 178 p. (ISBN 978-2-10-004382-8).\nW. J. Mander, The Unknowable: A Study in Nineteenth-Century British Metaphysics, Oxford, Oxford University Press, 2020.\nRobert Nadeau, Philosophie de la connaissance, Les Presses de l'Université de Montréal, 3 août 2016 (ISBN 978-2-7606-3662-0, lire en ligne). \nRichard Parry, « Episteme and Techne », The Stanford Encyclopedia of Philosophy,‎ 2020 (lire en ligne)\nValéry Rasplus (dir.), Sciences et pseudo-sciences : regards des sciences humaines et sociales (avec Raymond Boudon, Gérald Bronner, Pascal Engel, Nicolas Gauvrit, Dominique Lecourt, Régis Meyran, Alexandre Moatti, Romy Sauvayre), Matériologiques, 2014  (ISBN 978-2-919694-70-9)..\nJean-Pierre Olivier de Sardan, « La violence faite aux données », Enquête [En ligne], 3 1996, mis en ligne le 11 juillet 2013, consulté le 11 août 2024. [lire en ligne] ; DOI : https://doi.org/10.4000/enquete.363\nAnne-Françoise Schmid, « La Correspondance Inédite Entre Bertrand Russell et Louis Couturat », Dialectica, vol. 37, no 2,‎ 1983, p. 75–109 (lire en ligne, consulté le 3 mai 2023)\nAnne-françoise Schmid, L'âge de l'épistémologie : science, ingénierie, éthique, 2019 (ISBN 978-2-84174-916-4 et 2-84174-916-9, OCLC 1089340302, lire en ligne)\nLéna Soler (préf. Bernard d'Espagnat), Introduction à l'épistémologie, Paris, Éditions Ellipses, 2000, 2009, 2e éd., 335 p. (ISBN 978-2-7298-4260-4).\nJean Theau, « Remarques sur l’épistémologie française et l’épistémologie américaine », Philosophiques, vol. 3, no 2,‎ 1976, p. 183 (ISSN 0316-2923 et 1492-1391, DOI 10.7202/203053ar, lire en ligne, consulté le 15 juillet 2024).\nRoger Verneaux, Épistémologie générale ou critique de la connaissance, Beauchesne, 1959, 190 p. (ISBN 978-2-7010-0272-9)\nPierre Wagner, « Introduction », dans Pierre Wagner (dir.), Les Philosophes et la science, Paris, Éditions Gallimard, coll. « Folio-essais », 2002 (ISBN 9782070416257), p. 9–65, Présentation de l'ouvrage sur le site de la librairie Gallimard [lire en ligne]. \n\n\n==== Sur des auteurs ou des courants ====\nGregory W. Dawes, « Ancient and Medieval Empiricism », The Stanford Encyclopedia of Philosophy,‎ 2023 (lire en ligne). \n(fr)  Sylvie Mesure, « Dilthey Wilhelm (1833-1911) », Encyclopédie Universalis, site consulté le 7 mars 2020, [lire en ligne] \nMélika Ouelbani, « Les critiques du positivisme logique », dans M. Ouelbani, Le cercle de Vienne, Paris cedex 14, Presses Universitaires de France, « Philosophies », 2006, p. 127-146, [lire en ligne]\nAlexis Philonenko, L'œuvre de Fichte, Paris, J VRIN, 2003, 225 p. (ISBN 2-7116-0866-2, lire en ligne). \nAlexis Philonenko, « Johann Gottlieb Fichte (1762-1814) », sur /www.universalis.fr/encyclopedie (consulté le 20 avril 2023) \nEmmanuel Renault, Hegel : La naturalisation de la dialectique, Paris, Vrin, 2001, 320 p. (ISBN 978-2-7116-1502-5, lire en ligne). \nMaurice Sachot, Parménide d'Élée, fondateur de l'épistémologie et de la science, Strasbourg, 2017 (lire en ligne) \nXavier Tilliette, « L’absolu et la philosophie de Schelling », 41, n° 2, Laval théologique et philosophique, 1985, p. 205-213\nMiklós Vető, « Fichte », dans Dominique Folscheid (dir.), La philosophie allemande de Kant à Heidegger, Presses universitaires de France, coll. « Premier Cycle », 1993 (ISBN 2 13 045256 6), p. 47-74 .\nJean-Claude Vuillemin, « Réflexions sur l’épistémè foucaldienne », Cahiers philosophiques, vol. 130, 2012, p. 39-50.\n\n\n==== Œuvres notables ====\n\nGaston Bachelard, La formation de l'esprit scientifique : contribution à une psychanalyse de la connaissance, Paris, J. Vrin, coll. « Bibliothèque des textes philosophiques », 1993, 305 p. (ISBN 978-2-7116-1150-8, OCLC 1041334358, présentation en ligne).\nGaston Bachelard, Le nouvel esprit scientifique, Paris, Presses universitaires de France, coll. « Quadrige », 2013, 183 p. (ISBN 978-2-13-057655-6, OCLC 894310816).\nPierre Duhem (préf. Paul Brouzeng), Sōzein ta phainomena : essai sur la notion de théorie physique de Platon à Galilée, Paris, Librairie philosophique J. Vrin, coll. « Mathesis », 1990, 143 p. (ISBN 978-2-7116-0805-8, OCLC 31896755).\nGilles Granger, Pour la connaissance philosophique, Paris, Éditions Odile Jacob, 1988, 187 p. (ISBN 978-2-7381-0023-8, OCLC 935608113).\nGilles Gaston Granger, « Vienne (Cercle de) », sur www.universalis.fr (consulté le 17 février 2023)\nGeorg Wilhelm Friedrich Hegel (trad. Bernard Bourgeois), Philosophie de la Nature vol II, Vrin, coll. « Encyclopédie des Sciences philosophiques », 2004 (ISBN 2-7116-1654-1, lire en ligne). \nThomas Samuel Kuhn : La Structure des révolutions scientifiques, Paris, Flammarion, 1962  (ISBN 978-2-08-081115-8). Éd. poche, Paris, Flammarion, 2008, coll. « Champs ».\nAlexandre Koyré, Études galiléennes, Paris, Éditions Hermann, 1939.\nAlexandre Koyré, Du monde clos à l’univers infini, Paris, Gallimard, 1957  (ISBN 978-2-07-071278-6) ; Études d'histoire de la pensée scientifique.\nPaul K. Feyerabend, Contre la méthode : esquisse d'une théorie anarchiste de la connaissance, Paris, Éditions du Seuil, 1979, 350 p. (OCLC 5894367487).\nImre Lakatos, Preuves et Réfutations (en) : Essai sur la logique de la découverte mathématique, Paris, Hermann, 1984.\nJean-Louis Le Moigne et Edgar Morin, L'Intelligence de la complexité, Paris, Éditions L'Harmattan, 1999, 332 p. (ISBN 978-2-7384-8085-9, lire en ligne). \nEdgar Morin, La Méthode, t. 3, La Connaissance de la connaissance, Éditions du Seuil, coll. « Points », 1986, Nouvelle éd.\nJean Piaget, Logique et connaissance scientifique, Éditions Gallimard, coll. « Encyclopédie de la Pléiade », 1967.\nJean Piaget, Introduction à l'épistémologie génétique, Paris, PUF, 1950.\nHenri Poincaré, La Science et l'Hypothèse, Paris, Flammarion, coll. « Champs », 1989, 252 p. (ISBN 978-2-08-081056-4, OCLC 1014100208)\nHenri Poincaré, Science et méthode, Paris, Éd. Kime, coll. « Philosophia scientiae / Cahier spécial », 1999, 253 p. (ISBN 978-2-84174-149-6, OCLC 468368863).\nKarl Popper (trad. de l'anglais par Nicole Thyssen-Rutten et Philippe Devaux, préf. Jacques Monod), La logique de la découverte scientifique [« Logik der Forschung »], Paris, Payot, coll. « Bibliothèque scientifique », 1973, 480 p. (ISBN 978-2-228-88010-7, OCLC 956556197)\nKarl Popper (trad. de l'anglais par Jean-Jacques Rosat), La connaissance objective [« Objective knowledge »], Paris, Flammarion, coll. « Champs » (no 405), 1998, 578 p. (ISBN 978-2-08-081405-0, OCLC 742496309).\n\n\n==== Dictionnaires ====\nBurguière, Dictionnaire des sciences historiques, Presses universitaires de France, 1986 (ISBN 2-13-039361-6 et 978-2-13-039361-0, OCLC 801895455, lire en ligne). ,\n« Sciences » par Pierre Redondi,\nMichel Blay, Dictionnaire des concepts philosophiques, Paris, Larousse, 2013, 880 p. (ISBN 978-2-03-585007-2). \nCatherine Chevalley et Barbara Cassin (dir.), Épistémologie (all. Erkenntnistheorie, angl. epistemology) dans le Vocabulaire européen des philosophies : dictionnaire des intraduisibles, Le Robert, 2004 (ISBN 2-02-030730-8, 978-2-02-030730-7 et 2-85036-580-7, OCLC 60525992, lire en ligne), p. 358–365. \nÉlisabeth Décultot (dir.), Michel Espagne (dir.) et Jacques Le Rider (dir.), Dictionnaire du monde germanique Texte imprimé, Bayard, 2007 (OCLC 1009693477, lire en ligne). \n« Geisteswissenschaften (“sciences de l'esprit”) », Myriam Bienenstock, p. 388-389\n« Philosophie de la nature (Naturphilosophie) », Gilles Marmasse, p. 849-850\nChristian Godin, Dictionnaire de philosophie, Paris, Fayard, 2004, 1534 p. (ISBN 978-2-213-62116-6)\nAndré Lalande, Vocabulaire technique et critique de la philosophie (ISBN 978-2-13-058582-4 et 2-13-058582-5, OCLC 1368855018, lire en ligne). \n« Épistémologie », p. 293-294, « Paradoxe épistémologique », p. 735.\n« Théorie de la connaissance », p. 1129.\nDominique Lecourt et Thomas Bourgeois, Dictionnaire d'histoire et philosophie des sciences, PUF, coll. « Quadrige Dicos Poche », 2006, 4e éd. (1re éd. 1999) (ISBN 978-2-13-054499-9, lire en ligne).\nRobert Nadeau, Vocabulaire technique et analytique de l'épistémologie, Paris, PUF, 1999, 863 p. (ISBN 978-2-13-049109-5, OCLC 474190845).\nJean-François Trinquecoste, Epistémodico : portrait de sciences en mosaïque, 2022 (ISBN 978-2-37687-573-4 et 2-37687-573-0, OCLC 1313901574, lire en ligne), « ÉPISTÉMOLOGIE », p. 141-146. . Présentation de l'ouvrage chez l'éditeur [lire en ligne]\nAlain Rey, Dictionnaire historique de la langue française, t. 1, Dictionnaires Le Robert, 1992 (réimpr. 2000) (ISBN 2-85036-532-7 et 978-2-85036-532-4, OCLC 283735389, lire en ligne). \nEntrée « Épistémologie », p. 1274\nAlain Rey et Josette Rey-Debove, Le nouveau petit Robert: dictionnaire alphabétique et analogique de la langue française, le Robert, 2007 (ISBN 9782849023211)\n\n\n=== Articles connexes ===\n\nPar champ scientifique\n\n\n=== Liens externes ===\n\nRessources relatives à la recherche : Internet Encyclopedia of Philosophy Stanford Encyclopedia of Philosophy  \n\nIntroduction à l'épistémologie par Yannis Delmas.\nMétaphore et connaissance par Jean-Jacques Pinto.\n\n Portail des sciences   Portail de l’histoire des sciences   Portail de la philosophie   Portail de l’histoire"
        }
    ],
    "Technologie": [
        {
            "pageid": 15748906,
            "ns": 0,
            "title": "Faculté des sciences et de la technologie université de Guelma",
            "content": "La Faculté des Sciences et de la Technologie de l'Université 8 mai 1945 de Guelma est une entité universitaire consacrée à l'enseignement et à la recherche dans les domaines des sciences, technologie, architecture, urbanisme et métiers de la ville. Elle est située à Guelma, Algérie.\nLa faculté propose un large éventail de programmes académiques, tant au niveau de premier cycle que de cycles supérieurs, dans des domaines tels que l'automatique, l’électrotechnique, télécommunications, l'électronique, l’hydraulique, la mécanique, l'ingénierie, et bien d'autres. Ces programmes visent à fournir aux étudiants une solide formation théorique et pratique, ainsi que les compétences nécessaires pour faire face aux défis scientifiques et technologiques contemporains.\nEn plus de l'enseignement, la Faculté des Sciences et de la Technologie mène des activités de recherche dans différents domaines scientifiques. Les enseignants et les chercheurs de la faculté sont engagés dans des projets de recherche novateurs, contribuant ainsi à l'avancement des connaissances et à la résolution de problèmes scientifiques et technologiques.\nElle joue un rôle important dans la promotion de l'éducation scientifique et technologique en Algérie. Elle vise à former des professionnels compétents, à encourager la recherche scientifique de qualité et à contribuer au développement socio-économique du pays en favorisant l'innovation et la création de connaissances dans les domaines scientifiques et technologiques.\n\n\n== Historique ==\nLa Faculté a été créée par décret exécutif n° 10-12 du 12 janvier 2010. Elle a été formée de 05 départements, puis la composition des départements a été modifiée par l’ajout du Département d’architecture par la résolution ministérielle n°1135 du 17 novembre 2014.\n\n\n== Formation ==\nLa faculté propose une gamme diversifiée de programmes académiques dans les deux domaines suivants :\n\nDomaine Sciences et Techniques\nDomaine Architecture, Urbanisme et Métiers de la Ville\nLes programmes sont conçus pour offrir aux étudiants une solide formation théorique ainsi que des opportunités de recherche et d'application pratique.\n\n\n== Recherche et contributions ==\nLa Faculté joue un rôle actif dans la recherche scientifique et technologique. Ses professeurs et chercheurs sont engagés dans des projets de recherche novateurs qui contribuent à l'avancement des connaissances et à la résolution de problèmes du monde réel. Ils collaborent également avec des institutions nationales et internationales pour promouvoir l'échange d'idées et de ressources.\n\n\n== Infrastructures et ressources ==\nLa faculté dispose d'installations classiques pour soutenir l'apprentissage et la recherche. Elle abrite des laboratoires équipés d'instruments de pointe, des salles de classe interactives, de bibliothèque spécialisée et des centres de recherche. Les étudiants bénéficient également d'un accès à des ressources en ligne, de programmes d'échange et de stages en entreprise pour enrichir leur expérience éducative.\n\n\n== Partenariats et collaborations ==\nLa Faculté entretient des partenariats étroits avec des entreprises et des organisations du secteur. Ces collaborations favorisent la connexion entre la théorie et la pratique, offrent des opportunités de stage et facilitent l'employabilité des diplômés. De plus, la faculté participe à des projets communautaires visant à promouvoir la sensibilisation scientifique et à contribuer au développement durable de la région.\nElle s'est engagée à fournir une éducation de qualité et à promouvoir l'excellence académique dans les sciences et la technologie. Grâce à ses programmes rigoureux, ses installations de pointe et ses collaborations dynamiques, elle prépare les étudiants à relever les défis du monde moderne et à contribuer à l'avancement des connaissances scientifiques et technologiques.\n\n\n== Laboratoires ==\nLa faculté compte 10 laboratoires de recherches\n\nLaboratoire d'automatique et informatique de Guelma - LAIG\nLaboratoire de génie civil et d'hydraulique - LGCH\nLaboratoire d'analyse industrielle et de génie des matériaux - LAIGM\nLaboratoire de mécanique et de structures - LMS\nLaboratoire de génie électrique de Guelma - LGEG\nLaboratoire des Télécommunications -LT\nProblèmes Inverses : Modélisation, Information et Systèmes-PI : MIS\nLaboratoire des Silicates, Polymères et des Nano composites (LSPN)\nLaboratoire de contrôle avancé (LABCAV)\nLaboratoire de Mécanique Appliquée des Nouveaux Matériaux (LMANM)\n\n\n== Notes et références ==\n\n Portail de l’Algérie   Portail des universités"
        },
        {
            "pageid": 1170850,
            "ns": 0,
            "title": "Technologie",
            "content": "La technologie est, au sens premier, l'étude des outils et des techniques, ce que Jacques Ellul appelle le « discours sur la technique ». Elle s'intéresse à l'art, à l'artisanat, aux métiers, aux sciences appliquées et éventuellement aux connaissances et livre des observations sur l'état de l'art aux diverses périodes historiques, en matière d'outils et de savoir-faire.\nLe terme « technologie » est apparu dans la langue française au XVIIe siècle dans le sens de « traité ou dissertation sur un art, exposé des règles d'un art ». Il a pris depuis cette époque une grande diversité de sens, et subi des mutations sémantiques, notamment sous l'influence de l'anglo-américain technology depuis l'après Seconde Guerre mondiale, son sens étant dès lors souvent confondu, abusivement, avec le sens du terme « technique », constituant dans ce cas un anglicisme sémantique.\nPar extension, le mot désigne les systèmes ou méthodes d'organisation que permettent les diverses techniques, ainsi que tous les domaines d'étude et les produits qui en résultent. Il désigne aussi, particulièrement dans l'enseignement, « l'étude des techniques ». On parle ainsi en France des instituts universitaires de technologie (IUT).\nAujourd'hui, avec le développement considérable des techniques, le terme technologie entretient des liens étroits avec les notions de technique, de technicité et de technocratie.\n\n\n== Étymologie et évolution sémantique ==\nLe mot technologie vient du grec technología (τεχνολογία) téchnē (τέχνη), « art », « compétence », ou « artisanat » et -logos (λόγος), « parole », « langue », capacité de communiquer, et signifie traité sur un art, exposé des règles d'un art, qui traite d'un art ou des règles d'un art.\nL'idée d'une théorie des pratiques techniques (activités agricoles, artisanales, mécaniques…) est très ancienne, mais cette technologia est longtemps restée un discours descriptif plutôt confus.\nDans l'Encyclopædia d'Alsted (1630), la technologia est d'abord une classification des disciplines techniques. En français, le terme technologie apparaît en 1656 pour désigner l'ensemble de termes techniques d'un domaine (approche terminologique).\nAu XVIIIe siècle, en Angleterre et en Allemagne, la technologia désigne d'abord la description et la science des arts (au sens d'artisanat) scientia artium et operatum artis. L'Encyclopédie de Diderot traite de la normalisation du langage des arts comme préalable à une approche scientifique de la technologie. En Allemagne, la technologie devient universitaire en 1772 avec Johann Beckmann de l'Université de Göttingen, l'approche de Beckmann est socio-économique en rapport avec l'organisation sociale. En Angleterre, avec la révolution industrielle, la technologie est la recherche d'une rationalité économique des modernisations techniques (machinisme) dans le cadre du capitalisme.\nAu XIXe siècle, ce qui est appelé technique (issue de la révolution industrielle) est désigné comme technology en anglais, usage qui se répand au XXe siècle, notamment avec le poids croissant des États-Unis : technologie ou technology est alors synonyme de « technique innovante » ou technique de pointe par opposition aux « techniques traditionnelles ». Ce changement de sens, le plus souvent inaperçu, se produit dans les années 1930.\nEn même temps, dans le même contexte culturel, la technologie est aussi une histoire des techniques, rapprochée d'une histoire des sciences et d'une histoire des civilisations. Dans la mesure où les techniques sont partie intégrante du processus d'hominisation, une anthropologie technique ou technologie culturelle se développent au XXe siècle. Il s'agit d'analyser les rapports entre les phénomènes techniques et l'ensemble des phénomènes socio-culturels.\nLe terme technologie est polysémique, pouvant désigner l'étude, la théorie ou la science des techniques (par exemple les techniques lithiques de la préhistoire) ; l'ensemble des savoirs théoriques et pratiques d'un domaine technique (par exemple, les IUT en France) ; les techniques de pointe, modernes et complexes (biotechnologie, nanotechnologie…). L'utilisation du terme technologie comme synonyme de technique est un anglicisme jugé abusif.\n\n\n== Le concept de technology selon Jacob Bigelow ==\n\nC'est semble-t-il un professeur de Harvard, Jacob Bigelow, qui aurait pour la première fois systématisé l'usage du mot technology en anglais dans son ouvrage Elements of technology' (1829). Botaniste et professeur à la chaire Rumford de Harvard consacrée à « l'application de la science aux arts utiles » (useful arts). Appelant à une véritable « fusion » entre les arts et la science, il réfute les savoirs fondamentaux qui ne s’articulent pas avec une pratique concrète et parallèlement les techniques (les arts dans les mots de l'époque) qui s’inscrivent dans une tradition sans le recours systématique au savoir scientifique. En appelant à une sectorialisation accrue des savoirs scientifiques et une répartition scientifique des tâches dans le domaine du travail, il va fournir à la société capitaliste américaine bientôt en expansion un véritable modèle d’éducation. C'est d'ailleurs sur les recommandations du professeur de Harvard que le MIT (Massachusetts Institute of Technology) empruntera son nom[réf. nécessaire], en lieu du « School of Industrial Science » comme prévu dans le projet du fondateur, mais aussi, de nombreuses orientations pédagogiques qui en feront un des centres de recherches « technologiques » les plus performants au monde (dans le domaine de la communication, de l'informatique et aujourd'hui de la robotique et de l'intelligence artificielle).\nLe mot « technology » ne désignait pas pour Bigelow simplement les « arts utiles » mais suggérait en fait la convergence à restaurer à l’aube de la révolution industrielle entre les arts (tekhnê) et la science (logos) : une convergence compromise alors par l'angoisse naissante d'une impossible articulation des savoirs scientifiques se fragmentant avec leur diversification, et des arts nécessairement enfermés dans une tradition (ce que les membres du comité des arts et sciences américain nommaient « une routine empirique »). C'est ainsi que les premiers usages du terme dans le sens qu'en donna Bigelow précédèrent les bouleversements techniques du XIXe siècle, et que l'usage du terme se répandit pendant la révolution industrielle.\nBigelow s'inscrit largement dans le sillage du « millénarisme technologique » qui anime avec ferveur l'enthousiasme scientifique et technique des nations occidentales (pour l'historien David Noble, il faut remonter au moine bénédictin Érigène promoteur d'un salut grâce aux « arts mécaniques »). Millénarisme séculier qui renvoie plus ou moins à l'idée d'un paradis sur terre qui s'incarne désormais dans le progrès technique (idée dont la diffusion est largement redevable aux philosophies progressistes de l'histoire européenne qui émergent au siècle des Lumières). L'une des influences majeures de cette téléologie du progrès technique fut sans aucun doute Francis Bacon : le chancelier d'Angleterre qui a initié la philosophie expérimentale, philosophie inductive qui marque une rupture fondamentale avec les approches scolastiques médiévales de la science (pour qui la nature s'appréhende par le prisme des dogmes de l'Église : la méthode « aprioriste »). Bacon était un fervent millénariste profondément imprégné de la rationalité puritaine (il restera anglican : fonctions obligent...).\n\n\n== Critique du mot ==\nDéjà auteur de deux essais volumineux sur la technique, La Technique ou l'Enjeu du siècle (1954) et Le Système technicien (1977), Jacques Ellul publie en 1988 un autre ouvrage , intitulé Le Bluff technologique. Il y dénonce « l’usage abusif » du mot, qui « imit(e) servilement l’usage américain qui est sans fondement. Le mot « technologie », quel qu’en soit l’emploi moderne des médias, veut dire « discours sur la technique ». Faire une étude sur une technique, faire de la philosophie de la technique ou une sociologie de la technique, donner un enseignement d’ordre technique... voilà la technologie ! » (Le Robert dit effectivement technologie : « étude des techniques »).\nJacques Ellul réserve l’emploi du mot « technologie » au discours (logos) sur la technique (tekné) et refuse de céder à ce qu’il tient pour une « mode langagière » pouvant prêter à confusion.\nC’est sur la base de cet argument qu’en 2012 en France une association d’inspiration ellulienne se donne pour nom « Technologos » et pour devise « penser la technique aujourd’hui ».\nSelon l'historien François Jarrige, le mot « technologie », comme « technique » et « technoscience » recouvre une grande part d’ambivalence et de flou, et il n’existe pas de définition universelle acceptée par tous.\nDans l'encyclique Laudato si' sur la sauvegarde de la maison commune, le pape François utilise à de nombreuses reprises le mot « technologie », dans le sens d'ensemble des outils et des matériels utilisés dans l'économie mondialisée, pour en souligner les risques afférents pour la maison commune et l'humanité : « la critique du nouveau paradigme et des formes de pouvoir qui dérivent de la technologie » (§ 16), « La technologie, liée aux secteurs financiers, qui prétend être l’unique solution aux problèmes, de fait, est ordinairement incapable de voir le mystère des multiples relations qui existent entre les choses, et par conséquent, résout parfois un problème en en créant un autre (§ 20), « La soumission de la politique à la technologie et aux finances se révèle dans l’échec des Sommets mondiaux sur l’environnement » (§ 54), etc.,.\n\n\n== Histoire ==\n\n\n=== Paléolithique ===\nLes premiers représentants du genre Homo sont le résultat d'une évolution à partir d'hominidés qui étaient déjà bipèdes, avec une masse cérébrale d'approximativement un tiers de celle de l'homme moderne. Les outils ont relativement peu évolué durant la plus grande partie de l'histoire humaine. Cependant, il y a environ 50 000 ans un ensemble complexe de comportements et d'utilisations d'outils a émergé. Certains archéologues y voient un lien avec l'émergence du langage structuré.\nLes ancêtres des hommes modernes ont utilisé des outils en pierre bien avant l'émergence d'Homo sapiens il y a 200 000 ans. Les plus anciens outils de pierre connus, regroupés sous le nom de Pré-Oldowayen ou d'Oldowayen, datent d'il y a 2,3 millions d'années. Des traces interprétées par leurs inventeurs comme des traces d'utilisation d'outils ont été observées sur des ossements découverts en Éthiopie dans la Vallée du Grand Rift. Elles datent d'il y a 2,5 millions d'années voire de 3,4 millions d'années,. Ces premières utilisations de la pierre marquent le début du Paléolithique, qui s'achève avec le développement de l'agriculture il y a environ 12 000 ans.\nPour fabriquer les plus simples outils en pierre, un bloc de roche dure aux propriétés mécaniques particulières, comme le silex, devait être frappé avec un percuteur également en pierre de façon à en détacher un éclat. Cette action produit un bord tranchant à la fois sur le bloc taillé et sur l'éclat qui en a été détaché, tous deux pouvant être utilisés comme outils. Les formes les plus simples sont le galet taillé et l'éclat, qui peut être transformé en racloir. Avec ces outils, les premiers humains, chasseurs-cueilleurs, ont pu exécuter différentes tâches, dont la découpe de la viande, la fracture des os pour accéder à la moelle osseuse, la coupe du bois, l'ouverture des noix, le dépouillement des carcasses animales pour récupérer la peau, et, par la suite, la fabrication d'autres outils avec des matériaux plus tendres comme l'os et le bois. Les premiers outils en pierre sont relativement peu élaborés techniquement mais impliquent la maîtrise d'un nombre important de paramètres (choix de la matière première, choix du percuteur, intensité du coup, angle de percussion, etc.) hors de portée de tout espèce animale à l'exception de l'homme. À l'Acheuléen, il y a 1,65 million d'années, de nouvelles méthodes de taille de la pierre apparaissent (façonnage), se traduisant par la production d'outils plus complexes comme le biface ou le hachereau. Il y a 300 000 ans, le Paléolithique moyen est caractérisé par la généralisation du débitage Levallois, permettant la production de séries d'éclats de forme prédéterminée aux dépens d'un même nucléus. Au Paléolithique supérieur, il y a environ 35 000 ans, le débitage de lames se généralise et permet la production de nouveaux outils (burin, grattoirs, etc.). La technique de la retouche par pression (attestée dès le Middle Stone Age en Afrique du Sud) est utilisée pour retoucher certaines pointes de projectiles telles que les feuilles de laurier ou les pointes à cran.\nLa découverte et l'exploitation du feu est un tournant de l'évolution technologique du genre humain. La date exacte de sa découverte est inconnue. La présence d'os d'animaux brûlés dans la région du Cradle of Humankind, à une cinquantaine de kilomètres à l'ouest de Johannesburg, suggère que la domestication du feu est apparue plus d'un million d'années avant le présent. La communauté scientifique est quasi unanime pour considérer que Homo erectus a contrôlé le feu aux alentours de 500 000 à 400 000 avant le présent,. Le feu, alimenté avec du bois, mais également avec du charbon, permettait aux premiers hommes de cuire leur nourriture et d'en améliorer la digestibilité, la durée de conservation et la valeur nutritive en diversifiant la variété des préparations possibles.\n\nD'autres avancées technologiques ont été faites pendant le Paléolithique, ère où sont apparus le vêtement et l'habitation. L'adoption respectivement de ces deux pratiques ne peuvent être exactement datées, mais elles ont l'une et l'autre une position clé dans l'histoire du progrès de l'humanité. Les vêtements, adaptés de la fourrure et des peaux des animaux chassés, aidaient les êtres humains à évoluer plus indifféremment dans des régions plus froides.\n\n\n=== Du Néolithique à l'Antiquité classique ===\n\nL'ascension technologique de l'Homme s'est accélérée lors de la période néolithique (« Nouvel âge de pierre »). L'invention de la hache en pierre polie fut une avancée considérable car elle a permis le défrichement à grande échelle des forêts ouvrant la route future à la création de fermes. La découverte de l'agriculture a permis l'alimentation d'une plus grande population, mais aussi la transition vers un mode de vie sédentaire qui augmenta le nombre d'enfants qui pouvaient être simultanément élevés, vu que les jeunes enfants n'avaient plus besoin d'être transportés comme c'était le cas avec un style de vie nomade. En outre, les enfants pouvaient contribuer aux tâches agricoles de manière plus constante que dans un mode de vie chasseur-cueilleur,.\nL'augmentation de la population et de la force de travail disponible ont autorisé un accroissement de la spécialisation des tâches. Cependant, il reste à clarifier pourquoi et comment des villages néolithiques ont évolué vers les premières villes, comme Uruk, et vers les premières grandes civilisations comme Sumer. On pense cependant que l'émergence de structures sociales de plus en plus hiérarchiques, la spécialisation des tâches, le commerce et la guerre entre cultures adjacentes, et le besoin d'action collective pour surmonter les défis environnementaux, tels que la construction de digues et de réservoirs, ont tous joué un rôle dans cette évolution.\nUne progression continuelle et qui amènera ultérieurement par exemple, au fourneau, et à sa ventilation, a fourni la capacité à fondre et à forger, d'abord les métaux les plus accessibles (ceux qui sont présents dans la nature sous une forme relativement pure). Les premiers métaux ainsi travaillés étaient l'or, le cuivre, l'argent et le plomb. Certains avantages des outils en cuivre sur ceux en pierre, en os, ou en bois sont apparus rapidement et les outils en cuivre ont probablement été utilisés pour la première fois vers le début du Néolithique (environ 8000 av. J.-C.). Le cuivre ne se trouve pas naturellement en grande quantité mais il se trouve assez communément dans le minerai de cuivre et on le transforme assez facilement quand on le brûle à l'aide du feu alimenté au bois et au charbon. À la longue, le travail du métal finira par conduire à la découverte des alliages tels que le bronze et le laiton (vers 4000 av. J.-C.). Les premières utilisations d'alliage de fer tel que l'acier datent d'il y a environ 1 400 ans av. J.-C.\n\n\n=== Moyen Âge et époque moderne ===\n\n\n=== Époque contemporaine ===\n\n\n==== Fossé de genre dans l'enseignement de la technologie ====\nMalgré des améliorations importantes ces dernières décennies, l'enseignement de la technologie n’est pas universellement disponible et les inégalités entre les genres persistent, notamment en raison du fossé entre les sexes dans l’accès, la confiance et l’utilisation de la technologie. Dans l’enseignement supérieur, les femmes ne représentent que 35 % de tous les étudiants inscrits dans des domaines d’études liés aux STEM. Les femmes quittent les disciplines des STEM de façon disproportionnée durant leurs études supérieures, dans leur transition vers le marché du travail et même durant le cycle de leur carrière.\nDes études ont constaté que dans le deuxième cycle du secondaire, les garçons avaient des objectifs de carrière dans le domaine des technologies plus ambitieux que les filles. Une recherche menée parmi des adolescents dans des pays d’Amérique du Nord et d’Europe a constaté que les garçons sont dans une certaine mesure plus enclins que les filles à valoriser les mathématiques, les sciences physiques, les ordinateurs et la technologie. Il a aussi été constaté que les possibilités d’interagir avec la technologie ont un effet sur l’intérêt porté aux sciences par les garçons comme par les filles.\n\n\n== Sens originel ==\n\nL'une des différences majeures entre les sciences de la nature et la technologie est que l'objet d'étude des premières, la nature, est relativement immuable alors que l'objet de la seconde, les techniques, est en perpétuelle expansion. L'invention et l'amélioration sont en effet des éléments exclusifs aux techniques, la nature, sujet d'études des sciences naturelles, ne pouvant pas, par définition, être modifiée ou inventée par l'homme sans perdre son caractère intrinsèque.\n\n\n== Sens dérivés ==\nL'amélioration ou l'invention de techniques ne fait, de manière stricte, pas partie de l'objet de la technologie mais de celui de la recherche technique.\nCertaines expressions sont néanmoins apparues dans l'usage, telles que :\n\nsaut technologique, lorsque la conception des produits techniques subit des évolutions majeures ;\ntransfert de technologie ;\nnouvelles technologies.\n\n\n== Autres espèces animales ==\n\nL'utilisation de proto-technologie est également une caractéristique de certaines espèces animales non humaines.\nL'usage d'outils était autrefois considéré comme une caractéristique définissant le genre Homo. Cette vision a été remplacée après la découverte de preuves d'utilisation d'outils parmi les chimpanzés et d'autres primates, les dauphins, et les corbeaux,. Par exemple, les chercheurs ont observé des chimpanzés sauvages utilisant des outils de base pour la recherche de nourriture, des pilons, des leviers, utilisant des feuilles comme éponges, et de l'écorce ou des lianes comme sondes pour attraper des termites. Les chimpanzés de l'Afrique de l'Ouest utilisent des marteaux et des enclumes en pierre pour casser des noix,, tout comme les sapajous de Boa Vista, au Brésil.\nL'utilisation d'outils n'est pas la seule forme d'utilisation de la technologie animale ; par exemple, les barrage de castors, construits avec des branches ou de grosses pierres, sont une technologie ayant un impact déterminant sur les habitats fluviaux et les écosystèmes.\n\n\n== Citation ==\n« La technique procure ce qu’il faut. La technologie procure ce dont on ignore avoir besoin. » Sylvain Tesson, Blanc, éditions Gallimard, 2022\n\n\n== Notes et références ==\n\n\n=== Notes ===\nCet article est partiellement ou en totalité issu de la page « Déchiffrer le code: l'éducation des filles et des femmes aux sciences, technologie, ingénierie et mathématiques (STEM) » de UNESCO, le texte ayant été placé par l’auteur ou le responsable de publication sous la CC BY-SA 3.0 IGO\n\n\n=== Références ===\n\n\n== Voir aussi ==\n\n\n=== Bibliographie ===\nJacques Ellul, Le bluff technologique, Paris, Hachette, 2012, 3e éd. (1re éd. 1988), 748 p. (ISBN 978-2-8185-0227-3, présentation en ligne).\nAsma Mhalla, « Technopolitique Comment la technologie fait de nous des soldats, Seuil, 2024\nAndré Georges Haudricourt, La technologie science humaine, Recherches d'histoire et d'ethnologie des techniques, éditions des sciences de l'homme Paris, 1987, lire en ligne\nGilbert Simondon, L'Invention dans les techniques, Seuil, Paris, 2005.\nJean-Claude Baudet, De la machine au système : histoire des techniques depuis 1800, Paris, Vuibert, 2004, 600 p. (ISBN 978-2-7117-5324-6, OCLC 255577385).\nJean-Claude Baudet, Le signe de l'humain : une philosophie de la technique, Paris, L'Harmattan, coll. « Ouverture philosophique », 2005, 172 p. (ISBN 978-2-7475-9044-0, OCLC 420539220, lire en ligne).\nYves Lasfargue, Halte aux absurdités technologiques, Paris, Éditions d'Organisation, 2003, 237 p. (ISBN 978-2-7081-2915-3, OCLC 1071378235).\n(en) David Noble, The Religion of Technology : The Divinity of Man and the Spirit of Invention, New York, Penguin Books, 1999 (1re éd. 1997), 273 p. (ISBN 978-0-14-027916-0, OCLC 860819098).\nFrançois Jarrige, Dompter Prométhée : Technologies et socialismes à l'âge romantique (1820-1870), Besançon, Presses universitaires de Franche-Comté, 2016, 288 p. (ISBN 978-2-84867-801-6 et 978-2-84867-560-2, ISSN 2967-8080 et 1771-8988, DOI 10.4000/BOOKS.PUFC.22364)..\n\n\n=== Articles connexes ===\n\n\n=== Liens externes ===\n\nRessources relatives à la santé : History of Modern Biomedicine Medical Subject Headings \nRessource relative à la littérature : The Encyclopedia of Science Fiction \nRessource relative à l'audiovisuel : France 24 \n\nTechnologie dans le Trésor de la Langue Française Informatisé\nDossier technologie sur l'encyclopédie de l'Agora, où il est indiqué que l'introducteur du terme dans la langue anglaise est John Bigelow\nDossier millénarisme dans l'encyclopédie de l'Agora, sur les origines millénaristes du terme, où il est indiqué que l'introducteur du terme dans la langue anglaise est Jacob Bigelow (incohérence dans l'encyclopédie de l'Agora)\nUne bibliographie commentée sur la philosophie contemporaine de la technologie (Michel Puech)\n\n Portail des technologies"
        },
        {
            "pageid": 685173,
            "ns": 0,
            "title": "Actigramme",
            "content": "L'actigramme est l'outil fondamental d'investigation du technicien, mais le champ d'application de l'investigation est infini.\n\n\n== Mise en situation ==\nL’actigramme, fondamental dans l'étude des systèmes, permet d'identifier une matière d’œuvre et d'en définir la valeur ajoutée par différence, à partir de l’observation des entrées et sorties de tout ou partie d’un système.L'Actigramme montre l'utilité d'un produit définit\n\n\n== Méthodologies associées ==\nBien que déjà significatif par lui-même, la véritable puissance de cet outil d'investigation provient de la méthode d'analyse descendante (\"SADT\"), à laquelle il est le plus souvent associé. Lorsqu'à cela, on ajoute encore la notion de chemin critique, alors, on commence à prendre conscience de ce qu'est la rigueur constructive des techniciens.\n\n\n== Précisions méthodologiques ==\nNotion générale d'activité\nNotion générale de niveau - Niveau \"de plus haut niveau\", noté (A - 0) ; niveaux détaillés, notés A- 1), (A - 2), ... (A - n)\nNotion générale de réglages\nNotations conventionnelles : voir article consacré à la SADT\n\n\n== L'actigramme en sociologie ==\nDans le domaine de la sociologie, la présentation de l'actigramme renvoie à des connaissances déjà intégrée pour partie, avec le \"béhaviorisme\", ou cognitivisme.\n\n\n== Historique ==\nBien que l'histoire des sciences nouvelles reste encore à écrire, on peut déjà mentionner la parution officielle de l'actigramme, en France, au BO spécial du 24.09.1992., du MJENR (ministère de la jeunesse, de l'éducation nationale de la recherche)- conformément à l'appellation en vigueur à cette même date.\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nHistoire des sciences et techniques\n\n\n=== Liens externes ===\n Portail du génie mécanique"
        },
        {
            "pageid": 224669,
            "ns": 0,
            "title": "Affichage mécanique",
            "content": "L'affichage mécanique permet d'afficher de l'information à l'aide d'éléments mécaniques.\n\n\n== Types d'affichage ==\n\nL'affichage mécanique statique\nLes affiches\nL'affichage mécanique dynamique\nAiguille\nHorloges et montres\nAppareil de mesure\nCompteurs sur tableaux de bord\nTélégraphe de Chappe\nsémaphore de signalisation des chemins de fer\nBandes déroulantes (Panneau frontal des anciens bus)\n\nAfficheur à palettes (Panneau d'information dans les gares et aéroports ou des radio-réveils comme une horloge à palettes)\nPastilles (Panneau frontal des nouveaux bus)\nVolume à base triangulaire, support de 3 publicités (publicité dans les stades)\n Portail du génie mécanique"
        },
        {
            "pageid": 12822473,
            "ns": 0,
            "title": "Agriculture numérique",
            "content": "L’agriculture numérique se définit comme la convergence de l’agriculture et des technologies de l’information (capteurs, réseaux intelligents, outils de la science de la données, application, voire automatisme et robotique) pour, tout au long de la chaîne de valeurs, améliorer la productivité et répondre aux attentes environnementales et sociétales. Ainsi, le numérique en agriculture mobilise de nombreuses technologies mises en œuvre seules ou en combinaison pour proposer des solutions à destination des agriculteurs.\nL’agriculture numérique est donc le fruit d’une convergence entre une succession de révolutions technologiques et une profonde nécessité de changement face aux nouveaux enjeux de l’agriculture moderne : remise en question de l’usage des produits phytosanitaires, enjeux alimentaires locaux - avec l’alimentation de proximité - et mondiaux, sécurité alimentaire tout au long des filières, transition agroécologique, etc.\n\n\n== Histoire ==\nL’émergence du numérique en agriculture date des années 1970-1980. Au début des années 1970, les programmes de satellites d’observation de la Terre (ERTS puis LANDSAT aux États-Unis) utilisaient des satellites conçus notamment pour répondre aux besoins du département de l’agriculture. Le début des années 1980 a été marqué par l’accès aux systèmes informatiques pour le grand public et le développement de la « télématique » (avec le Minitel et la météo en application-phare pour l’agriculture).\nEn 1983, apparaissent les premiers logiciels conçus pour l’agriculture (comptabilité agricole et gestion de parcelles) qui initient la transition vers le numérique.\nLes images satellites sont d’abord utilisées pour des objectifs d’étude des sols, de cartographie de l’usage des terres, grâce à la création de l’indice NDVI, élaboré en 1973. L’imagerie satellitaire est rapidement associée aux Systèmes d’information géographique (SIG) développés à la fin des années 1960 et qui permettent de superposer plusieurs couches de données sur une carte, données collectées par d’autres moyens (collecte manuelle ou par des capteurs embarqués) et géo-référencées grâce au système GPS, ouvert aux applications civiles dans les années 1980.[réf. nécessaire]\n\n\n=== L’apparition de l’agriculture de précision ===\nCes différents dispositifs, souvent combinés, donnent naissance à l’agriculture de précision (également appelée « gestion intra-parcellaire »), qui voit le jour dans les années 85-90, avec les premières cartes de rendement (1980). À la différence de l’agriculture conventionnelle, dont l’unité de gestion est la parcelle, l’agriculture de précision consiste à répondre aux besoins des plantes à une échelle sub-parcellaire, en déterminant un zonage, chaque zone ayant des besoins spécifiques.\nEn production animale, l’équivalent existe (élevage de précision : la gestion des animaux s’individualise par rapport à une gestion plus uniforme faite au niveau du troupeau).\nAinsi, l’agriculture de précision se construit sur un cycle « observation – diagnostic – préconisation – application ».\nL’agriculture de précision est rendue possible grâce à l'essor des capteurs numériques, leur coût étant par ailleurs devenu plus abordable (c’est la première phase, l’observation). Le rendement est la première unité mesurée. L’une des premières applications de l’agriculture de précision en productions végétales, a été l’intégration de capteurs de quantité récoltée (pesée, volume) dans une moissonneuse-batteuse, pour en déduire un rendement géo-référencé. Côté élevage, les robots de traite apparaissent en 1995 - après la présentation en 1985 du premier robot de traite français, proposé par le Cemagref (aujourd’hui Irstea) - et enregistrent les données de production (volumes produits, puis qualité).\n\n\n=== De l’agriculture de précision à l’agriculture numérique ===\nAu-delà de l’agriculture de précision, l’agriculture numérique est un concept qui apparaît au milieu des années 2010 (rapport agriculture innovation 2025).\nL’agriculture numérique dépasse l’agriculture de précision, concentrée sur la phase opérationnelle de production et la gestion intra-parcellaire ou intra-troupeau et ceci de 2 manières. D’une part, du fait de la multiplication des technologies d’acquisition et d’échange de données, d’autre part du fait des nouveaux services numériques de mise en relation, dont s’emparent les agriculteurs pour s’affranchir des intermédiaires du secteur (commercialisation, formation, échange de savoir et de matériel, locations entre pairs…).[réf. nécessaire]\nAinsi, en agriculture numérique, les TIC sont mises en œuvre à toutes les échelles de la production agricole et de son écosystème :\n\nau niveau de l’exploitation (optimisation des opérations culturales, de la conduite de troupeaux…)\ndans les services d’accompagnement (nouveaux services de conseil agricole basés sur des données collectées automatiquement),\nà des échelles plus larges comme dans un territoire (gestion de l’eau) ou dans une chaîne de valeur (traçabilité, amélioration des intrants comme les semences, meilleure adéquation entre la production et le marché…)\n\n\n== Les enjeux ==\n\n\n=== Traçabilité agricole ===\nLes technologies développées dans le cadre de l'agriculture numérique peuvent former des systèmes de traçabilité agricole numérique, qui permettent aux parties prenantes de suivre les produits agroalimentaires en temps réel. La traçabilité numérique offre un certain nombre d'avantages :\n\nRéduction du gaspillage alimentaire : en 2011 sur toutes les calories alimentaires produites en Europe, 11 % sont gaspillées entre la production à la ferme et la réception auprès des consommateurs. Les systèmes de traçabilité facilitent une meilleure identification des dysfonctionnements du côté de l'offre, où la nourriture est perdue en aval de l'exploitation et gaspillée. Les innovations numériques émergentes, telles que les cartons de lait qui suivent le lait de « la ferme au réfrigérateur », peuvent répondre au gaspillage du côté de la demande en fournissant aux consommateurs des dates de péremption plus précises.\nConfiance des consommateurs : garantir la sécurité, la qualité et l'authenticité des aliments est devenu une exigence réglementaire importante dans les pays à revenu élevé. L'utilisation d'étiquettes RFID pour certifier les caractéristiques des produits agroalimentaires pourrait fournir des signaux de qualité en temps réel aux consommateurs.\nAmélioration du bien-être des producteurs : les producteurs qui peuvent tirer parti de la certification environnementale pourraient vendre leurs produits à un prix supérieur, car les technologies de la chaîne de production pourraient permettre une plus grande confiance dans les labels tels que « durable », « biologique » ou « commerce équitable ».\n\n\n=== Enjeux alimentaires ===\nLa FAO estime que le monde devra produire 56 % de nourriture en plus (par rapport à 2010, dans le cadre d'une croissance \"business as usual\") pour nourrir plus de 9 milliards de personnes en 2050. En outre, le monde est confronté à des défis tels que la malnutrition, le changement climatique, le gaspillage alimentaire et l'évolution des régimes alimentaires. Pour produire un « avenir alimentaire durable », le monde doit augmenter la production alimentaire tout en réduisant les émissions de gaz à effet de serre et en maintenant (ou en réduisant) les terres utilisées pour l'agriculture. L'agriculture numérique pourrait relever ces défis en rendant la chaîne de valeur agricole plus efficace, équitable et durable sur le plan environnemental.\n\n\n=== Enjeux agronomiques ===\nLe numérique, au sens large, impacte le secteur de la production agricole[Interprétation personnelle ?]. Au travers des différentes technologies mises en œuvre, il permet à l’agriculture de gagner en efficience et de la transformer en profondeur.[Interprétation personnelle ?]\nLes masses de données produites par les équipements d’observation, les capteurs installés dans les parcelles agricoles, au sein des troupeaux ou dans les bâtiments permettent une meilleure caractérisation des systèmes agricoles. Leur exploitation à travers des systèmes d’aide à la décision doit permettre de mieux optimiser le pilotage des systèmes de production actuels.\nCes masses de données contribuent également au développement de nouveaux systèmes de production, nécessaire à la transition agroécologique, notamment la diversification des cultures, la transformation des pratiques culturales et l’apparition de nouvelles organisations de travail, tant au niveau des agriculteurs que des décideurs.[réf. nécessaire]\nElles permettent d’améliorer les connaissances du fonctionnement de la plante et des animaux et par conséquent l’amélioration des modèles pour faire des prévisions, anticiper les risques, améliorer la politique de gestion agricole des décideurs ou encore de renforcer les systèmes d’alerte précoce à la sécurité alimentaire.[réf. nécessaire]\nEn particulier, une meilleure caractérisation de l’état sanitaire des cultures peut permettre de réduire l’utilisation des produits phytosanitaires, en optimisant les dates et les doses de traitement et répondre ainsi aux enjeux du plan ECOPHYTO.\nÀ travers la caractérisation comportementale et physiologique des animaux, il est possible[Interprétation personnelle ?] de caractériser de manière plus objective leur état de santé et leur bien-être et ainsi de proposer des systèmes de production pour prendre en compte ce critère. Cela permettrait aussi d’offrir une véritable transparence sur ces aspects à destination des consommateurs.\nPar ailleurs, l’utilisation du potentiel de la télédétection dans la cartographie des zones de pâturages offre[Interprétation personnelle ?] un moyen de médiation pour limiter pour les conflits entre éleveurs et agriculteurs, souvent fréquents dans les pays africains.\n\n\n=== Enjeux économiques ===\nL’agriculture numérique offre la possibilité d’intégrer les avancées technologiques dans l’agriculture, dans le but de diminuer les coûts de production et la quantité d’intrants (eau, énergie, engrais, pesticides, etc.), participant à l’amélioration de la compétitivité de toute la chaîne de production.\nLes outils décisionnels construits à partir des données collectées permettent de contrôler et d’optimiser la quantité des intrants, en accord avec les récentes orientations législatives et environnementales de réduction des produits phytosanitaires.[réf. nécessaire]\nPar exemple, des outils de suivi des opérations culturales permettent ainsi déjà de mesurer précisément la consommation de carburant et ainsi mieux prendre en compte ces éléments dans le raisonnement de cette consommation. En mobilisant les technologies d’autoguidage basées sur le GPS RTK, l’agriculteur peut aussi nettement réduire sa consommation en optimisant son parcours au centimètre près.\nDans le domaine de l’économie collaborative, des plates-formes proposant des places de « marchés numériques » permettent aux producteurs de se rapprocher des consommateurs finaux. Des nouvelles formes de circuits courts pourraient ainsi se développer. Différents acteurs se positionnent déjà sur ce marché. Cela peut également concerner l’échange ou la location de matériel entre agriculteurs.\n\n\n=== Enjeux environnementaux ===\nL’un des principaux enjeux de l’agriculture numérique est d’optimiser le rendement de différents processus agricoles en utilisant le moins possible d'énergie et d'intrants, réduisant ainsi leur empreinte écologique. Dans cet objectif, l’agriculture numérique rejoint l’approche de l'agriculture de précision. L’utilisation de technologies (e.g. drones, capteurs, etc.) permet d’obtenir des indicateurs précis et réguliers sur une exploitation, tels que la vitesse du vent, la pluviométrie ou la nature des sols. Cette connaissance approfondie de l’environnement vise à adapter les pratiques agricoles à la spécificité du milieu de l’exploitation, de l’échelle parcellaire à intra-parcellaire.\nCela peut par exemple faciliter le contrôle de l’utilisation de produits phytosanitaires, la meilleure évaluation du besoin en engrais et en irrigation ou encore la maîtrise du régime alimentaire des animaux, afin de réduire la quantité d’azote rejetée dans les sols.\nL’agriculture numérique est un outil permettant de promouvoir la durabilité et le recyclage du matériel utilisé dans les exploitations par la réduction de l’obsolescence programmée.[réf. nécessaire]\n\n\n=== Renforcer le lien entre agriculteurs et citoyens ===\nLes outils numériques ont un rôle important à jouer dans le renforcement du lien de confiance entre consommateur et agriculteur. Ils accompagnent déjà l’essor massif des circuits courts (utilisés par un agriculteur sur 5 en 2010) via des plateformes qui mettent en relation directe producteur et consommateur. Au-delà de l’acte de vente, ces supports permettent à l’agriculteur de communiquer sur ses méthodes et ses produits, et au consommateur d’accéder aux informations sur la provenance de son alimentation.\nLes réseaux sociaux, sites et blogs, sont de formidables outils de diffusion des connaissances agricoles. En 2016, 82% des agriculteurs visionnaient des vidéos agricoles, dont 22% au moins une fois par jour. Créé en 2014, le site Agriculteurs d’aujourd’hui rassemble et trie les vidéos du web traitant de sujets agricoles, à destination des professionnels comme du grand public. L’offre de sites Internet destinés aux agriculteurs est aujourd’hui grandissante et propose un large éventail de services: de l’achat de fournitures agricoles à la location de matériel entre agriculteurs.[réf. nécessaire]\nL’espace numérique a également permis l’émergence de plateformes de financement participatif destinées à l’agriculture. Ces plateformes sont des alternatives au financement bancaire traditionnel. Elles offrent l’opportunité aux internautes d’investir dans des projets locaux et qui ont du sens.[réf. nécessaire]\nLa plus connue de ces plateformes, Miimosa, a ainsi collecté 12 millions d’euros et financé plus de 2000 projets depuis sa création.\n\n\n=== Enjeux sociaux ===\nL’intégration d’outils numériques dans un système agricole permet à l’agriculteur de diminuer la pénibilité et le temps demandé pour certaines tâches : utilisation de scans pour la création d’inventaires phytosanitaires automatiques, gestion automatisée des tâches de semis et de récolte grâce à la robotique, système de déclenchement d’arrosage à distance, etc.\nDes outils numériques permettent par ailleurs une réduction de la charge cognitive de l’agriculteur, grâce à des applications de gestion automatisée de la logistique et de systèmes d’alerte de risques climatiques, de dysfonctionnements techniques ou d’erreurs humaines : des applications résument les informations importantes pour le suivi des cultures, des capteurs météorologiques peuvent prévenir d’un futur gel, des capteurs intégrés sur le matériel détectent des pannes à venir, l’analyse de données issues de thermomètres connectés à des animaux aident à prédire leur futur vêlage ou maladie, etc.\nCes éléments vont permettre à l’agriculteur une amélioration de ses conditions physiques et mentales, une diminution de son stress et une augmentation de son temps libre, contribuant à son bien-être et à la qualité de ses relations familiales et sociales.\n\n\n== Acteurs et usages ==\nL'agriculture numérique s'adresse à tout type d'agriculture. Les usages sont spécifiques aux besoins aux filières et aux utilisateurs.\nIls varient selon les filières :\n\n\n=== En cultures végétales ===\nen grandes cultures : guidage, télédétection, modulation intra-parcellaire, etc\nen viticulture : modèles de prédiction des maladies, pilotage de la pulvérisation, etc\nen arboriculture et maraichage (fruits et légumes) : logiciels de gestions technico-économiques, robots de désherbage\n\n\n=== En productions animales ===\nen élevage bovin (laitier et viande) : robots de traite, capteurs de chaleur, etc\nen élevage monogastrique (lapins, porcins, volailles...) : capteurs de CO2 et de température, etc\nen élevage caprin et ovin : distributeurs automatiques de concentrés, systèmes de géolocalisation, etc\nd'autres filières comme l'aquaculture ou l'apiculture sont également concernées\nAu sein de chaque filière, différents utilisateurs sont amenés à utiliser des outils et services de l'agriculture numérique :\n\nChefs d'exploitation pour le pilotage des cultures, des élevages et de l'exploitation\nOuvriers agricoles pour l'observation et l'application au champ\nConseillers agricoles pour l'observation et la réalisation des préconisations\nCUMA / ETA pour la gestion du matériel agricole et du personnel\nStructures collectives (Coopératives, chambres d'agriculture, négoce, etc) pour la gestion de la production à l'Échelle d'un territoire\nStructures d'essais, laboratoires de recherche (voir Ecosystèmes de l’innovation) pour la validation de modèles ou expérimentations variétales\n\n\n=== Impacts sur les exploitations et les compétences de l'utilisateur ===\nL'arrivée des outils numériques engendre des aspects positifs et négatifs pour les exploitations.\nLes utilisateurs finaux confirment un gain en précision et en efficacité[réf. nécessaire], ainsi qu’une meilleure anticipation des différentes tâches à accomplir, en fonction des données collectées et analysées par les outils.\nDe plus, une fois maîtrisés, ces outils permettent une augmentation de la traçabilité, de la qualité des produits, du confort de travail, du respect des réglementations et de la simplification globale de la gestion de l'exploitation.[réf. nécessaire]\nIls permettent ainsi de moderniser l'image d'une exploitation et parfois de lui redonner un nouveau souffle.[réf. nécessaire]\nSi le prix des équipements et services est trop élevé[Interprétation personnelle ?] pour certains agriculteurs, ces équipements apportent néanmoins un gain significatif sur les coûts des charges (produits, intrants et alimentation) et de main-d’œuvre.\nLa connaissance de l'animal, la prévention des risques sanitaires et le respect de l'environnement sont, par la même occasion, améliorés.\nCependant, le manque de communication, l'éloignement entre les entreprises, les acteurs de l'innovation et les utilisateurs finaux conduisent à un manque d'informations sur le marché disponible et sur les possibilités offertes par les outils numériques.[réf. nécessaire]\nPar ailleurs, les utilisateurs se sentent submergés par la vague d'offres en outils et services.[Interprétation personnelle ?] Des craintes sur la complexité (manque d'interopérabilité entre les outils), la fiabilité et la dépendance aux outils numériques sont également apparues.[réf. nécessaire]\nDe plus, avec l'apparition de ces nouveaux outils, les utilisateurs ont besoin d'acquérir de nouvelles compétences pour leur déploiement et leur utilisation, compétences pour lesquelles ils pourront être formés auprès des entreprises (et/ou des collectivités) qui leur fournissent le matériel.[réf. nécessaire]\nMais plus que d'une simple formation, les utilisateurs font remonter un besoin de conseils personnalisés et d'un accompagnement pour le déploiement de ces outils sur leur exploitation.[réf. nécessaire]\n\n\n=== Services et outils numériques pour l’agriculture ===\nLe développement de services et outils numériques a considérablement favorisé l’essor de nouvelles sources d’informations en agriculture[réf. nécessaire]. L’ensemble des services/outils numériques aident à recueillir les données pour apporter de l’information qui permettent par la suite de conseiller ou d’orienter l’action ou le traitement agricole (irrigation, traitement pesticide, etc.). L’acquisition de ces informations est possible grâce à l’utilisation d’objets connectés (stations météo, smartphones) et de capteurs (imagerie, Lidar, etc.). Ces technologies permettent d’obtenir des informations sur le suivi de cultures (maladies, demande évaporative) ou de cheptel.\nParallèlement, les vecteurs mobiles (drone, tracteur, moissonneuse, robot…) sont des technologies qui s’améliorent permettant d’embarquer de plus en plus de capteurs (imagerie, Lidar, GPS RTK). Ce développement rend accessibles des informations à la fois précises et géolocalisées.\nLes services de type imagerie satellitaire (Sentinel), généralement développés pour d’autres secteurs d’activités, permettent le suivi de cultures à l’échelle d’un champ. L’utilisation de données mutualisées à travers les réseaux sociaux ou des applications smartphones permettent d’étudier l’information de manière régionalisée.\n\n\n=== La réglementation des données ===\nCompte tenu des enjeux économiques importants autour de la donnée agricole, et en l’absence de réglementation sur la maîtrise de l’usage des données en France, le contrat est la seule alternative permettant de rétablir les rapports de force dans les relations commerciales entre les acteurs de l’agriculture numérique.\nPlusieurs initiatives proposent des recommandations visant à favoriser les échanges de données, tout en les encadrant. Certains sont également allés plus loin en proposant de faire exister un cadre autour de la donnée, il s’agit de soft law. Par conséquent, elle ne lie juridiquement que ceux qui s’y soumettent, la contrainte est d’ordre social et économique. On a différents exemples de chartes établies dans le secteur agricole aux États-Unis, en France et en Europe.\nLes chartes sur les données agricoles ont plusieurs points communs :\n\nElles s’inscrivent dans une démarche volontaire d’auto-réglementation ;\nElles sont fondées sur des principes (résultats des pratiques en matière de données agricoles plutôt que sur le processus par lesquels cela doit être réalisé) ;\nElles ont été préparées par une combinaison d’acteurs (associations ou syndicats d’agriculteurs, de fournisseurs de technologies agricoles, de machines et d’intrants) ;\nElles s’articulent autour de trois points communs fondamentaux: le consentement, la divulgation et la transparence.\nL’adoption des chartes relatives aux données agricoles n’est pas encore assez large pour évaluer leur succès jusqu’à présent. On peut néanmoins souligner certains aspects positifs clés des chartes : (1) Elles renforcent la confiance. (2) Elles comblent des lacunes juridiques. (3) Elles simplifient l’évaluation des comportements (surtout quand elles sont accompagnées d’une certaine forme de certification). (4) Elles sensibilisent (tant les fournisseurs de technologies que les agriculteurs). (5) Elles favorisent la participation et l’inclusion (élaborées conjointement par différentes organisations représentant les parties prenantes concernées ; cela à son tour favorise la confiance et accroît la crédibilité).\n\n\n== Les technologies numériques en agriculture et leurs fonctions ==\nLes technologies mobilisées en agriculture numérique sont nombreuses et diverses. Elles regroupent les technologies de l’information et de la communication (TICs) au sens large avec notamment : les objets connectés et l’Internet des objets (IoT), les capteurs embarqués, les technologies satellitaires ou aéroportées (drones, ULM), les applications smartphones, les réseaux sociaux, ou encore les robots. Le numérique – ou ‘digital’ – renvoie au traitement de l’information via des opérations logiques. Ainsi, les technologies numériques permettent l’acquisition, le stockage, l’échange, la gestion des données de l’agriculture ainsi que leur traitement, analyse et modélisation. Par l’intermédiaire de ces processus, les TICs rendent possible la production et la mise en circulation d’informations entre acteurs du monde agricole : agriculteurs, conseillers, formateurs, entreprises, consommateurs etc.\n\n\n=== Outils et méthodes utilisés en agriculture numérique ===\n\n\n==== Acquisition des données ====\nLes données sont principalement acquises par deux moyens, soit de manière automatique, soit de manière plus ou moins manuelle. L’automatisation de l’acquisition des données met en œuvre des capteurs, qu’ils soient portatifs, statiques (station météo connectée, radar de pluie, piège connecté, quantité/ qualité du lait, etc.), implantés ou portés pour le suivi des animaux (capteurs de chaleurs, de monitoring santé etc des ovins-bovins)  ou embarqués sur matériel agricole (détection des mauvaises herbes, estimation du besoin d’azote…), sur des vecteurs aériens de type avion et drone ou satellite. Ils délivrent un signal ou une image qui sont spécifiques du phénomène observé ou au contraire qui sont très génériques (température, humidité, pression etc.), et qui – pour atteindre la grandeur recherchée - nourrissent un modèle spécifique. L’alternative à l’automatisation de la collecte est la saisie manuelle. Le smartphone est un dispositif très intéressant et de plus en plus utilisé car bien fourni en capteurs (appareil photo – voir application PixFruit pour déterminer le rendement des manguiers, géolocalisation, accéléromètre…) et facile à utiliser en tant que terminal de saisie par les agriculteurs (observation des maladies ou des ravageurs au champ, suivi de maturité, suivi de contrainte hydrique comme l’application Apex Vigne pour déterminer la contrainte hydrique de la vigne etc.).\nIl faut noter que les données acquises pour l’agriculture et en agriculture sont de plus en plus nombreuses. Le déploiement des objets connectés - équipés de capteurs et géo-localisés qui communiquent les données collectées via internet - et de l’internet des objets, ou IoT pour « Internet of things » émerge au début des années 2010. C’est - avec les images satellitaires et le phénotypage haut débit - un levier fort de cette massification. Le développement des objets connectés touche tous les secteurs avec selon le Digiworld institute - une forte attractivité de 3 domaines (utilities, automobile, et électronique grand public) – et 36 milliards d’objets connectés dans le monde en 2030 (Digiworld Yearbook 2017). En agriculture, des objets comme un smartphone, tracteur, un outil agricole tracté, une station météo, un piège à insectes etc., délivreront en temps réel les informations utiles à la gestion. Cette massification permettra rapidement de constituer des méga-données – ou Big Data – pouvant être analysées pour fournir à l’agriculteur des outils d’aide à la décision.\n\n\n==== Stockage et transfert ====\nLes données et informations acquises ou observées peuvent être reportées et échangées via des plateformes (réseaux sociaux, logiciel, etc.) disponibles sur smartphone ou tablette. Les échanges, stockages et transferts des données sont de plus en plus complexes du fait du volume croissant des données, de leur hétérogénéité (différentes sources), de leur complexité (différents référentiels) et des questions liées à la confidentialité. Cependant, la donnée seule n’est pas intéressante en soi mais c’est sa compilation avec un grand nombre de données et son traitement qui peut en faire ressortir de l’information. D’où le besoin de créer des plateformes pour faciliter ces échanges et rendre ces données accessibles. Un tel portail ouvrant les données agricoles favoriserait l’innovation ouverte (rapport Bournigal, 2017). En France comme ailleurs, des infrastructures et des entreprises émergent pour faciliter les échanges de données agricoles et encourager l’innovation ouverte à partir de ces données: en Australie avec AgReFed, aux Pays-Bas avec JoinData, en France avec API-Agro.\n\n\n==== Traitement, analyse des données et aide à la décision ====\nLes données peuvent être à la fois utilisées pour l’étalonnage, l’ajustement et la validation des modèles existants et pour la création des modèles représentant les phénomènes agronomiques mais aussi économiques, climatiques etc. La nouveauté est le caractère plus en plus massif des données, desquelles on infère des informations et des modèles en utilisant des méthodes mathématiques classiques (régression, classification, etc.), l’intelligence artificielle, et si les données sont trop abondantes, les méthodes de traitement du Big data. Dans ces approches d’inférence de modèles, qui s’opposent à la construction mécaniste des modèles, l’enjeu est alors de trouver des règles qui ont un sens du point de vue de l’homme de l’art. Ceci n’est pas trivial car avec ces méthodes, de type « boite noire », les relations entre paramètres ne sont pas explicites. L’analyse des données peut servir plusieurs fonctions.\nEn 2017, le think tank Villa numeris a organisé une Vision Camp #ImagineAgri, consacrée à une réflexion prospective sur les enjeux et besoins de l'« agriculture connectée », et à proposer des solutions. Cela a notamment débouché sur la création d'un portail de données agricoles (intégré dans le programme « Agriculture-innovation 2025 ») et préparé par un rapport remis par Jean-Marc Bournigal (Irstea) en janvier 2017 à Stéphane Le Foll (ministre de l'Agriculture), et à Axelle Lemaire) (secrétaire d'État au numérique et à l'innovation),.\nDes applications comme PlantVillage et Virtual Agronomist permettent à des agriculteurs, avec peu de moyens, de faire des diagnostics de maladies ou de besoins en intrants.\n\n\n==== Échanges et communications ====\nLe dernier maillon de la chaîne de traitement des données est la communication et l’échange des informations, connaissances, recommandations ou décisions tirées de l’analyse. Les modèles inférés, par les statistiques ou l’intelligence artificielle, ne sont pas appropriables directement par les acteurs (agriculteurs, commerçants, etc.), les 2 écueils étant trop de variables à renseigner et des sorties incompréhensibles. Les informations peuvent être transmises sous forme de recommandations (ex : date de traitement), d’indicateurs bruts (ex : température moyenne) ou de distribution (ex : cartographie). La transmission de ces informations peut se faire au travers d’outils d’aide à la décision (logiciels, application, alerte sms, etc..) qui alimentent des systèmes automatiques (ex : pilotage automatique de l’irrigation) ou qui sont soumis à l’agriculteur pour qu’il prenne lui-même la décision par rapport à la recommandation. L’aide à la décision est la fonction la plus régulièrement citée et la plus mise en œuvre par les entreprises quand il s’agit de l’analyse de données en agriculture. Cependant le résultat de cette analyse peut être autre chose qu’une aide à la décision. Cela peut servir à de l’évaluation a posteriori, à de la réflexion, à une prise de recul. Cela pourrait être utilisé pour de la capitalisation de connaissances, à de nouvelles formes d’expérimentations agronomiques.\nPar ailleurs les transferts de données ne sont pas forcément associés à de l’analyse. Des données ‘brutes’ peuvent être transmises, on pense notamment à tout ce qui est traçabilité.\nOn notera toutefois que la variété des formes d’échanges d’informations en agriculture ne saurait se réduire aux complexes processus d’acquisition, de traitement et de transmission de données numériques décrits jusqu’à présent. En effet, ces échanges peuvent aussi s’opérer suivant des circuits de communication plus directs. C’est le cas notamment des échanges entre agriculteurs, qui au moyen de leurs smartphones produisent eux-mêmes des informations sur leurs activités et les partagent à un public plus ou moins ouvert sur les réseaux sociaux. Les technologies utilisées sont alors beaucoup plus ordinaires (smartphones équipés de caméra) et les données transférées sous forme de captations audiovisuelles sont le plus souvent montées et accompagnées par un discours (texte et/ou prise de parole) qui participe de les rendre intelligibles. Ainsi, les technologies numériques génériques peuvent servir à échanger des connaissances mais aussi de biens et de services.\n\n\n=== Domaines d’application des technologies ===\nDepuis le milieu des années 2010, de nombreuses start-up ont été créées dans le domaine de l’agriculture numérique. Il s’agit principalement de sociétés de service, qui produisent un conseil à partir de données collectées dans les champs, qui connectent les agriculteurs et des tiers (agriculteurs, consommateurs…) pour vendre/ acheter des produits et services ou qui proposent des services et des technologies disruptives (robots). Ils sont regroupés dans une association, La Ferme Digitale. Au-delà des technologies proposées par ces start-up, des technologies numériques, allant du simple site Internet au tracteur connecté, sont aussi mises sur le marché par des entreprises d’agrofournitures ainsi que par des organisations professionnelles agricoles, des associations, des organisations publiques ou parapubliques. Le numérique en agriculture comprend aussi le numérique non spécifique à l’agriculture mais utilisé dans ce domaine, avec des technologies proposées alors par les acteurs classiques du numérique.\n\n\n==== Production végétale ou animale ====\nDe nombreuses technologies numériques sont développées pour la production végétale et animale en tant que telle. Certaines technologies permettent de collecter des données pour connaitre l’état de l’environnement et des productions (plantes, animaux) : sont concernés les sols (hygrométrie, texture, teneur en azote etc.), les plantes (stress hydrique, maladies, croissance), l’environnement (météo, qualité de l’eau, qualité de l’air), les animaux (détection des chaleurs, santé, alimentation etc.). Par exemple, plusieurs entreprises proposent des stations météo connectées équipées de pluviomètre, d’anémomètre, de thermomètre, d’hygromètre etc.\nLa donnée collectée peut être traitée pour fournir des indicateurs, des préconisations, ou pour automatiquement régler le fonctionnement de l’outil. Ainsi, une des utilisations est l’agriculture de précision où l’apport des intrants (produits phytosanitaires, engrais, irrigation, semences, alimentation animale, traitements pharmaceutiques) est piloté - en quantité, en qualité - selon des caractéristiques spatiales et temporelles mesurées.\nPour ce qui concerne la production au sens strict, il y a également tous les outils de robotique et d’automatisation : robot de traite, distributeur automatique d’aliment, robot de désherbage etc., les outils de géolocalisation, etc.\n\n\n==== Recherche et développement ====\nLe numérique en agriculture peut être utilisé pour la recherche et le développement, apportant de nouvelles connaissances mais aussi sûrement de nouvelles manières de faire et d’organiser la recherche et le développement. Les données collectées sur les parcelles, en conditions réelles, peuvent servir à alimenter des thématiques de recherches. Par ailleurs, on constate le développement des sciences participatives, notamment sur tout ce qui concerne la biodiversité et l’environnement. Un domaine de recherche particulièrement touché par le développement du numérique est la génétique.\nSélection variétale accélérée par le phénotypage haut-débit\nLe phénotypage haut-débit est un verrou technologique à lever pour accélérer la sélection de variétés plus adaptées aux nouvelles conditions de climat et de marché et moins consommatrices en intrants. L’objectif est de caractériser des collections de génotypes de plantes en fonction de leur réponse à divers scénarios environnementaux associés aux changements climatiques. Tout l’enjeu est de faciliter l’identification de gènes d’intérêt agronomique afin de sélectionner les plantes pour des systèmes de culture innovants, à bas niveau d’intrants, ou de mieux bénéficier de leur diversité génétique. Les technologies haut-débit ont ici un potentiel important et viennent bousculer les manières de faire de la sélection variétale.\n\n\n==== Outils d’information et de formation ====\nNe l’oublions pas, le numérique est avant tout un outil qui permet de transférer de l’information. Et avoir accès à de l’information en agriculture est déjà très utile et parfois compliqué. Le numérique peut faciliter cet accès-là, que ce soit pour des données météo, des données techniques, réglementaires ou économiques. Le développement du secteur de la formation digitale impacte aussi le secteur agricole. Grâce à Internet, à la vidéo et aux simulateurs 3D, de nouvelles offres de formations se développent. On peut citer Icosystème, une plateforme numérique de formation en ligne en agro-écologie, Ver de Terre Production ou encore Le Mas Numérique et sa visite virtuelle.\n\n\n==== Outils de communication et de collaboration entre acteurs du monde agricole ====\nAvec Internet et les NTIC, le numérique permet de créer facilement la mise en relation entre producteurs, acteurs du monde agricole, ou encore consommateurs.\nParmi les outils de mise en relation entre agriculteurs et acteurs du monde agricole (prestataires, fournisseurs), citons des exemples comme LinkinFarm (plateforme numérique de mise en relation entre agriculteurs et prestataires de travaux agricoles), Wefarmup.com ou VotreMachine.com (sites de location de matériel agricole), ou encore Agriconomie, un site internet qui permet (i) aux agriculteurs de trouver tout ce dont ils ont besoin pour leur exploitation au même endroit et au meilleur prix, afin de leur faire économiser du temps et de l’argent ; (ii) aux distributeurs et fournisseurs d’étendre leur périmètre géographique de ventes et de proposer leurs offres à un plus grand nombre d’agriculteurs en Europe, à moindre coût. Au-delà de ces services marchands, les NTIC peuvent servir à communiquer et coopérer pour la gouvernance des structures collectives, à co-construire et diffuser des connaissances, à se coordonner entre acteurs, à mettre en œuvre des projets etc. Une liste exhaustive ne peut être réalisée, les potentiels de coopération via les NTIC étant multiples. Le numérique peut permettre de mettre en relations les agriculteurs entre eux et notamment permettre le partage de connaissances, d’expériences et de services entre agriculteurs.\n\n\n==== La valorisation des productions ====\nLes technologies numériques en agriculture servent aussi à la valorisation de la production.\nLes NTIC se développent pour des échanges entre les agriculteurs et le reste de la société. Elles sont également utilisées pour vendre ou mieux vendre sa production. Plusieurs exemples : Connecting Food, un site Internet qui permet une transparence pour les consommateurs des processus de production de leur alimentation, Les Grappes qui mettent en relation vignerons et consommateurs, Miimosa, une plateforme de financement participatif de projets agricoles, Panier Local, un outil qui propose la gestion de l'ensemble des tâches liées à la commercialisation (prise de commande, stock, préparation, livraison, facturation, etc), ou encore Poiscailles, un site qui permet d’acheter en circuit court des produits de la mer. Sur un autre registre, le site Comparateur Agricole est une place de marché en ligne pour la vente de céréales à la tonne ou l’application Captain Farmer fournit une aide pour vendre au meilleur prix sur les marchés.\nPour valoriser la production, les outils de traçabilité numérique se sont développés et se complexifient avec notamment la technologie de la blockchain.\n\n\n==== Traçabilité - Gestion globale- Logistique ====\nEnfin, le numérique est et sera de plus en plus utilisé pour la gestion globale de l’exploitation agricole. Notamment, les exigences réglementaires de traçabilité sont fortes – concernant les apports d’intrant par exemple – et le temps passé par les agriculteurs pour les respecter est important, de 5 à 10 h par semaine, pour l’ensemble des tâches administratives. Le numérique permet d’automatiser certaines entrées de données de traçabilité, de les rassembler, les organiser, les sauvegarder. Cependant, les nouveaux outils numériques requièrent souvent de la saisie manuelle et de la manutention, ce qui peut amener de nouvelles tâches à effectuer (et donc du temps passé en plus). Ainsi, numérique et gain de temps ne sont pas toujours associés.\nAu-delà du respect des exigences réglementaires, le numérique peut servir à rassembler de la donnée afin de gérer son parcellaire et ses rotations, de gérer son parc matériel, ses stocks, d’organiser la main-d’œuvre, la logistique et les différentes tâches (ex du système Keyfield qui facilite la saisie d’opérations agricoles avec un scanner RFID ou le logiciel Agreo de Smag, cahier de culture électronique). Avoir une traçabilité des tâches effectuées permet de constituer un capital informationnel, de calculer des coûts de production et d’apprendre à partir des années passées.\nLe numérique est aussi là pour diminuer la charge mentale des agriculteurs, leur niveau de stress : le succès du GPS a largement été dû au fait que les agriculteurs pouvaient relâcher leur concentration lors des labours ou autres tâches répétitives. Les systèmes d’alerte (chaleurs ou de vêlage pour les vaches, fuites d’eau, portes ouvertes…) facilitent la gestion et permettent de maîtriser les gros risques. C’est la promesse de sociétés comme Ekylibre. Cependant, les effets induits par l’utilisation de ces technologies ne correspondent pas toujours aux effets attendus. Par exemple, l’utilisation de ces technologies numériques en élevage amène parfois à un stress et une charge mentale plus importants pour les éleveurs.\nL’anticipation et la gestion des risques est l’un est des domaines avec lesquels le numérique est attendu.\nIl ne faut pas oublier non plus le numérique qui sert pour les services bancaires, la comptabilité, les déclarations administratives, les demandes d’aide.\nGlobalement, les outils numériques peuvent intervenir dans différentes fonctions et à différentes échelles, allant de la plante au territoire. Ils peuvent être utilisés directement par les exploitants, mais également par d’autres acteurs du secteur, que ce soient les conseillers, les fournisseurs, le secteur aval, etc. Un des points de vigilance est que les effets promis par ceux qui proposent ces technologies peuvent être différents des effets constatés lors de l’usage en agriculture. Où peuvent avoir des effets différents selon le contexte dans lequel la technologie est utilisée. Ainsi, l’utilisation du numérique doit être raisonnée en fonction d’un ensemble de paramètres tels que les objectifs recherchés, le contexte socio-économique et environnemental, les compétences mobilisables, etc. De plus, le numérique en agriculture a de nombreuses potentialités, mais ne peut pas être la solution à tout type de problème et en toutes circonstances. Le numérique apporte un ensemble d’outils, mobilisables parmi d’autres types d’outils. L’articulation entre technologies, numériques ou non, est un élément important à considérer pour un raisonnement global et cohérent des systèmes agricoles.\n\n\n== Écosystème de l'innovation ==\n\n\n=== Recherche & Enseignement ===\n\n\n==== En France ====\nAu niveau français, tous les grands Instituts agronomiques sont impliqués dans l’agriculture numérique : l'Inra, le Cirad, Acta - les instituts techniques, ou encore Irstea. Afin de réunir différents acteurs de la recherche en agriculture numérique, l’Institut Convergences Agriculture Numérique #DigitAg a été créé en 2017. Il réunit 17 partenaires publics et privés et a pour vocation de créer un socle de connaissance pour permettre le déploiement de l’agriculture numérique en France et dans le monde. La chaire AgroTIC est l’une chaire d’entreprises permettant l’association d’établissements d’enseignement et de recherche (Montpellier SupAgro, Bordeaux Sciences Agro et Irstea) avec des acteurs socio-économiques dans le domaine de l’agriculture numérique. Montpellier SupAgro, l’ESA d’Angers et Bordeaux Sciences Agro proposent des formations en agriculture numérique\n\n\n==== A l’international ====\nEn Hollande, l’Université de Wageningen s’engage également dans la recherche en agriculture de précision et numérique, à l’instar de l’Université de Californie Davis aux États-Unis. En Australie, de nombreux acteurs sont impliqués dans l’agriculture numérique comme les universités du Queensland, Griffith, Curtin, le CSIRO ou encore Food Agility.\n\n\n=== Les entreprises et startups ===\nDepuis quelques années, on assiste à l’émergence d’un nouvel écosystème de startups œuvrant dans l’AgTech, marqué par des levées de fonds dépassant la centaine de millions d’euros, comme avec l’entreprise Ynsect (125 millions d’euros en 2019). Selon le media AgFunder news, ces levées ont augmenté de 550 % sur les six dernières années, ce qui est un indicateur important du dynamisme du secteur.\nCes startups sont accompagnées par des incubateurs spécialisés dans les thématiques agricoles. Aux États-Unis, les plus gros d’entre eux se situent en Californie (Terra, PNP) ou plus proche des centres traditionnels de production comme the Yield Lab (St Louis, Missouri). En France, Euratechnologies a une antenne consacrée à l’AgTech, et des réseaux d’accélérateurs comme Le Village by CA permettent à des startups de se développer à proximité des agriculteurs. En France, certaines de celles-ci s’organisent en association comme la Ferme Digitale afin de promouvoir l’innovation et le numérique dans l’agriculture, par exemple à travers des événements comme les LFDays.\nToujours en France, la plupart des acteurs privés de l’agriculture numérique sont regroupés au sein de la Chaire AgroTIC.L'agriculture numérique a suscité l'intérêt d'entreprises historiques de différents domaines tels que l'électronique, le développement de logiciels et les producteurs de produits agricoles. Par exemple, les grands acteurs du marché de la sélection et de la gestion des cultures, tels que Bayer et Syngenta, ou bien des agroéquipementiers comme John Deere  ont commencé à fournir des services numériques aux agriculteurs. Leur rôle est important par leur capacité à intégrer des entreprises innovantes dans leur cœur de métier (rachat de Climate corporation par Monsanto/Bayer, rachat de Blue river Technology par John Deere).\nCertaines entreprises du numérique commencent également à s’investir dans l’agriculture numérique, identifiée comme un levier de croissance potentiel. Microsoft a lancé l’initiative “AI for Earth” ainsi que Farm Beats avec pour but de développer l’utilisation de l’intelligence artificielle pour le développement durable et l’agriculture. Enfin, Bosch développe aussi des capteurs connectés pour l’agriculture.\n\n\n=== Living Lab ===\nPlusieurs Living labs se sont développés ces dernières années pour promouvoir le développement de l'agriculture numérique.\nIl y a par exemple l'Institut BioSense en Serbie, le Yeesal Agrihub à Thiès (Sénégal) ou encore le projet OccitaNum qui vise à faire de l’Occitanie le leader de l’agriculture et de l’alimentation de demain en mobilisant les technologies numériques dans une approche d’innovation ouverte.\n\n\n== Risques et controverses ==\nL’agriculture numérique est un domaine qui évolue très rapidement, tout comme les multiples controverses qui y sont liées. Pour illustration, les 24es Controverses européennes de Bergerac, organisées en juillet 2018 se sont interrogées sur “L’agriculture augmentée : quels impacts sociaux et économiques ?”.\nSont énumérées ci-dessous les principales controverses :\n\n\n=== Automatisation du travail sur l’exploitation agricole ===\n\n\n=== Individuation des traitements sur les plantes et les animaux ===\n\n\n=== Acquisition des connaissances sur l’exploitation ===\n\n\n=== Gestion de données ===\n\n\n=== Un secteur en constante évolution ===\n\n\n=== Impact environnemental ===\n\n\n=== Agriculture numérique dans les pays en développement ===\n\n\n== Références ==\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nAgriculture\nAgriculture de précision\nAgriculture durable\nTransition écologique\nAgroécologie\nTechnologies de l'information et de la communication\nTransformation numérique\n Portail de l’agriculture et l’agronomie"
        },
        {
            "pageid": 16348078,
            "ns": 0,
            "title": "Airgo Networks",
            "content": "Airgo Networks (anciennement Woodside Networks) est une société basée à Palo Alto, en Californie, spécialisée dans le développement de la technologie sans fil à entrées et sorties multiples (MIMO). Airgo Networks a été fondée en 2001 par Gregory Raleigh, V.K. Jones, David Johnson, Geert Awater, Rolf de Vegt et Richard van Nee.\nAirgo a été l'un des principaux promoteurs de la norme 802.11n. La société a commencé à livrer les premiers jeux de puces MIMO-OFDM au monde en 2003.\nEn septembre 2005, Airgo Networks a lancé son jeu de puces True MIMO de troisième génération, qui prend en charge des débits de données allant jusqu'à 240 Mbit/s.\nLe 3 décembre 2006, Qualcomm a annoncé l'acquisition d'Airgo Networks pour un montant non divulgué,. Airgo appartient désormais à Qualcomm, bien qu'elle soit toujours située à Palo Alto, tandis que le siège de Qualcomm se trouve à San Diego.\n\n\n== Références ==\n\n Portail des technologies   Portail de l’informatique"
        },
        {
            "pageid": 210530,
            "ns": 0,
            "title": "Alimentation par le sol",
            "content": "L'alimentation électrique par le sol (APS) est une méthode d'alimentation pour tramways se substituant, dans certaines zones de centre-ville ou de rues étroites, à la ligne aérienne de contact (LAC). Cette technologie est destinée aux transports électriques sur espaces partagés avec d'autres modes. La récupération de l'énergie de freinage est possible. Son développement, sous ses différentes technologies, s'affirme, surtout depuis 2009, le marché attire de plus en plus d'entreprises.\n\n\n== Les solutions du début du XXe siècle ==\nDès la fin du XIXe siècle, des ingénieurs s'intéressèrent à l'alimentation des tramways électriques directement par le sol. Déjà à cette époque, les municipalités étaient réticentes à l'établissement de lignes aériennes dans les quartiers historiques. La première expérience fut menée par Jean Claret un entrepreneur de travaux publics dont le bureau d’études est à l’origine de la première ligne de tramway électrique en France en 1888 à Clermont-Ferrand, puis du funiculaire de La Bourboule en 1896. Son système qui équipa en 1894  la ligne de tramways Pont Lafayette-Parc de la Tête d'Or à Lyon était simple avec des plots installés dans l'axe de la voie. Ce système modifié, système Claret-Vuilleumier, équipa également en 1896 la ligne Place de la République à Paris-Romainville. Ce système séduisant qui permettait de s'affranchir du fil aérien ou du transport de batteries à bord des voitures avait cependant ses limites. Pour des raisons de sécurité, les plots ne devaient être mis sous tension que lors du passage des tramways. Ceci nécessitait des solutions techniques difficiles à mettre en œuvre et qui ne donnèrent jamais parfaitement satisfaction.\nLes différents systèmes de plots utilisés à Paris peuvent se classer en deux catégories : les systèmes à distributeurs pour lesquels les plots sont successivement mis sous tension par un distributeur commandant une série de contacts (systèmes Claret-Vuilleumier et Vedovelli) et les systèmes à plots indépendants comprenant chacun leur appareillage de commande (systèmes Diatto et Dolter).\nTous les systèmes à plots souffraient des mêmes maux récurrents : l'impossibilité d'obtenir à coup sûr la mise sous tension et surtout la coupure d'alimentation après le passage du tramway. Il en résultait un grand danger pour les piétons et les chevaux.\nTous ces systèmes disparurent en France dès 1910 et furent remplacés par des installations à caniveaux dans lesquels la voiture captait son énergie à l'aide d'une charrue pénétrant dans une tranchée où étaient établis deux petits rails d'alimentation.\n\n\n== La technologie Innorail ==\n\nUn procédé d'alimentation en courant électrique de véhicules électriques fut mis au point par l'inventeur Michael Goulet avec demande de brevet déposé en décembre 1992 à l'Institut national de la propriété industrielle publié en avril 1994. Ce procédé fut adapté par Innorail, une filiale de la SGTE (groupe Spie) créée en novembre 1996 et basée à Vitrolles qui déposa un brevet le 30 avril 1997. La société, dissoute en 2009, est devenue filiale Alstom en 2004.\nLe système se compose schématiquement, : \n\nd’un dispositif au sol comprenant un rail d’alimentation au milieu des deux autres (troisième rail central) et les coffrets permettant de capter le courant continu à partir du réseau local, ce sont eux qui alimentent le rail central ;\nd’un système embarqué de captation du courant (frotteurs) et de détection de la rame (antennes) ;\nd’un système intégré à la rame, assurant le dialogue entre le matériel roulant et le dispositif APS embarqué. Il assure le fonctionnement du système de traction et de freinage, de la cabine de pilotage et des autres systèmes électriques de la rame.\nLe rail d'alimentation est segmenté en tronçons qui sont alimentés uniquement lorsqu'ils sont entièrement recouverts par le tram, évitant ainsi tout risque d'électrocution pour les autres usagers (piétons, cycles, motocycles).\n\n\n=== Les essais ===\nLe 24 avril 1998, une convention de partenariat fut signée entre la Régie des Transports de Marseille et la société SGTE afin de procéder à la réalisation concrète du brevet. Le site propre de la tranchée Blancarde fut retenu. Tous les aménagements et essais préliminaires eurent lieu de nuit, afin de laisser libre la circulation du tramway en journée (ligne 68). L'expérimentation s’avéra difficile. Les premiers matériels de coupures traction-continue étaient issus de la gamme des produits de source tension-alternative. Le premier essai eut lieu dans la nuit du 20 avril 1999, sur une longueur de 350 mètres. D'autres essais se déroulèrent sur le site d'Alstom, à Aytré (Charente-Maritime).\nA la fin du siècle dernier les premiers constructeurs, Spie-Innorail dispositif (APS), Alstom (dispositif ALISS) et Ansaldo (dispositif Stream) annoncent leurs résultats de recherches,.Les revues spécialisées détaillent ces nouvelles technologies,,.\n\n\n=== La résolution des difficultés initiales de la première expérimentation mondiale ===\nC'est à Bordeaux que fut réalisée en 2003 la première implantation commerciale dans le monde de cette technologie : la ville a pu ainsi conserver son centre historique sans dénaturer le paysage urbain par l'utilisation de câbles aériens. Les premiers essais commencent en juin 2003 et la Communauté Urbaine de Bordeaux reçoit le 21 octobre le premier arrêté préfectoral l'autorisant à commencer les essais du système Innorail sur la voie publique et sans restriction d'accès pour les piétons. Mais après la mise en service, le système ne fit pas la démonstration de la disponibilité et de la fiabilité attendue.\nAprès une difficile période de mise au point et le règlement de nombreuses difficultés techniques, le système dépassa le stade de l'expérimentation pour rentrer dans celui de l'exploitation normale. Pendant les premiers mois de 2004, tandis que la mairie s'impatiente,,, jusqu'à sept versions ont été installées. Innorail a procédé aux améliorations suivantes : \n\nrenfort de l'isolation électrique des jeux de barres (liaisons en cuivre qui transportent le courant en amont et en aval des contacteurs) ;\nchangement du matériau utilisé pour les frotteurs, afin d'augmenter leur durée de vie et diminuer leur coût d'entretien ;\nsurmoulage de l'ensemble des cartes électroniques dans une résine de plusieurs millimètres d'épaisseur ;\naffinage de la détection du signal qui annonce l'arrivée d'un tramway ;\nmise en place d'aimants supplémentaires pour chasser au plus vite un arc électrique pouvant se produire à l'ouverture des contacteurs ;\najout d'un système de drainage des eaux.\nMalgré une amélioration sur le front des pannes, celles-ci sont restées fréquentes jusqu'en octobre 2005 et la question de l'abandon de l'APS avait été ouverte. Cependant, une transformation de la ligne en tramway à alimentation aérienne aurait été synonyme de travaux coûteux et aurait entraîné l'interruption de la circulation des tramways pour plusieurs mois. La Communauté urbaine de Bordeaux en tant qu'autorité organisatrice de transports a donc adressé un ultimatum au constructeur Innorail pour faire chuter le taux de pannes sous la barre des 1 % avant la fin de l'année 2005.\nDepuis 2005, des progrès considérables en matière de fiabilité du système ont été réalisés, le taux de disponibilité du matériel dépassant désormais les normes fixées par la CUB à plus de 99 % - proche de 100 % pour la ligne C mise en service en avril 2004 ou pour les extensions de la ligne A. Le tramway de Bordeaux totalise 14 km en APS,. Ces bons résultats ont été couronnés du trophée de l'innovation dans la catégorie Environnement / Énergie au salon Transports Publics en 2006. Des essais ultérieurs permirent de renforcer la disponibilité et la fiabilité du système qui fonctionne à Bordeaux sur 28 km de voies simples. Cette fiabilité se maintient dans le temps.\n\n\n=== Les mises en service commerciales après Bordeaux ===\nL'APS Innorail repris par Alstom équipe sur tout le parcours ou certaines sections le tramway d'Angers, celui de Reims, celui d'Orléans et celui de Tours en septembre 2013.\nLes autres utilisateurs de l'alimentation par le sol Alstom sont : \n\nen avril 2011, la ville de Reims utilise ce système dès l'inauguration de son nouveau tramway pour les portions des lignes A et B situées en centre-ville, soit 2 km sur les 12 km.\nen juin 2011, le tramway d'Angers ouvre avec deux portions d'APS sur la première ligne, 1,5 km sur les 12 km de la ligne.\nen septembre 2006 l'agglomération d'Orléans commande un tramway Alstom pour la ligne B avec 2,1 km en APS sur 12 km, qui est inauguré le 30 juin 2012.\nle 30 août 2013, le tramway de Tours ouvre avec 1800 mètres d'APS sur la première ligne du tramway, soit 2 km en APS pour un linéaire total de 15 km.\nsignature du contrat en juin 2008 pour la ligne Al Sufouh du tramway de Dubaï, mise en service en novembre 2014, première ligne intégralement en alimentation par le sol.\ndémarrage du projet du tramway de Cuenca (Equateur) en octobre 2013 avec une partie d'APS dans le centre historique de la ville, soit 2 km sur 10 km au total, inauguré en novembre 2015.\nsignature du contrat en septembre 2013 et en juin 2016, le tramway de Rio (Brésil) est inauguré pour les Jeux Olympiques. Il combine la solution APS avec un système de stockage d'énergie à base de supercondensateurs.\nen décembre 2014, la ville de Sydney (Australie) retient la solution APS (2 km) pour sa seconde ligne de tramway inaugurée en décembre 2019.\nen juin 2014, le contrat du tramway de Lusail (Qatar) est signé. Ce projet est majoritairement réalisé en APS. Le tramway sera inauguré au premier semestre 2020.\nen avril 2018, la ville d’Istanbul commande une solution complète d'alimentation par le sol.\nSelon Alstom, 92 km de voies simples de tramway fonctionnent avec une alimentation électrique par le sol.\n\n\n=== Aspects économiques en France ===\nA Bordeaux le surcoût de la solution fut de 15 millions d'euros pour la première phase. Le surcoût de la solution APS, d'alimentation par le sol, serait de l'ordre de 2,5 millions d’euros par kilomètre et de 50 000 € par rame, par rapport à l'alimentation par ligne aérienne de contact. Ce surcoût est à rapprocher du coût de construction du km de tramway qui varie de 15 à 30 millions d’euros par km. La solution représente donc un surcoût de l'ordre de 10% à 20%. Malgré le surcoût le système suscite un intérêt de la part des villes voulant se doter d'un tramway.\n\n\n=== Expérimentations pour application aux véhicules routiers ===\nAlstom expérimente en Suède une application d'alimentation par le sol qui consiste à faire rouler un camion équipé d'un patin au niveau d'un rail électrique. En 2015, Alstom lance une offre APS aux camions hybrides ainsi qu'une solution de recharge statique par le sol adaptée à la fois aux tramways et aux bus électriques équipés de supercondensateurs. Les supercondensateurs sont plus rapidement rechargeables que les batteries et peuvent stocker une quantité d'énergie plus importante avec une durée de fonctionnement plus longue. Les expérimentations continuent.\n\n\n== Les technologies Tramwave d'Ansaldo, le Primove de Bombardier ==\nEn 2009, deux sociétés présentent leurs solutions d'alimentation électrique par le sol.\n\n\n=== Le TramWave d'Ansaldo ===\n\n\n==== Les premières expérimentations ====\nLes premiers essais d'Ansaldo se déroulent à Trieste à partir de la fin du siècle passé. Les commutations sont réalisées par une bande de matériau ferromagnétique prise en sandwich entre deux conducteurs et attirée lors du passage du véhicule par des aimants permanents embarqués. Une expérimentation du système STREAM (Système de Transport Électrique à Attraction Magnétique) en grandeur réelle à partir de juillet 2001 avec deux bus est réalisée sur un premier tronçon expérimental de trois km en voie double. Les véhicules sont équipés d'une batterie de traction embarquée dimensionnée pour pouvoir parcourir cinq km en cas de panne de l’alimentation du système, l'un d’une batterie nickel-métal-hydrure et l'autre d’une batterie nickel-cadmium. L'expérimentation ne semble pas avoir donné des résultats concluants.\n\n\n==== Le système TramWave, le principe ====\nCe n'est qu'en 2009 qu'Ansaldo annonce disposer d'une technologie de tramway sans caténaire pour commercialisation. Cette solution revient à insérer dans le sol, entre les rails de roulement, de petites sections alimentées sous forme de boîtiers de 3 à 5 m de long couronnés par une suite de plaques de contact en acier. L’alimentation du matériel roulant est assurée par des frotteurs rétractables,,. Ansaldo réalisa une ligne expérimentale de 600 mètres à Naples.\n\n\n===== Les applications =====\nLe premier contrat d'Ansaldo pour ce système sera signé en novembre 2013 pour construire une ligne de tramway de 8,7 km de voie double sans ligne aérienne de contact  à Zhuhai. Cette ligne a été mise en service en 2015, de même qu'un tronçon de 4 km sur une ligne du tramway de Beijing.\n\n\n=== Le Primove de Bombardier ===\nLa société Bombardier a présenté en janvier 2009 lors du salon Innotrans à Berlin le système Primove qui permet d’alimenter sans contact un tramway lorsque celui-ci est en mouvement. Les équipements au sol sont réduits par rapport à la solution troisième rail d'Innorail. Cette technologie est également applicable à la recharge de batteries d'autobus. Les batteries embarquées permettent aux tramways Bombardier de parcourir plus de 40 km sans caténaire. Des expérimentations se déroulèrent sur le site de Mannheim depuis 2003.\n\n\n==== Le principe, la captation inductive ====\nLe principe de transmission d’énergie est la captation inductive,[1]. Le système d’émission, enterré dans le sol, constitue le primaire. Il est composé de grandes boucles de courant non connectées entre elles. Le système de réception fixé sous le tramway constitue le secondaire. Il est composé d’un bobinage et d’un circuit magnétique. Comme pour les transformateurs classiques le transfert d’énergie se fait via les flux magnétiques échangés entre primaire et secondaire. Ce système, fonctionnant sur la voie publique, doit respecter les normes de compatibilité électromagnétique. Par conséquent, comme pour l’APS, les boucles de courants primaires alimentées sont uniquement celles qui sont totalement recouvertes par le tramway. Comme dans l'APS, dans cette solution l’équipement installé dans le sol nécessite un bon système de drainage des eaux.\n\n\n==== Des supercondensateurs sur le toit ====\nLe système se cumule avec des condensateurs installés sur le toit du véhicule qui stockent l’énergie de freinage du véhicule, puis la réutilisent durant l’accélération ou la marche. Cette solution réduit la consommation d’énergie dans une proportion pouvant atteindre 30 %. Cette technologie peut aussi accroître la performance du véhicule à l’accélération. Elle est fondée sur une technologie de condensateur à double couche (aussi appelée « ultracondensateurs »), dispositif de stockage qui capte l’énergie électrique libérée lors de l'utilisation des freins. Les accumulateurs des condensateurs se rechargent au moyen de l’énergie qui est transformée durant le freinage.\n\n\n==== Les applications de la technologie ====\nLa première application de la technologie Bombardier fut réalisée en 2010 pour le tramway d'Augsbourg. Néanmoins l'expérimentation se poursuivit sur une section de 800 mètres et si les essais sont annoncés comme concluants en 2012, l'opérateur annonça ne pas vouloir renoncer aux caténaires.\n\n\n== Systèmes électriques concurrents de l'APS ==\n\n\n=== Le Greentech de CAF ===\nCette solution sans caténaire proposée par la société CAF ne nécessite plus aucun équipement sur ou dans le sol. L’énergie est stockée à travers des équipements dédiés, basé sur des ultracapacités et des batteries ion-lithium, placés dans le train. Il s’agit alors de recharger ces équipements de stockage pendant le temps d'arrêt en station. L'énergie cinétique de freinage est également récupérée et transformée en énergie électrique. Le système fonctionne en 750 V continu.\n\n\n==== Les applications de la technologie ====\nLa première application de la technologie CAF fut réalisée sur les cinq véhicules de la petite ligne du tramway de Séville en 2010. Les autres utilisateurs sont :\n\nen 2011, le tramway de Saragosse avec 11 tramways équipés\nen 2015, le tramway de Kaohsiung  avec 9 tramways équipés\nen 2017, le tramway de Grenade avec 13 tramways équipés\nen 2017, le tramway du Luxembourg avec 21 tramways équipés\nen 2019, le tramway de Birmingham avec 21 tramways équipés.\nLa société propose également le système d'équipements des ultracapacités et des batteries ion-lithium seul, en complément des caténaires, qui fonctionnent sur le tramway de Tallinn et celui de Cuiaba.\n\n\n=== Le Sitras HES de Siemens ===\nLe système Sitras Hybrid Energy Storage de Siemens est similaire à celui de CAF : l'énergie est embarquée grâce à des équipements de stockage installés sur le toit. Les équipements se rechargent pendant les arrêts du tramway. La première expérimentation du système est opérationnelle depuis fin 2008 à Lisbonne. Avec un contrat obtenu en 2018, Siemens va installer le système sur les véhicules Avenio du tramway de Qatar Education City,, qui en est la première application clé-en-main sur la totalité du parcours.\n\n\n=== Le SRS d'Alstom ===\nAlstom a mis au point pour le tramway de Nice le SRS (système de recharge statique par contact au sol). Il s'agit d'un système avec batteries où l'alimentation électrique par le sol ne se fait que lors des arrêts aux stations.\n\n\n== Bibliographie ==\n- Dossier Energy Traction Systems DCStreetcar, 218 pages, 2014 \n\n\n=== Références ===\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nSystème de captage du courant :  alimentation par le sol (APS), caténaire, ligne aérienne de contact (LAC), profil aérien de contact, troisième rail\nTramway (section : alimentation électrique)\n\n\n=== Liens externes ===\nUne explication en vidéo de l'alimentation par le sol\nUne autre explication en vidéo de l'alimentation par le sol\nUn reportage sur l'alimentation par le sol à Tours\n Portail des transports en commun   Portail de l’électricité et de l’électronique   Portail du chemin de fer"
        },
        {
            "pageid": 16614247,
            "ns": 2,
            "title": "Utilisateur:Almak/Brouillon01 Techno-optimisme",
            "content": "Le techno-optimisme est une posture intellectuelle soutenant que les progrès technologiques conduiront à une amélioration significative de la condition humaine et permettront de résoudre les défis auxquels l'humanité est confrontée. Cette vision considère la technologie comme un moteur de progrès, capable de générer des solutions innovantes dans divers domaines tels que la santé, l'environnement, l'économie et la société. Le techno-optimisme s'appuie sur l'idée que l'ingéniosité humaine, conjuguée au pouvoir de la technologie, est en mesure de surmonter les obstacles et de créer un avenir meilleur      . \nCe concept suscite de vifs débats, opposant les fervents défenseurs du progrès technologique aux critiques qui s'inquiètent de ses potentiels effets négatifs . L'importance du techno-optimisme réside dans son influence sur les politiques publiques, les investissements et les orientations de la recherche, ainsi que sur la perception collective de l'avenir. \n\n\n== Bénéfices des innovations technologiques espérés par les techno-optimistes ==\nLe techno-optimisme repose sur la conviction que la technologie est un outil puissant au service du bien commun . Il met l'accent sur la capacité de l'innovation technologique à :\n\nAméliorer la qualité de vie : en offrant des solutions aux problèmes de santé, de pauvreté, de faim et en facilitant l'accès à l'éducation et à l'information .\nAméliorer le quotidien : en simplifiant la vie, en favorisant le bien-être et en offrant de nouvelles opportunités. Il faut aussi améliorer la santé mentale au quotidien par la réduction de l'anxiété grâce à l'espoir de solutions technologiques  .\nStimuler la croissance économique : en créant de nouveaux marchés, de nouveaux emplois et en augmentant la productivité.\nGénérer du progrès et de l'abondance : La technologie, en constante évolution, résoudra les problèmes actuels et futurs, conduisant à une société d'abondance. Ainsi Marc Andreesen affirme : « Nous pensons qu'une société technologiquement stagnante dispose d'une énergie limitée au prix de la ruine de l'environnement. Une société technologiquement avancée dispose d'une énergie propre illimitée pour tous. » \nRésoudre les problèmes environnementaux : en développant des énergies renouvelables, en réduisant la pollution et en favorisant une gestion durable des ressources . Ainsi John Kerry a déclaré : « Fifty per cent of the reductions we have to make to get to net-zero by 2050 or 2045 are going to come from technologies that we don’t yet have » (« 50% des réductions de gaz à effet de serre que nous devons effectuer d’ici 2050 ou 2045 viendront de technologies qui restent à inventer »).\nPromouvoir la paix et la coopération internationale : en facilitant la communication, en favorisant la compréhension mutuelle et en créant des opportunités de collaboration.\nEn pratique, les techno-optimistes expriment plus particulièrement leurs espoirs grâce au développement de technologies innovantes, de rupture ou même radicales, telles que :\n\nl'intelligence artificielle,\nla conquête spatiale,\nla fusion nucléaire,\nles nanotechnologies,\nle génie génétique,\nla biologie synthétique.\n\n\n== Historique ==\nLe techno-optimisme est un courant de pensée qui a des racines profondes. Il a émergé à différentes époques de l'histoire, souvent en parallèle avec des périodes de progrès technologique significatif :\n\nL'idée de progrès par la raison et la science, promue par les philosophes des Lumières au XVIIIe siècle, peut être considérée comme un précurseur du techno-optimisme. Ce courant de pensée valorisait l'utilisation de la raison et de la méthode scientifique pour comprendre le monde et améliorer la condition humaine, jetant ainsi les bases de la confiance moderne dans le progrès technologique.\nLa révolution industrielle avec les bonds technologiques souvent associés à une amélioration des conditions de vie pour certains, a alimenté une vision optimiste de l'impact de la technologie. Cette période a été marquée par un fort optimisme quant à la capacité des machines à transformer la société et à améliorer les conditions de vie .\nAu XXe siècle, l'essor de l'informatique, des télécommunications et de la biotechnologie a alimenté de nouveaux espoirs quant au potentiel de la technologie à résoudre les problèmes les plus urgents de l'humanité .\nAu XXIe siècle, l'avancement des technologies numériques, avec la généralisation d'internet, a suscité un regain d'enthousiasme pour le potentiel transformateur de la technologie, alimentant le discours techno-optimiste, voire une vision de l'avenir relevant d'une utopie technologique.\nDes figures emblématiques comme Steve Jobs, Elon Musk, Ray Kurzweil ou Marc Andreessen ont incarné cette vision optimiste, en promouvant des technologies révolutionnaires et en prédisant un avenir radicalement transformé par le progrès technologique . \nLe techno-optimisme se distingue par plusieurs caractéristiques :\n\nUne grande confiance dans le progrès: les techno-optimistes sont convaincus que la technologie est une force positive qui conduira inévitablement à un avenir meilleur   .\nUne vision à long terme: ils se projettent dans un futur où les innovations technologiques auront résolu les problèmes actuels et ouvert de nouvelles perspectives pour l'humanité .\nUne focalisation sur les solutions: ils privilégient une approche pragmatique, centrée sur la recherche de solutions technologiques aux problèmes.\nUne espérance en l'ingéniosité humaine: ils font confiance à la capacité des scientifiques, des ingénieurs et des entrepreneurs à inventer des technologies qui amélioreront la condition humaine .\nMais il est important de noter que le techno-optimisme ne doit pas être confondu avec une foi aveugle dans la technologie . Un techno-optimisme éclairé prend en compte les limites et les risques de la technologie, tout en reconnaissant son potentiel à contribuer à un avenir meilleur . La recherche d'un équilibre entre progrès technologique et responsabilité, entre innovation et éthique, sera cruciale pour construire un futur où la technologie sera véritablement au service de l'humanité . \n\n\n== Nuances du techno-optimisme ==\nLe techno-optimisme, loin d'être une position monolithique, représente un ensemble de points de vue liés à la conviction que la technologie conduira à un futur meilleur. Il se décline en plusieurs dimensions :\n\nDegré de prépondérance des bénéfices de la technologie : l'impact positif des technologies est-il minime ou significatif ?\nOrientation temporelle : les bénéfices prédominent-ils actuellement ou le feront-ils dans l'avenir ?\nRobustesse de l'optimisme : quelle est la probabilité que les bénéfices l'emportent sur les inconvénients ?\n\n\n== Implications ==\nLe techno-optimisme a des implications importantes sur plusieurs plans :\n\nSociétales : il influence la perception du rôle de la technologie dans la société, en encourageant l'adoption de nouvelles technologies et en promouvant une vision positive de l'avenir .\nÉconomiques : il favorise les investissements dans la recherche et le développement de nouvelles technologies, en stimulant l'innovation et la croissance économique  .\nEnvironnementales : il nourrit l'espoir de trouver des solutions technologiques aux défis environnementaux, en encourageant le développement des énergies renouvelables, des technologies de capture du carbone et des solutions d'adaptation au changement climatique  .\n\n\n== Positions du techno-optimisme envers l'État ==\nLes positions du techno-optimisme envers l’État sont multiples et nuancées, oscillant entre une confiance dans le soutien institutionnel et un scepticisme envers l'intervention étatique. Pour autant, le techno-optimisme ne se situe ni à droite, ni à gauche, et peut être vu comme anti-politique  par la primauté donnée à la technologie sur l'évolution des sociétés par rapport aux mouvements sociaux.\n\n\n=== L’État comme catalyseur de l’innovation technologique ===\nCertains partisans du techno-optimisme voient l’État comme un acteur clé dans le développement et la diffusion des technologies. Cette perspective repose sur plusieurs arguments :\n\nInvestissement dans la recherche et développement (R&D) : l’État peut jouer un rôle de mécène en finançant la recherche fondamentale et appliquée, souvent risquée et peu rentable à court terme pour le secteur privé. Par exemple, des innovations majeures comme l’Internet ou le GPS trouvent leur origine dans des projets publics.\nMise en place d’un cadre réglementaire favorable : un cadre juridique adapté peut encourager l'innovation tout en protégeant les citoyens des risques associés aux nouvelles technologies (dérives éthiques, atteintes à la vie privée, etc.). Les politiques publiques en faveur des énergies renouvelables, telles que les subventions aux panneaux solaires ou aux éoliennes, illustrent cette approche.\nRôle de régulateur pour éviter les externalités négatives : les techno-optimistes reconnaissent parfois que l’État est indispensable pour encadrer les usages potentiellement néfastes des technologies, comme l’intelligence artificielle ou la biotechnologie, en promouvant une utilisation éthique et responsable.\n\n\n=== Le scepticisme techno-optimiste envers l’État ===\nD’un autre côté, certains techno-optimistes expriment une méfiance à l’égard de l’intervention étatique, perçue comme un frein potentiel à l’innovation. Cette position s’appuie sur les points suivants :\n\nBureaucratie et inertie institutionnelle : les processus décisionnels étatiques sont souvent perçus comme lents et peu adaptés à la rapidité des avancées technologiques. Les lourdeurs administratives et les réglementations jugées excessives peuvent étouffer la créativité des entreprises et des start-ups.\nNeutralité technologique contestée : certains acteurs estiment que l’État favorise des technologies spécifiques au détriment d’autres, faussant ainsi la concurrence et limitant la diversité des solutions. Cela peut créer des dépendances à des technologies inadaptées ou inefficaces à long terme.\nCapacité limitée à comprendre les enjeux technologiques : les gouvernements sont parfois accusés de manquer d’expertise et de moyens numériques pour évaluer et mettre en œuvre correctement les innovations, ce qui conduit à des politiques mal calibrées ou à des investissements peu judicieux.\n\n\n=== L’État et les limites des technologies ===\nUne autre position, plus nuancée, considère que l’État a un rôle à jouer pour définir les limites du techno-optimisme. Selon cette vision, les technologies ne peuvent pas résoudre tous les problèmes sociaux et environnementaux, et une gouvernance publique forte est nécessaire pour garantir un équilibre entre progrès technologique et bien-être collectif. Cela inclut :\n\nLutte contre les inégalités : les innovations technologiques risquent de creuser les écarts entre les populations si elles ne sont pas accessibles à tous. L’État peut intervenir pour démocratiser l’accès aux technologies (par exemple, via des politiques d’éducation numérique).\nPrévention des dérives : les technologies émergentes, comme l’intelligence artificielle, soulèvent des questions éthiques complexes que le marché seul ne peut résoudre. L’État est alors vu comme un garant des droits fondamentaux face à ces défis.\nEncadrement des usages militaires : les technologies avancées, notamment dans le domaine des drones ou de la cyberguerre, nécessitent un contrôle étatique pour prévenir leur utilisation abusive.\n\n\n=== Exemples récents ===\nLes débats récents autour des régulations de l’intelligence artificielle, des biotechnologies ou encore des cryptomonnaies illustrent ces tensions entre techno-optimisme et rôle de l’État. Par exemple, en Europe, le Règlement sur l’IA (AI Act) proposé par la Commission européenne cherche à encadrer l’utilisation de l’intelligence artificielle tout en stimulant son développement. De manière similaire, les subventions publiques pour la transition énergétique, comme celles prévues dans le Pacte vert pour l'Europe (European Green Deal), reflètent une approche techno-optimiste du rôle de l’État.\n\n\n=== Synthèse ===\nLes positions du techno-optimisme envers l’État oscillent entre collaboration et défiance. Si certains voient en l’État un levier indispensable pour maximiser les bénéfices de l’innovation, d’autres dénoncent son incapacité à suivre le rythme des progrès ou à éviter les biais. Cette dualité met en lumière la nécessité d’un dialogue équilibré entre les acteurs publics, privés et la société civile, afin de construire un cadre qui permette à la technologie de répondre aux enjeux contemporains tout en respectant les valeurs humaines fondamentales.\n\n\n== Controverses et débats ==\nLe techno-optimisme est un sujet controversé qui suscite de nombreux débats. Ses critiques mettent en avant les points suivants    :\n\nLe risque d'une confiance excessive dans la technologie : ils craignent que la focalisation sur les solutions technologiques conduise à négliger les causes profondes des problèmes et à sous-estimer les risques potentiels de certaines technologies   .\nL'impact environnemental des technologies : ils soulignent que le développement et l'utilisation de certaines technologies peuvent avoir des conséquences néfastes sur l'environnement, en consommant des ressources rares, en générant de la pollution ou en contribuant au changement climatique.\nLes questions éthiques et sociales : ils s'inquiètent des implications éthiques et sociales de certaines technologies, notamment en matière de surveillance, de contrôle social, de manipulation de l'information et de perte d'emplois .\nCes critiques sont portées en particulier dans le cadre du techno-pessimisme, une posture qui s'oppose frontalement au techno-optimisme, voire par des mouvements inspirés par le technophobie.\n\n\n== Le techno-optimisme face à la crise écologique ==\nLe rôle de l'innovation technologique face à la crise écologique est un sujet de débat complexe et polarisant. Deux positions principales s'affrontent : le techno-optimisme, qui voit la technologie comme la solution, et une vision plus critique, qui met en garde contre le risque de voir la technologie aggraver la crise.\n\n\n=== Arguments en faveur du techno-optimisme ===\nL’histoire montre que la technologie a permis à l’humanité de surmonter des défis majeurs et d’améliorer sa qualité de vie, par exemple en augmentant la production alimentaire, en améliorant la santé et en facilitant les communications . La technologie peut offrir des solutions concrètes pour atténuer l'impact environnemental de nos activités . Des exemples comme les énergies renouvelables, les véhicules électriques, les villes intelligentes et les technologies de captage de carbone illustrent ce potentiel. L'innovation technologique est une force motrice de la croissance économique, qui permet de créer de la richesse et de financer le développement de nouvelles technologies plus durables. La synergie entre la technologie et les forces du marché peut conduire à une spirale ascendante de progrès et d'abondance matérielle, bénéficiant à l'ensemble de la société .\n\n\n=== Arguments en faveur d'une vision plus critique ===\nL'optimisme technologique peut conduire à une forme de déni et de fuite en avant, en reportant aux générations futures la responsabilité de résoudre les problèmes écologiques   . L'innovation technologique ne fait pas que résoudre des problèmes, elle en crée de nouveaux . Par exemple, les véhicules électriques, tout en réduisant les émissions de CO2 en utilisation, posent des problèmes liés à l'extraction des ressources nécessaires à la fabrication des batteries.\nLa plupart des innovations actuelles ne sont pas orientées vers la résolution des enjeux écologiques. La course à la nouveauté et à la performance conduit souvent à une augmentation de la consommation de ressources et à une obsolescence programmée. La technologie peut aggraver les inégalités sociales et économiques, en concentrant les bénéfices du progrès entre les mains d'une élite. La dépendance excessive à la technologie peut nous rendre plus vulnérables aux crises et aux catastrophes. \n\n\n=== Synthèse ===\nL'innovation technologique est un outil puissant, mais elle n'est pas une solution miracle à la crise écologique. Il est essentiel de repenser l'innovation technologique dans un contexte de limites écologiques, en privilégiant les technologies qui contribuent à la durabilité et à la justice sociale. Il est crucial de ne pas se laisser aveugler par le techno-optimisme et de rester critiques face aux promesses de la technologie. La technologie ne doit pas être considérée comme une alternative à des changements profonds dans nos modes de vie et de consommation .  Il est important de démocratiser les choix technologiques afin d'impliquer l'ensemble de la société dans les décisions qui façonneront notre avenir. Finalement, la question de savoir si l'innovation technologique est la solution à la crise écologique n'appelle pas de réponse simple. Si la technologie peut jouer un rôle important dans la transition vers un avenir plus durable, il est crucial de l'utiliser de manière responsable et réfléchie, en tenant compte de ses limites et de ses impacts potentiels. La solution ne réside pas uniquement dans la technologie, mais dans une approche globale qui combine innovation technologique, changements sociaux et politiques responsables.\n\n\n== Liens entre le techno-optimisme et l'accélérationnisme ==\nLe techno-optimisme et l'accélérationnisme sont deux courants de pensée qui, bien que distincts, partagent une vision commune du progrès technologique comme moteur essentiel de l'évolution sociétale. Le techno-optimisme repose sur la conviction que les avancées technologiques peuvent résoudre les défis contemporains. En revanche, l'accélérationnisme est une doctrine politique qui soutient que l'accélération des processus capitalistes et technologiques peut conduire à une transformation radicale de la société. Ce courant, développé notamment par des penseurs tels qu'Alex Williams et Nick Srnicek dans le manifeste de l'accélérationisme, se divise en variantes de gauche et de droite, avec des objectifs divergents quant à l'utilisation de la technologie pour dépasser le capitalisme. \n\n\n=== Points de convergence ===\nConfiance dans le potentiel transformatif des technologies. Les deux courants partagent la conviction que la technologie est un moteur puissant de transformation, capable de remodeler les structures sociales, politiques et économiques. Par exemple, l'IA est perçue comme un outil de progrès par les techno-optimistes et comme une force de rupture systémique par les accélérationnistes.\nRejet de la nostalgie et de l'idéalisation du passé. Ces idéologies rejettent les solutions basées sur un retour aux modèles traditionnels ou préindustriels, privilégiant une approche proactive centrée sur l'innovation pour répondre aux crises actuelles et futures.\nDépassement des limites humaines. Le transhumanisme, concept partagé par les deux mouvements, vise à améliorer les capacités humaines grâce à la technologie, qu'il s'agisse de prolonger la vie, d'augmenter l'intelligence ou de coloniser d'autres planètes.\n\n\n=== Différences fondamentales ===\nOrientation éthique et politique :\nLe techno-optimisme adopte généralement une perspective humaniste et consensuelle, mettant l'accent sur les bénéfices individuels et collectifs des technologies.\nL'accélérationnisme, en revanche, peut embrasser une vision plus radicale, acceptant les conséquences perturbatrices de l'accélération technologique comme un mal nécessaire pour provoquer des changements systémiques. A contrario, selon le philosophe Laurent de Sutter : « Les accélérationnistes considèrent que les nouvelles générations de téléphones, d'ordinateurs, sont en réalité des innovations extrêmement marginales ».\nRapport au capitalisme :\nLes techno-optimistes opèrent souvent au sein des structures capitalistes existantes, misant sur le marché pour promouvoir l'innovation.\nL'accélérationnisme, particulièrement dans sa version de gauche, envisage le capitalisme comme une étape transitoire à dépasser, en utilisant la technologie pour créer une société post-capitaliste.\n\n\n=== Critiques communes ===\nRisques d'utopies technologiques. Ces deux courants sont accusés de promouvoir une vision utopique qui sous-estime les dangers liés aux inégalités d'accès, à la destruction environnementale ou à la perte de contrôle sur les technologies émergentes.\nAbsence de considération pour les dimensions humaines et éthiques. Les critiques soulignent que ces visions manquent d'attention aux conséquences sociales, culturelles et psychologiques des transformations qu'elles encouragent, négligeant les impacts sur le bien-être humain.\n\n\n=== Synthèse ===\nLe techno-optimisme et l'accélérationnisme entretiennent des liens étroits, notamment dans leur confiance en la technologie comme vecteur de progrès et de changement. Cependant, leurs divergences éthiques et politiques révèlent des approches différentes quant à la manière de naviguer dans un monde dominé par l'accélération des innovations. Une analyse critique de ces deux courants est essentielle pour évaluer leur pertinence face aux défis contemporains.\n\n\n== Perspectives futures ==\nAu delà des récits de science-fiction dystropique ou d'utopie technologique, l'avenir du techno-optimisme dépendra de la capacité des technologies à répondre aux attentes et à résoudre les défis auxquels l'humanité est confrontée. La réussite de la transition énergétique, la lutte contre le changement climatique, le développement de solutions d'intelligence artificielle responsables et la réduction des inégalités seront des éléments clés pour déterminer si le techno-optimisme se confirmera ou se transformera en désillusion  . \nEn pratique, il y a un débat pour établir sur quels critères évaluer une posture comme le techno-optimisme   : \n\nDéfinir ce qui est considéré comme \"bénéfique\" et \"néfaste\" pour servir de métrique dans l'évaluation des conséquences des innovations technologiques .\nDéterminer les faits, en analyser les données historiques et actuelles pour mesurer l'impact des technologies sur les critères de bénéfices et d'inconvénients .\nÉvaluer le pour et le contre : peser les avantages et les inconvénients de la technologie pour déterminer si la balance penche du côté du bien ou du mal .\n\n\n== Conclusion ==\nLe techno-optimisme représente une vision du futur où la technologie joue un rôle central dans l'amélioration de la condition humaine .  Cette perspective s'ancre dans la confiance envers un progrès continu et la capacité de l'innovation à solutionner les problèmes mondiaux   . Toutefois, cette vision optimiste suscite des critiques qui soulèvent des questions cruciales quant aux limites de la technologie, à son impact environnemental et aux défis éthiques qu'elle pose . La viabilité du techno-optimisme dépendra donc de la capacité des sociétés à exploiter le potentiel de la technologie tout en atténuant ses risques et en l'orientant vers un futur durable et équitable.\n\n\n== Références ==\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\n\n Portail des technologies"
        },
        {
            "pageid": 665714,
            "ns": 0,
            "title": "Automate mécanique",
            "content": "Reproduire l'apparence de la vie nécessite des qualités artistiques, comme pour reproduire l'inanimé (statues...) et techniques (mouvement) par des automates mécaniques. Cela demande un certain savoir-faire et des compétences en de multiples domaines.\n\n\n== Historique ==\n\nAprès avoir reproduit des peintures et des statues, l'homme a voulu représenter le mouvement.\nL'invention de la roue et des principaux systèmes mécaniques provoque l’apparition de machines renfermant des automatismes.\nL'utilisation de ces véritables automates primitifs remonte à plusieurs siècles avant Jésus-Christ ; elle est attestée dans différents foyers de civilisation. Statues ou poupées animées servaient déjà de décoration ou de jouet pour enfant. Les exemples les plus connus sont égyptiens mais aussi chinois, assyriens ou précolombiens…\nLes premiers essais et réflexions furent apportés par les prêtres, principaux hommes de science durant l'Antiquité. Des statues automatisées représentant les puissances divines intimidaient les fidèles et renforçaient le pouvoir du culte. Ainsi, la statue de Râ désignait de la main le nouveau Pharaon parmi la file des prétendants. De même, en allumant un feu devant un temple, les portes de ce dernier s'ouvraient d'elles-mêmes. \nCertains savants grecs de l'École d'Alexandrie, dont Philon de Byzance, Ctésibios ou Héron d'Alexandrie, ont apporté indices et témoignages de ces créations dans des écrits relatant leurs travaux. Ces textes furent traduits en arabe et alimentèrent de nouvelles expériences, celles d'Al-Jazari, par exemple. Au VIIIe siècle, le sultan Haroun al-Rachid fit parvenir à Charlemagne une horloge hydraulique qui, selon le chroniqueur Éginhard, s’ouvrait sur le coup de 12 heures pour laisser sortir une troupe de douze cavaliers.\nLes automates les mieux documentés de l'Antiquité, dont certains ont pu être reconstitués, sont les suivants : \n\nle jeu miniature d’Héraclès et Ladon (Héron d’Alexandrie),\nla servante automatique distribuant de l’eau et du vin (Philon de Byzance),\nla fontaine à intermittence de la chouette et des oiseaux chanteurs (Héron d’Alexandrie)\nles portes automatiques d’un temple miniature (Héron d’Alexandrie),\nle théâtre automatique à base mobile (Héron d’Alexandrie).\nL’apparition des horloges mécaniques en occident à partir du XIVe siècle donna une nouvelle impulsion à la fabrication des automates, dont la forme la plus populaire (et la plus spectaculaire) fut sans doute le jacquemart, automate à forme humaine qui s’animait pour frapper l’heure sur la cloche de l’église. Certaines horloges étaient munies de plusieurs automates et, à l’heure dite, offraient un véritable spectacle animé. Un des exemples les plus célèbres et les plus tardifs est celui de l’horloge astronomique de Strasbourg. \nCe n'est qu'à partir du XVIe siècle, que l'automate devint un objet de luxe à la mode auprès de la noblesse occidentale. Au siècle des Lumières, l'automate est conçu comme une curiosité scientifique, tel le canard de Vaucanson, censé digérer réellement de la nourriture, ou les automates des horlogers suisses Pierre Jaquet-Droz et Henri-Louis Jaquet-Droz. Certains, seulement en partie automatisés comme le fameux Turc mécanique, furent des canulars si crédibles qu'ils marquèrent durablement l'imaginaire collectif.\nÀ la fin du XIXe siècle et au début du XXe siècle, l'automate se banalise et devient un outil publicitaire.\nL'automate, après cette période faste de 1850 à 1914, a peu à peu décliné jusqu'à être supplanté par le robot, électronique et « reprogrammable ».\n\n\n== Exemples d'automatismes ==\n\n\n=== Les Portes du temple ===\nL'ouverture automatique des portes du temple est le théorème 37 des Pneumatiques de Héron d’Alexandrie.\nLe principe est le suivant : « construction d'une chapelle telle qu'en allumant du feu, les portes s’ouvrent toute seules et se referment quand le feu est éteint. »\nLe feu chauffe l'air contenu dans le socle creux. L’air se dilate sous l'effet de la chaleur et s'échappe par un conduit jusque dans un réservoir sphérique hermétique contenant un volume d'eau suffisant.\nSous l’effet de la pression exercée par l'air dilaté, l’eau contenue dans la sphère s'écoule dans un récipient relié aux portes du temple par un système de cordages et de poulies. Ce récipient s'alourdit au point que sa masse dépasse celle du contre-poids qui maintient les portes fermées. Ce faisant, le récipient alourdi entraîne l’ouverture automatique des portes.\nQuand le feu est éteint, l'air dilaté se rétracte et aspire l'eau versée dans le récipient vers la sphère. Le récipient s’allège et le contre-poids contraint la fermeture automatique des portes.\n\n\n== Voir aussi ==\n\n\n=== Bibliographie ===\nAlfred Chapuis et Edmond Droz, Les Automates, figures artificielles d’hommes et d’animaux, histoire et technique, éditions du griffon, 1949\nPierre Arnaud, Des moutons et des robots. Architecture de contrôle réactive et déplacements collectifs de robots, Collection Meta, PPUR presses polytechniques, 2000,  (ISBN 288074458X), 9782880744588\nLehni, L'Horloge astronomique de la cathédrale de Strasbourg, Savoir Découvrir, Éditions la goélette, 1997  (ISBN 2-906880-18-3)\n\n\n=== Article connexe ===\nAutomate d'art\nIbn Khalaf al-Muradi\nHans Schlottheim\nNef de Charles Quint\n\n\n== Notes et références ==\n\n Portail du génie mécanique   Portail des technologies"
        },
        {
            "pageid": 899840,
            "ns": 0,
            "title": "Banc à étirer",
            "content": "Le banc à étirer ou autrefois trivialement banc à tirer, est un appareil servant à étirer les métaux malléables et/ou ductiles en fil ou pour en faire des tubes sans soudure. Dans le cas d'outil à étirer les fils de fer, il prend le nom de banc de tréfileur.\n\n\n== Métallurgie générale ==\nLe banc à tirer, défini par le mécanicien Louis-Benjamin Francœur, est une machine d'atelier pour tirer à la filière des tuyaux métalliques, des pignons de montre ou de pendule, des fils de fer, d'acier, de laiton etc. que l'opérateur veut réduire à un calibre donné. Il existe vers 1820 trois sortes de bancs à tirer, suivant l'usage recherché, de taille plus ou moins grande, plus ou moins fort. Leurs mécanismes étaient soit à sangles, soit à vis ou à engrenages. Les bancs à tirer occupent beaucoup de place dans les ateliers de grosse chaudronnerie, puisque, à la dimension du banc à tirer plutôt grand ou puissant, il faut ajouter évidemment la longueur des tuyaux à amincir.\nLe banc à tirer servait à fabriquer un tuyau de cuivre. Prenant une bande de métal, l'opérateur en amincissait les bords à la machine à raboter, à la fraise, au marteau, voire à la lime si le cuivre est très mince. Il cintrait la pièce au marteau en plaçant le métal d'abord dans une rigole semi-circulaire ou matrice, puis sur un chevalet de même diamètre que le tube à préparer. Après avoir assemblé et formé le tuyau, il pouvait assurer sa dimension exacte avec le banc à tirer. L'appareil se compose essentiellement d'une pince à tirer, composée d'une plaque métallique percée d'un trou conique, qui saisit ici l'extrémité du tuyau et d'une filière, rondelle d'acier, percée de trous de diverses grandeurs, à travers laquelle on fait passer le tube préformé. Un mandrin est placé à l'intérieur du tube, précisément de la grosseur qu'il doit acquérir. La pince est attachée à l'un des maillons d'une chaîne sans fin, à laquelle un système convenable d'engrenage à manivelle, précise Pierre Larousse, permet de donner le mouvement. Le tube, pris par la pince, est entrainé dans le mouvement de la chaîne, passe à travers les trous de plus en plus petits de la filière et finit par acquérir le diamètre voulu par l'opérateur. La vitesse d'étirage des tuyaux est de l'ordre de 2 à 3 cm par seconde, parfois 8 cm par seconde pour les petits tubes en cuivre.\nLe fil à étirer est saisi entre les deux mâchoires semi-coniques de la pince. Placées dans l'ouverture de la plaque, les mâchoires s'y enfoncent d'autant plus et serrent d'autant mieux l'objet à étirer, que la traction subie est plus forte. La filière est formée d'une rondelle aciérée, en acier sauvage, consolidée par une plaque de fer. L'acier des forges catalanes employé pour les filières françaises, de grande qualité, expliquent la course au rachat de seconde main, lancée au milieu du XIXe siècle par les entreprises anglaises du secteur.\nLa vitesse d'étirage pour le tréfilage de fer est d'autant plus faible que le fil initial est plus dur et plus gros. Plus la vitesse augmente, plus vite le fer est aigri. Il faut alors pour éviter l'écrouissage et durcissement de sa surface, recuire le fer au cours de l'opération, parfois plusieurs fois. Le banc à tirer permet d'obtenir des fils d'une ténuité extrême, de l'ordre de 1, 25 mm de diamètre, utilisable dans les croisements des lunettes, en particulier en enfermant un fil de platine dans une gaine d'argent. On étire le montage, les deux métaux gardant les mêmes proportions dans leurs diamètres. Par un moyen chimique, dissolution chimique par l'acide nitrique ou amalgame au mercure, la gaine d'argent est ôtée. Ne reste que le fil de Pt.\n\n\n== Bijouterie : banc à étirer pour métaux précieux or, argent etc. ==\nLe banc à étirer, évolution moderne de l'argue, est un outil de bijoutier. L'argue est d'abord le lieu où les tireurs d'or tréfilaient leurs lingots. Par extension c'est devenu l'étau coinçant les filières lors de l'étirage du fil, et le support de filières. On peut supposer que le mot argue fait référence au berger Argos de la mythologie grecque dont les cent yeux rappellent la multitude de  trous des filières.\n\nLa filière à étirer est une plaque métallique percée de trous coniques au  diamètre décroissant, dans lesquels passent  les fils d'or, d'argent ou de cuivre destinés à être affinés. Le trou se nomme pertuis, l'entrée, embouchure et la sortie, œil. Entre deux battes de bois, une  chaîne galle sans fin est manœuvrée par le  pignon d'une manivelle.\nLe fil à étirer est  cramponné dans une pince à étirer : la \"grenouille\", qui, par un mors, peut s'accrocher à cette chaîne ou  s'en décrocher.\nAvant l'étirage, le métal est  recuit, c’est-à-dire chauffé au rouge : 750° pour l'or gris, 650° pour l'or jaune, et aux environs de 600° pour l'argent et autres métaux.\nUn banc à étirer daté de 1565 et conçu par Leonhard Danner pour Auguste Ier de Saxe est conservé au musée national de la Renaissance, au château d'Écouen : http://www.musee-renaissance.fr/bancdorfevre/\n\n\n== Notes et références ==\n\n\n== Bibliographie ==\nMichèle Bimbenet-Privat (direction), Le banc d'orfèvre de l'électeur de Saxe, Paris, RMN, 2012.\nLouis-Benjamin Francœur (1773-1849) parmi un collectif de rédaction, Dictionnaire technologique ou nouveau dictionnaire universel des arts et métiers, et de l'économie industrielle et commerciale, 22 volumes, Thomine et Fortic, Paris, 1822-1835. En particulier, Tome II à l'entrée \"Banc à tirer\" (variantes Banc à tirer à sangle, Banc à tirer à vis, Banc à engrenage), p. 515-517. Article accessible à la bibliothèque numérique du CNAM, cote CNAM-BIB 8 Ky 1 (texte) 4 Ky 3 (atlas).\nLouis Benjamin Francœur, Henri-Edmond Robiquet, Anselme Payen et Edmond Pelouse, Abrégé du grand dictionnaire de technologie, 6 volumes, in octo, Thomine, Paris, 1833-1836, accessibles sur gallica.bnf.fr. En particulier, Tome I, 500 p. à l'entrée Banc à tirer p. 296 renvoi à Tome VI, 513 p. avec errata, par en particulier \"Tréfilerie\", p. 327-332, Tuyaux p.339-343\nHenry de Graffigny, Dictionnaire des termes techniques employés dans les sciences et dans l'industrie, Imprimerie Deslis Frères (Tours), H. Dunod et E. Pinat éditeurs, Paris, 1906, 839 pages, préface de Max de Nansouty. Recueil de 25.000 mots techniques avec leurs différentes significations. Entrées Banc page 67-68, simple définition de \"banc à étirer\".\nPierre Larousse (dir.), Grand dictionnaire universel du XIXe siècle : français, historique, géographique, mythologique, bibliographique...., en 17 volumes, Administration du grand Dictionnaire universel, Paris, 1866-1890, en particulier Tome 2 B, entrée Banc à tirer, partie technique, p. 145.\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nLaminoir à plaques\nLaminoir à fils\nLingotière à bascule\n Portail de l'or   Portail du travail et des métiers"
        }
    ],
    "Culture": [
        {
            "pageid": 8082,
            "ns": 0,
            "title": "Culture",
            "content": "En sociologie, comme en éthologie, la culture est définie de façon plus étroite comme « ce qui est commun à un groupe d'individus » et comme « ce qui le soude », c'est-à-dire ce qui est appris, transmis, produit et inventé. Ainsi, pour une organisation internationale comme l'UNESCO : « Dans son sens le plus large, la culture peut aujourd’hui être considérée comme l'ensemble des traits distinctifs, spirituels, matériels, intellectuels et affectifs, qui caractérisent une société ou un groupe social. Elle englobe, outre les arts, les lettres et les sciences, les modes de vie, les lois, les systèmes de valeurs, les traditions et les croyances. » Ce « réservoir commun » évolue dans le temps par et dans les formes des échanges. Il se constitue en de multiples manières distinctes d'être, de penser, d'agir et de communiquer en société.\nEn philosophie, le mot culture désigne ce qui est différent de la nature.\nPar abus de langage, le mot « culture » est employé pour désigner presque exclusivement l'offre de pratiques et de services culturels dans les sociétés modernes, et en particulier dans le domaine des arts et des lettres.\n\n\n== Définitions ==\nLa culture est, selon le sociologue québécois Guy Rocher, « un ensemble lié de manières de penser, de sentir et d'agir plus ou moins formalisées qui, étant apprises et partagées par une pluralité de personnes, servent, d'une manière à la fois objective et symbolique, à constituer ces personnes en une collectivité particulière et distincte » (Edward B. Taylor, Primitive Culture, 1871)\n\n\n=== Pluralité de définitions ===\nDifférentes définitions du mot « culture » reflètent les théories diverses pour comprendre ou évaluer l’activité humaine.\nEn 1952, les anthropologues Alfred Kroeber et Clyde Kluckhohn ont rédigé une liste de plus de 150 définitions différentes du mot culture dans leur livre Culture: a critical review of concepts and definitions.\nLa définition que peuvent en faire les gouvernements lorsqu’ils fixent sa mission au Ministère de la Culture diffère de celle que l’on en donne dans les sciences humaines ou de celle qui correspond à la « culture générale » de chacun.\nIl existe de puissants enjeux politiques et économiques pour définir et encadrer la culture. Lorsque les entrepreneurs tentent de faire valider la notion de « culture d'entreprise » ou les ingénieurs celle de « culture technique », ils contribuent à étendre l'amplitude des significations mais au prix d'en diluer certaines caractéristiques spécifiques, comme l'opposition plus traditionnelle entre des styles plus spontanés, artistiques, religieux, fondés, comme le disait Georg Wilhelm Friedrich Hegel, sur le « sentiment » et des types d'actions davantage fondés sur le calcul, la cognition, la règle. Bien que fréquemment les deux mondes s'entrecroisent, devons-nous pour autant les confondre, contribuant alors à privilégier une conception totalisante de la culture ?\nLe mot « culture » est parfois employé dans un sens restreint pour désigner l'industrie des « biens culturels », c'est-à-dire les entreprises et activités de production, de distribution et de gestion de droits d'exploitation de spectacles et de contenus audio-visuels reproductibles (voir Économie de la culture). Ce secteur, sous l'effet du développement des technologies de l'information et de la communication, est en pleine transformation et son avenir fait l'objet de controverses politiques tendues.\nSelon Geert Hofstede, la culture est une programmation mentale collective propre à un groupe d’individus.\nDe manière plus générale, en éthologie, la culture animale désigne tout comportement, habitude, savoir, système de sens (en anthropologie) appris par un individu biologique, transmis socialement et non par héritage génétique de l’espèce à laquelle appartient cet individu. La culture se définit en ce sens comme un ensemble de connaissances transmis par des systèmes de croyance, par le raisonnement ou l’expérimentation, qui la développent au sein du comportement humain en relation avec la nature et le monde environnant. Elle comprend ainsi tout ce qui est considéré comme « acquisition de l’espèce », indépendamment de son héritage instinctif, considéré comme naturel et inné. Ce mot reçoit alors des définitions différentes selon le contexte auquel on se réfère.\nMais la culture n'est pas réductible à son acception scientifique, car, comme l'indique la définition de l'UNESCO, elle concerne les valeurs à travers lesquelles nous choisissons aussi notre rapport à la science. En ce sens, elle relève davantage de la communauté politique des êtres humains que de l'espèce comme objet de science.\n\n\n=== Approche critique ===\nPlus récemment, le concept de culture a largement été critiqué, notamment par des chercheurs comme Ingrid Piller (en), Joana Breidenbach (de), Unni Wikan (en) ou Adam Kuper (en). Pour ces chercheurs non francophones, principalement issus de l'anthropologie ou des sciences politiques, le concept de culture serait avant tout un outil qui aurait remplacé le concept de race, reproduisant un certain nombre de stéréotypes essentialistes. \nLe haut commissariat des droits de l'homme des Nations-Unies, par la voix de ses rapporteuses spéciales dans le domaine des droits culturels, publie des rapports sur les droits culturels, notamment ceux des femmes. Farida Shaheed (en), en 2012, propose « de passer d’un modèle qui considère la culture comme un obstacle aux droits des femmes à un modèle qui vise à garantir une égalité de jouissance des droits culturels. En outre, une telle approche constitue un outil important pour la réalisation de tous les droits de l’homme. ».\n\n\n=== Culture individuelle et culture collective ===\nEn langue française, le mot « culture » désigne tout d’abord l’ensemble des connaissances générales d’un individu. C’est la seule définition qu’en donne en 1862 le Dictionnaire national de Bescherelle. Les connaissances scientifiques y sont présentées comme élément de premier plan. C’est ce que nous appelons aujourd’hui la « culture générale ».\nAprès le milieu du XXe siècle, le terme prend une seconde signification. Par exemple, le Petit Larousse de 1980 donne, en plus de la conception individuelle, une conception collective : ensemble des structures sociales, religieuses, etc., des manifestations intellectuelles, artistiques, etc., qui caractérisent une société. Le terme peut alors revêtir l’un ou l’autre sens, mais la proximité des domaines d’utilisation de chacun en fait une source d’ambiguïté.\nEn langue allemande, la définition de la culture individuelle ou culture générale correspond au mot Bildung. Il existe un autre mot, Kultur, qui correspond à un patrimoine social, artistique, éthique appartenant à un ensemble d’individus disposant d’une identité. Ainsi, ce terme homophone, qui correspond plutôt en français à l’une des acceptions de civilisation, et par les échanges d’idées entre la France et l’Allemagne, s’est petit à petit amalgamé avec le sens initial du mot culture en français. Cette seconde définition est en train de supplanter l’ancienne, correspondant à la culture individuelle. Néanmoins, les dictionnaires actuels citent les deux définitions, en plaçant le plus souvent la « culture individuelle » en premier.\nIl y a en français deux acceptions différentes pour le mot culture :\n\nla culture individuelle de chacun, construction personnelle de ses connaissances donnant la culture générale ;\nla culture d'un peuple, l'identité culturelle de ce peuple, la culture collective à laquelle on appartient.\nCes deux acceptions diffèrent en premier lieu par leur composante dynamique :\n\nla culture individuelle comporte une dimension d’élaboration, de construction (le terme Bildung est généralement traduit en éducation), et donc par définition évolutive et individuelle ;\nla culture collective correspond à une unité fixatrice d’identités, un repère de valeurs relié à une histoire, un art parfaitement inséré dans la collectivité ; la culture collective n’évolue que très lentement, sa valeur est au contraire la stabilité figée dans le passé, le rappel à l’Histoire.\nC’est dans cette dichotomie que ces deux significations peuvent s’opposer :\nLa culture collective comporte une composante de rigidité pouvant s’opposer au développement des cultures individuelles, ou pouvant conduire à des contrecultures, concept qui est inimaginable avec le sens individuel, la connaissance ne pouvant être que positive.\nLa science, toujours en évolution, n’est de ce fait pas raccrochée au concept de culture individuelle, dans les acceptions populaires, alors qu’elle en est une des composantes principales dans la teneur initiale du terme.\nMais c’est par l’art et l’histoire que les deux concepts se rejoignent. La culture individuelle inclut la connaissance des arts et des cultures, celle des différentes cultures humaines, mais bien évidemment celle affiliée à la culture (collective) à laquelle l’individu s'apparente.\nC’est là le point d’amalgame entre les deux acceptions : la culture (individuelle) est comprise comme connaissance de la culture (collective) dont on dépend. Fusionnant ainsi deux acceptions différentes, le terme « culture » tend actuellement, en France, vers un compromis dans son acception courante, où il désignerait essentiellement des connaissances liées aux arts et à l’Histoire, plus ou moins liées à une identité ethnique.\nLes deux sens doivent cependant être analysés distinctement : la culture collective et la culture individuelle se recoupent en réalité, non seulement par leur homonymie, mais aussi par la filiation de l'espèce et de l’individu à une entité culturelle.\n\n\n=== Langage courant ===\nL’utilisation populaire du mot « culture » dans beaucoup de sociétés occidentales, permet de réaliser un classement de son caractère en fonction de croyance, de la consommation de biens ou de l’exercice d’activités considérées comme élitistes : la cuisine, l’art, et la musique par exemple.\n\n\n=== En ethno-archéologie ===\n\nEn ethno-archéologie et en anthropologie sociale et culturelle, par sa « culture » nous voulons distinguer chaque groupe humain occupant un certain espace géographique pendant une période donnée. Nous nous appuyons pour ce faire sur la répartition homogène dans cet espace géographique d'un certain nombre de types d'objets (formes de poteries, outils de silex, art mobilier, architecture, pratiques funéraires, etc.) et ainsi définir la « culture matérielle » de chaque « culture » entendue de ce point de vue ethno-archéologique. Le Chasséen tire son nom du site de Chassey-le-Camp en Saône-et-Loire, la culture de Yangshao tire le sien d'un site près du village de Yangshao, au Nord du Henan, etc. ce sont autant de « cultures » sur une aire géographique bien plus étendue que ces sites mais en constante redéfinition en fonction de l'état des recherches.\n\n\n=== Point de vue sociologique ===\nOutre le fait que nous ayons tendance à ne distinguer qu'une culture unique à l'échelle — la Culture — et qu'il est d'usage de la définir par rapport à la Nature, la sociologie propose d'étudier les phénomènes culturels au pluriel. Lorsque nous nous intéressons à la culture d'un point de vue sociologique, il s'agit de constater dans les faits qu'il existe des pratiques culturelles qui diffèrent selon l'espace et le temps, selon la position sociale occupée par les individus au sein de la société, selon le genre, le travail exercé, etc. De manière plus précise, chaque pratique culturelle est étroitement liée aux représentations ; Pierre Bourdieu appelle ces représentations habitus, un concept qui s'apparente à des « lunettes ». Ainsi, chaque individu, étant donné qu'il est un produit socio-historique, un homme vivant parmi d'autres hommes, éduqué d'une certaine façon et habitué à des pratiques sociales qu'il juge « normales », chaque individu donc, se fait ses propres représentations de ce qui est légitime et non-légitime, beau et laid, bien ou mal. L'individu n'est pas un atome isolé qui ferait ce qu'il fait naturellement, mais répond à des exigences dont il ne se rend pas compte. Quand nous nous interrogeons sur la Culture — qu'est-ce qu'une culture et qu'est-ce qui n'en est pas une ? — il faut dépasser un certain ethnocentrisme, il faut étudier la Culture comme étant un objet scientifique comme un autre, comme le physicien par exemple étudie les astres. Car définir la Culture, doit différer et se débarrasser des jugements de valeurs et des jugements moraux.\nNous avons tendance à penser la Culture comme un noyau dur inaltérable qui ne changerait que par « périodes » ou « stades » de l'évolution humaine. Mais la Culture est une affaire de tous les jours ! La Culture est ce que les hommes en font ; elle n'est pas par ailleurs chose tombée du ciel, par une sorte d'ésotérisme réservée à des génies qui seraient génies innés. Norbert Elias, dans son opus du Processus de civilisation, invite à penser la Culture comme une dynamique de transformations successives dans l'histoire de l'Homme en liaison aux changements sociaux (centralisation de l'État, principe de différenciation et de logique concurrentielle entre les individus, pacification des mœurs) et aux évolutions techniques. Derrière le mot Culture donc, des forces coexistent, se repoussent et fusionnent entre elles. Dans son ouvrage Mozart : sociologie d'un génie, N. Elias montre que le compositeur autrichien est lui-aussi déterminé sociologiquement ; dans une optique de démystification de l'histoire et non de destruction de croyance, Elias explique que bien que Mozart soit un génie célébré, il est situé dans un espace social propice qui lui a permis d'être ce qu'il est devenu (entre autres, père compositeur et violoniste pour la cour d'Autriche de son époque).\nPour Pierre Bourdieu, la Culture ne fait référence qu'à l'existence d'une culture dominante et légitimée, antagonique à une culture dominée dont les valeurs ne sont pas reconnues. Chaque individu, doté d'un capital économique, d'un capital social et d'un capital culturel déterminé par son champ social, est forcé d'intérioriser les normes reconnues par le champ dominant pour pouvoir à son tour être reconnu. Par exemple, la musique symphonique/classique est dite légitime, alors que la musique métal ou le rap ne le sont pas. Dans les pratiques culturelles des individus positionnés en haut de l'échelle sociale, il est à noter qu'aller à l'opéra est plus fréquent que pour un ouvrier ; pour saisir cette domination, il faut non pas simplement s'intéresser aux œuvres-mêmes, mais les catégories sociales dont les individus sont issus, car selon la position sociale occupée, les représentations changent : il y a l'influence de règles sociales sur les pratiques individuelles.\nLa Culture demeure du sens, des schèmes culturels individualisés, c'est-à-dire façonnés par l'individu en rapport avec son héritage, la manière dont il en hérite, et en rapport aux autres : la Culture n'est viable qu'au pluriel.\n\n\n=== Types de composants ===\nUne représentation de la culture consiste à la regarder comme formée de quatre éléments qui sont « transmis de génération en génération en apprenant » :\n\nles valeurs ;\nles normes ;\nles institutions ;\nles artefacts.\nJulian Huxley donne une division légèrement différente, en mentifacts, socifacts et artifacts, pour des sous-systèmes idéologiques, sociologiques, et technologiques respectivement. La socialisation, du point de vue de Huxley, dépend du sous-système de croyance. Le sous-système sociologique oriente l’interaction entre les gens. Les objets matériels et leur utilisation forment le sous-système technologique.\nEn général, les archéologues se focalisent sur la culture matérielle, alors que l’anthropologie culturelle se focalise sur la culture symbolique, encore qu'in fine les deux groupes s’intéressent aux relations entre ces deux dimensions. De plus, les anthropologues conçoivent le mot « culture » pour se référer non seulement à la consommation de biens, mais au processus général qui produit de tels biens et leur donne une signification, et aux relations et pratiques sociales dans lesquelles de tels objets et processus sont imbriqués.\n\n\n==== Les valeurs ====\nLes systèmes de valeurs comprennent des idées et des matériaux qui semblent importants dans la vie. Elles guident les croyances qui composent la culture en partie.\nIl est possible de reconnaître des systèmes de valeur associés de préférence à des civilisations. Ainsi, dans ce que nous appelons encore l'Occident, il semble que la conversation culturelle se préoccupe beaucoup de la question de la règle, de la mesure, de la loi physique ou sociale, alors qu'en Extrême-Orient, l'affaire la plus importante concerne l'identité dans le monde. Les valeurs des sociétés villageoises (comme en Afrique ou en Amérique latine) portent davantage sur l'équilibre entre l'homme et la nature, garanti par l'intercession des hommes-médecine. Les valeurs des sociétés nomades sont plutôt attachées à résoudre les problèmes des antagonismes inévitables entre groupes sur le territoire commun. À l'intérieur de la sphère occidentale, le point de vue anglo-saxon insiste encore davantage sur la loi (culture de la common law, et de la rule of law). Ceci correspond à une religiosité inspirée des protestantismes préoccupés de l'usage rationnel du temps personnel (comme le montrait Max Weber), ce qui permet l'autodiscipline, libère un certain libéralisme et fait l'économie d'un contrôle par l'autorité collective.\nEn France, le plus laïc des pays occidentaux - tradition que l’on pourrait faire remonter au gallicanisme de Philippe le Bel, à la Pragmatique Sanction de Bourges, ou aux positions de Bossuet - nous avons plutôt affaire à une reprise administrative nationale de l'ancienne autorité catholique, où se trouve préservé un principe d'arbitrage divin et royal, désormais déposé dans l'État laïc. La Révolution française introduit un statut civil équivalent pour tous les citoyens, indépendamment des croyances ou appartenances religieuses, mais ne renie pas longtemps - avec Napoléon - le principe du pouvoir transcendant et paternaliste. Celui-ci subsiste aujourd’hui dans la trame culturelle de ce pays qui demeure de ce point de vue de tradition catholique. Néanmoins, comme partout ailleurs en Europe, nous y rencontrons le débat avec les deux religions et cultures du « Livre » (la Bible), qui forment les deux autres variantes de la culture occidentale au sens large : la tradition judaïque, qui insiste sur l'alliance entre Dieu et son peuple, au travers d'une loi interprétable ; et la tradition musulmane, qui veut rétablir le principe de la liberté absolue de Dieu. Nous constatons ici combien le monde des valeurs ne se développe pas au hasard, mais bien comme système logique de différences assumées. Nous pouvons aussi observer que ce caractère de conversation entre les valeurs demeure le plus souvent inconscient, caché par l'intransigeance de leurs partisans respectifs.\n\n\n==== Les normes ====\nLes normes sont constituées par les attentes sur la façon dont les personnes doivent se comporter dans diverses situations. Chaque culture a des méthodes, appelées sanctions, pour imposer ses normes. Les sanctions varient avec l’importance de la norme ; les normes qu’une société impose formellement ont le statut de lois.\nIl est à noter qu’en France, la langue française a le statut de langue officielle, et qu’à ce titre, elle est la langue de l’administration et du droit civil.\nAux États-Unis, il existe une tradition normative très importante en matière industrielle et financière. Les normes comptables en Europe sont actuellement assez largement inspirées des normes américaines.\n\n\n==== Les institutions ====\nLes institutions sont les structures de la société dans et par lesquelles les valeurs et les normes sont transmises.\nNous avons vu que, dans le cas de la France, la défense de la langue fut prise très tôt en charge par le souverain, François Ier pour le statut de langue officielle du français par l'Ordonnance de Villers-Cotterêts (1539), Richelieu pour l’Académie française. De là est née, en France et dans la plus grande partie de l’Europe, une tradition qui lie la culture avec les institutions publiques.\nAux États-Unis, il n’existe pas une emprise aussi importante de la puissance publique sur la culture proprement dite. Ainsi, de nombreuses grandes entreprises ont des collections d’œuvres d'art telles qu’elles ouvrent des musées privés. Des hommes d'affaires et milliardaires n'hésitent pas à réaliser du mécénat et par leur philanthropie alimentent de grandes fondations (qui portent d'ailleurs souvent leur nom) et qui ont développé des actions dans le secteur de la culture, des arts et de l'enseignement artistique (des grands musées comme le Metropolitan ou Guggenheim à New-York, les Fondations comme Ford, Carnegie, etc.). Les industries culturelles, mettant en œuvre les bases d'un véritable management culturel, se sont dès le départ développées sur un modèle d'entreprises privées avec au fil des décennies un mouvement de forte concentration financière faisant des grands groupes américains du secteur les principaux protagonistes d'un oligopole mondial des industries de l'entertainment et des médias (Time Warner, Disney, Fox, etc.). Ainsi, depuis les années 1950, l’industrie américaine du cinéma, concentrée à Hollywood, domine non seulement économiquement mais aussi symboliquement, la distribution des films à grand succès et la consécration des grandes vedettes.\nEn France, la majorité des institutions culturelles sont des organisations en gestion publique ou des organisations de type associatif mais avec une forte dépendance à des collectivités publiques : académies, musées, bibliothèques, médiathèques, conservatoires, salles de concert et de théâtre, orchestres, opéras, Maisons des jeunes et de la culture. La France a été l'une des premières démocraties modernes à se doter d'un ministère de la Culture en 1959. Elle fut suivie par de nombreux autres pays en Europe mais selon des formules adaptées à leur contextes respectifs. Les « petits pays » (petits par leur taille) comme les Pays-Bas, la Finlande, l'Autriche ou le Portugal, ont dans leurs organisations gouvernementales respectives un ministère plus large (Éducation par exemple) auquel est rattaché un secrétariat d'État chargé de la Culture. Les pays à structure fédérale ont des équivalents dans leurs régions (en réalité des États fédérés) qui exercent la compétence culturelle. Ainsi, en Allemagne, on trouve dans le gouvernement de chaque Land une direction de la Culture et des Arts, le plus souvent rattachée à l´Enseignement, la Recherche et la Formation professionnelle (ce qui s'explique notamment par l'importance des institutions d’enseignement artistique). L’Espagne s'est quant à elle dotée d’un ministère de la Culture en 1978, dès que la page du franquisme fut tournée. Le Royaume-Uni constitue un exemple des plus intéressants dans la prise en compte de l’action étatique en faveur de la culture car il s’agissait d’abord pour le gouvernement d’intervenir et de soutenir les institutions artistiques et en particulier celles du spectacle vivant (théâtre, danse, musique) telles que la Royal Shakespeare Company, le Royal Opera House Covent Garden, les grands orchestres londoniens, etc.\nNous considérons donc un schéma assez voisin dans les pays européens. Dans le cas de la musique classique par exemple, nous pouvons observer que toutes les institutions musicales (hormis quelques notables exceptions) bénéficient du soutien de collectivités publiques (État, régions, villes). Le Royaume-Uni toutefois se distingue du reste de l'Europe car les institutions musicales y sont plus autonomes, assez rarement des établissements publics. En revanche dans le domaine des musées, une forte proportion des institutions sont publiques. De ce point de vue, le Royaume-Uni se distingue des États-Unis, les traditions culturelles des deux pays étant assez distinctes.\nQue ce soit en France ou en Europe, certains lieux privés peuvent être considérées comme des institutions : des châteaux privés comme Chenonceau, des abbayes comme Fontfroide à Narbonne, l'écomusée d'Alsace ou encore de grandes manifestations d'animation existant depuis longtemps comme La Cinéscénie issus d’une initiative locale, même si le rayonnement est national. Depuis une trentaine d’années les collectivités locales (communes, départements et régions) se sont dotées de leur propre politique culturelle et jouent un rôle essentiel dans l’animation et la régulation de la vie culturelle locale. Ces politiques, souvent menées en partenariat avec les services de l’État, participent de plusieurs logiques : faciliter l’accès à la culture du plus grand nombre, soutenir la production artistique et les artistes, contribuer au développement économique et renforcer l’image des collectivités locales.\nDepuis le Traité de Maastricht, certains aspects de la culture font maintenant partie des responsabilités de l’Union européenne, dans le cadre des principes de subsidiarité. En particulier, l’Union européenne doit veiller à l’application de la politique linguistique européenne, qui pose certaines difficultés de mise en œuvre.\nNous distinguons donc deux modèles : le modèle américain, caractérisé par une alliance forte entre public et privé (où le privé joue un rôle prépondérant en matière purement culturelle), et le modèle européen, essentiellement public.\n\n\n==== Les artefacts ====\nLes artefacts — choses ou aspects de la culture matérielle — décrivent des valeurs et des normes d’une culture.\n\n\n== Les grandes manifestations de la culture collective ==\n\n\n=== Culture et art ===\n\nLa culture est aussi indissociable du patrimoine artistique, au sens où elle est un rattachement à des valeurs traditionnelles. Cet aspect de la culture est beaucoup plus marqué en Europe et en Asie, qu’en Amérique et surtout aux États-Unis, pour des raisons historiques évidentes.\nNéanmoins, les États-Unis admirent le patrimoine culturel européen {{Interprétation personnelle}}, car il s’agit de leurs racines culturelles : nous le constatons dans les acquisitions des œuvres d'art, dans leur présence dans les lieux artistiques (Paris, Bruges, Venise, etc.), dans les mécénats américains pour la restauration de quelques éléments symboliques du patrimoine européen (château de Versailles, etc.), dans les échanges musicaux (chefs d'orchestre, etc.), etc. Le respect des Américains pour l’histoire monarchique de la France paraît surprenant au premier abord, mais il révèle cet attachement à un patrimoine historique qu’ils n’ont pas, et une reconnaissance au rôle joué par la France dans l’Histoire et dans la défense des libertés aux États-Unis.\nLorsque est évoqué le patrimoine, on a tendance à penser au patrimoine bâti et à l’architecture, mais c’est aussi la sculpture, la peinture, le vitrail, la musique, la littérature, le folklore, la langue. Depuis plusieurs années, l'UNESCO a développé un programme en direction du patrimoine immatériel (convention de 2003 pour la sauvegarde du patrimoine culturel immatériel avec 3 actions clés :\n\nla liste du patrimoine immatériel nécessitant une sauvegarde urgente ;\nla Liste représentative du patrimoine culturel immatériel de l’humanité (Géants et dragons processionnels de France et de Belgique ou en Italie, le Canto a tenore qui est un chant pastoral sarde) ;\nle Registre de bonnes pratiques de sauvegarde.\nLe patrimoine est par ailleurs très riche aussi en Asie et en Afrique du Nord, comme c'est le cas par exemple pour les civilisations chinoise, indienne, arabe et berbère; et le patrimoine de l’Afrique noire (arts premiers) commence aussi à être redécouvert par la communauté internationale.\n\n\n=== Culture et langage ===\nVoir aussi : Catégorie langue et culture\nLa langue est probablement, dans les sociétés humaines, ce qui permet le mieux de véhiculer une culture, tant orale qu’écrite. C’est ainsi que la culture française s’est développée dans l’Europe des Lumières, en fait essentiellement parce qu’elle était parlée dans plusieurs cours princières. Cette prééminence du français était due au rayonnement culturel de la France au XVIIIe siècle, et à l’admiration que des souverains étrangers (en Prusse, en Russie, etc.) portaient, à tort ou à raison, aux souverains français.\nCette prééminence avait en réalité été préparée par l’édit de Villers-Cotterêts, signé par François Ier en 1539, qui établissait le français comme langue officielle, c’est-à-dire comme langue de l’administration et du droit (écrit). Dans ce sens, Richelieu fonde en 1635 l’Académie française, qui sert dès lors de régulateur et d'académie linguistique officielle. Au XVIIe siècle, de grands écrivains comme Corneille, La Fontaine, Molière, Racine ou encore Madame de Lafayette, donnèrent au français classique ses lettres de noblesse.\nAujourd’hui, la langue anglaise est devenue une langue véhiculaire, porteuse d’un grand nombre d’informations dans des domaines comme le militaire, la finance, la science, et aussi et surtout l’informatique, la plupart des langages informatiques étant historiquement formés sur des mots de la langue anglaise. Les normes, en particulier comptables (l’informatique étant issue à l’origine de la comptabilité générale), tendent à imposer un certain modèle culturel.\nEn France, après la Seconde Guerre mondiale, certains choisirent de réagir contre cette forme d’« impérialisme linguistique » en établissant des liens culturels avec les pays de langue française dans le monde : la francophonie. La protection de la langue française est aujourd’hui intégrée dans le droit français : article 2 de la Constitution de 1958, loi Toubon. Également,dans certaines régions francophones, tel que le Québec, des lois ont aussi été mises en place pour contrer la propagation de l'anglais.\nNous pouvons également dénoter la présence de liens culturels qui s'établissent autour de l’espagnol (entre l’Espagne et l’Amérique du Sud).\nL’arabe est également un bon exemple des liens culturels établis autour de cette langue parlée le plus souvent dans le monde musulman, et qui se développa en même temps que la grande civilisation musulmane entre le VIIIe et le XVe siècle.\nLe multilinguisme est, au moins officiellement, reconnu dans la politique linguistique de l'Union européenne, comme portant une valeur de diversité culturelle.\nLe langage étant l’un des modes de communication les plus importants (mais pas le seul), nous voyons apparaître des modèles linguistiques de communication fondés sur les fonctions du langage. Dans le schéma de Jakobson, par exemple, nous voyons ces concepts culturels liés au message lui-même, contenus notamment dans le code de communication.\n\n\n=== Culture et technique ===\nSciences et techniques sont en interaction permanente, puisque les techniques sont les applications des sciences dans la société. Parler des manifestations techniques de la culture revient donc à aborder ses relations avec les sciences.\nNous pouvons constater, depuis plus de trois siècles, une incompréhension entre les sciences (plus précisément les sciences « exactes ») et la culture, voire à des conflits.\nJacques Ellul a notamment développé la thèse selon laquelle la technique s'auto-accroît, imposant ses valeurs d'efficacité et de progrès technique, niant l'homme, ses besoins, et notamment sa culture.\nClaude Allègre note, dans Un peu de science pour tout le monde :\n\n« Dans un monde que la rationalité façonne, l’irrationalité tend à prendre le pouvoir, comme le montre l’essor sans précédent des astrologues, cartomanciens, et sectes de tout poil. La raison principale de cette dérive est qu’au nom d’une spécialisation nécessaire et toujours exigeante, les scientifiques se sont isolés et ont laissé la science s’abstraire de la culture générale. Or, il n’y a pas d’avenir pour un savoir humain, quel qu’il soit, en dehors de la culture, et il ne saurait être de culture dans le monde d’aujourd’hui qui tienne la science à distance ».\nLe philosophe Hans Jonas montre en effet, dans le Principe responsabilité (1979), que l’homme tend à adopter, vis-à-vis de la science et surtout de ses applications technologiques, un comportement prométhéen. Il prône le principe de précaution et se trouve à l’origine des principes philosophiques du développement durable.\nL’astrophysicien Jean Audouze, ancien directeur de l’Institut d’astrophysique de Paris, dresse le même constat, et appelle de ses vœux une réconciliation entre la science et la culture.\n\n\n== Importance et place de la culture collective ==\n\n\n=== La diversité culturelle dans les communautés humaines ===\nNous pouvons distinguer à travers le monde les cultures écrites et les cultures orales.\nLa langue, écrite ou orale, joue ainsi un rôle essentiel dans l’élaboration d’une forme de connaissance sociale, qui est la pensée du sens commun, socialement élaborée et partagée par les membres d’un même ensemble social ou culturel. Nous appelons quelquefois cette connaissance commune une représentation sociale.\nDans le domaine de l’archéologie et de l’anthropologie, la culture se définit comme étant l’ensemble des connaissances et des comportements qui caractérisent une société humaine, ou plus généralement un groupe humain à l’intérieur d’une société.\nSeulement quelques cultures sont parvenues à l’état de civilisation dans l’Histoire de l’humanité.\nMême s’il existe une culture dominante dans une société, généralement formée autour de la culture de l’élite, il se forme toujours des groupes sociaux dont les intérêts, les pratiques, sont particuliers par rapport à la culture dominante. Nous trouvons ainsi diverses formes de cultures, comme la culture populaire, la culture de masse, la culture de jeunesse, ou ce que l’on appelle la subculture (ou culture intime).\nDans la définition que donne l'UNESCO du patrimoine culturel immatériel, la diversité culturelle apparaît comme un élément déterminant :\n\n« Ce patrimoine culturel immatériel, transmis de génération en génération, est recréé en permanence par les communautés et groupes en fonction de leur milieu, de leur interaction avec la nature et de leur histoire, et leur procure un sentiment d'identité et de continuité, contribuant ainsi à promouvoir le respect de la diversité culturelle et la créativité humaine ».\nLa culture enfantine se distingue de celle des adultes, car les systèmes de représentation d’un enfant et d’un adulte sont nécessairement différents.\nFaire dialoguer des personnes de cultures différentes peut nécessiter une médiation interculturelle. Des personnes se sont spécialisées dans la médiation culturelle.\nSelon Christian Puren, il existerait plusieurs composantes à la compétence culturelle : transculturelle, métaculturelle, interculturelle, pluriculturelle, co-culturelle. Cette distinction permet d'analyser les situations où elles s'articulent, se combinent ou se superposent.\n\n\n=== La culture par rapport à la nature ===\nBeaucoup de personnes aujourd’hui identifient souvent la culture ou la « civilisation » à un état évolué de l’humanité, qui s’opposerait, selon eux, à l’état sauvage, la « nature » étant un état sauvage selon eux. Beaucoup de projets réalisés du XVIIIe siècle au début du XXe siècle, qui eurent lieu dans le cadre de la révolution industrielle, s'orientèrent dans le sens précédent.\nTel n’était pourtant pas le cas de beaucoup de philosophes des Lumières, comme John Locke qui fonda la philosophie politique sur la loi de la nature (law of nature), Robert Boyle, auteur d’ouvrages sur la méthode expérimentale (voir philosophie de la nature), Jean-Jacques Rousseau (rêveries d’un promeneur solitaire), Samuel von Pufendorf (qui inspira la constitution des États-Unis), ou de nombreux courants de peinture au XIXe siècle (école de Barbizon, impressionnisme, etc.).\nDans les dernières décennies, de nombreux philosophes se sont inquiétés des rapports avec la nature (René Dubos, Hans Jonas, etc.).\nSelon la philosophie moderne, et en particulier dans le sillage de Claude Lévi-Strauss, nous considérons généralement que la culture est naturelle à l'homme, dans le sens où tous les hommes en ont une et qu'un quelconque « état de nature » (état pré-culturel) ne serait qu'une pure fiction. Pour ce thème, voir l'article Jean-Jacques Rousseau.\nDes découvertes récentes montrent que la nature, le biologique, influence la culture. Par leurs recherches, Robert Stoller et ses collaborateurs ont montré que, dans des cas d'erreur sur la détermination du sexe à la naissance résultant d'une anomalie biologique non apparente, des forces de la nature agissent « sur les attitudes et comportements d'un enfant à travers ses jeux, son habillement, ses choix de partenaires de jeu, etc., autrement dit, que l'inné peut influencer l'acquis ».\nVoir aussi : état de nature, Philosophie de la nature, développement durable, Droits de la nature.\nMême si la culture physique était à l’origine cantonnée aux gymnases, le développement des activités sportives modernes tend à se rapprocher de la nature : alpinisme, ski (notamment le ski de fond), cyclisme, kayak, canyoning, etc.\n\n\n=== Le facteur culturel dans la mondialisation ===\nLa mondialisation fait sans aucun doute intervenir des enjeux culturels considérables. Ainsi, après la fin de la guerre froide, les différentes sociétés ont parfois vécu ce que nous appelons le choc des civilisations.\nDepuis la chute du mur de Berlin (1989) apparaît ainsi un modèle prédominant, le modèle anglo-saxon réputé « libéral », mais où, en fait, nous trouvons un engagement très fort de la puissance publique américaine dans l’industrie de l’armement et l’industrie informatique. L’emprise américaine est particulièrement forte sur les aspects culturels, et joue sur les interactions multiples (entreprises, partenariats avec des ONG) à partir des composants fondamentaux de la culture (valeurs, normes, institutions, artefacts). L'influence socioculturelle s'exerce par l'intermédiaire du social learning, et de ses composantes que sont l'enseignement, la langue, et le cinéma.\nCe modèle anglo-saxon, appuyé sur l’anglais comme langue véhiculaire, tend à imposer certains modes de fonctionnement dans les institutions mondiales, notamment commerciales, qui, selon certains observateurs, peuvent traduire une forme d’impérialisme culturel et linguistique.\nLe développement de la culture de masse depuis les années 1930, dans le sillage de l’américanisation, a favorisé des modes de consommation et de production qui ne sont plus forcément aujourd’hui compatibles avec les contraintes sociétales contemporaines. Dans le sixième essaicomposé pour la Crise de la culture, Hannah Arendt analyse la culture de masse, transformation de l'objet culturel en un loisir, pour ensuite proposer l'attitude à adopter vis-à-vis de l'art pour ne pas le soumettre à la logique de la société de consommation.\nFace à cette forme de domination, certains pays réagissent en prônant la diversité culturelle, et s’organisent en conséquence.\nEn France, l’expression exception culturelle recouvre l'ensemble des solutions adoptées pour défendre la diversité culturelle. Elles passent par des formes d’action concentrées autour de l’État (aides publiques et subventions aux différentes formes de médias, comme les décrets dits Tasca de 1990) mais aussi par la démarche privée grandissante et l'effort de qualité sur le matériel artistique.\n\n\n=== La culture par rapport au patrimoine ===\n\nSpontanément, l'expression patrimoine culturel fait penser à un patrimoine matériel (sites, monuments historiques, œuvres d'art, etc.). L'UNESCO a établi en 1972 une liste du patrimoine mondial, composée de plusieurs centaines de sites dans le monde.\nCette conception du patrimoine a évolué depuis une quinzaine d'années. Nous lui avons d'abord adjoint une liste Mémoire du monde (1992), qui recense les collections documentaires d'intérêt universel (déclaration des droits de l'homme et du citoyen, instauration du système métrique, mémoire du canal de Suez, etc.).\nEn 1997, la notion de patrimoine oral et immatériel de l'humanité a été définie par l'UNESCO.\nNous nous orientons donc progressivement vers une conception du patrimoine qui inclut à la fois un patrimoine matériel, mais aussi un patrimoine culturel immatériel (PCI).\nCe changement de conception du patrimoine n'est pas sans conséquences sur les représentations sociales et la psychologie sociale des communautés, puisque les traditions vivantes (carnaval de Binche par exemple) et documentaires sont reconnues au même titre que les monuments et œuvres d'art du passé.\nLorsque des effets similaires se produisent sur un ensemble d’individus appartenant à une même communauté, nous parlons alors de biais culturel.\n\n\n=== Les relations entre culture et entreprises privées ===\n\nL’objectif des entreprises n’est pas le plus souvent de produire de la culture. Néanmoins, et même dans les secteurs autres que la culture, d’une part, nous pouvons trouver d'une part que les liens avec les activités culturelles sont accrus, et d’autre part que la notion de culture d'entreprise se développe, notamment avec l’apparition de chartes définissant les valeurs partagées des personnes travaillant dans une même entreprise.\nHistoriquement, ce fut la création des comités d'entreprise qui permit d’abord aux employés de bénéficier d’activités culturelles proches de leur lieu de travail (prêt de livres, de disques, etc.).\nPlus récemment, les activités de mécénat se sont multipliées, afin de renforcer l’image des entreprises : par exemple le sport (voile, tennis, football, cyclisme, etc.), pour donner une image d’esprit d'équipe.\nLe mécénat tend à s’ouvrir aujourd’hui à des activités plus artistiques. Nous voyons par exemple des entreprises privées participer à l’organisation d’expositions. Ainsi une entreprise du secteur pétrolier peut trouver des intérêts à participer à des expositions en relation avec la culture arabo-musulmane par exemple.\nDans le cadre de stratégies de développement durable et de responsabilité sociétale, nous observons aujourd’hui une multiplication des messages des entreprises autour de chartes d’entreprise, et de mécénats culturels ou sociaux. Ces différents aspects ont pour objectif de renforcer l’image de l’entreprise.\nCe type d’activité est très naturel aux États-Unis, où les relations entre entreprises et ONG s’établissent facilement. Ce mode de fonctionnement décentralisé et privé n’est pas encore totalement passé dans les mœurs dans beaucoup de pays européens, particulièrement en France, où la puissance publique joue traditionnellement un rôle important. Les ONG culturelles peuvent pourtant favoriser l’éducation dans les pays en développement (en Afrique par exemple), et renforcer les liens.\nNéanmoins, si l’entreprise considère le mécénat comme de la communication pure dans ses rapports d’activité annuels (voir responsabilité sociétale), cela peut cacher dans certains cas des insuffisances dans les stratégies.\nLa culture d'entreprise, impulsée par les décideurs, et expliquée aux employés et aux parties prenantes de l’entreprise, devrait ainsi participer d’une manière générale à la construction d’une culture stratégique d’entreprise.\n\n\n== Évolution, diffusion et sélection culturelles ==\n\n\n=== Principes généraux de l’évolution culturelle ===\nLes cultures concernant la seule espèce humaine, et que nous pouvons repérer dans le vivant au lien étroit qu'elles entretiennent avec le langage symbolique et avec les formes spécifiques d'organisation, les techniques et technologies qui en découlent, se modifient sans cesse depuis leur émergence, il y a plusieurs centaines de milliers d'années. Elles se situent dans le prolongement des cultures des primates qui furent nos ancêtres, et qui ressemblaient plausiblement en partie à celles qui sont encore celles de « nos cousins » les grands singes. Toutefois, entre l'utilisation de la voix (dans l'aria des gibbons) ou le recours à l'instrumentation simple, voire l'existence de relations sociales très complexes (chez les chimpanzés, comme le démontrent les travaux de Jane Goodall), et le fonctionnement découlant d'une interposition d'une grille de signifiants commune entre les individus d'une même société et le monde, il existe une rupture. Celle-ci est difficilement niable, quels que soient les efforts - méritoires et fort utiles - pour abolir la notion de « propre de l'homme », qui reste à expliquer, notamment pour ce qu'il a entraîné une divergence assez extraordinaire entre le destin de notre espèce et ceux des autres, les plus proches.\nIl se manifeste deux lignes d'analyse antagoniques sur ce problème : l'une met en avant légitimement l'appartenance de l'humanité à la nature, et se défie des préjugés religieux (préférant situer l'origine de l'homme dans une décision divine), ou de la réticence largement partagée à accepter que nous sommes aussi une espèce animale. La seconde, fondant les sciences humaines et sociales, tente de résister à un « naturalisme » réducteur en défendant leur domaine propre, irréductible à d'autres niveaux de réalité : le domaine d'une anthropologie qui trouve précisément son territoire dans l'étude de ce que l'homme ne partage pas avec les autres animaux. Il faut sans doute dépasser les formes dogmatiques de cet antagonisme inévitable pour définir plus finement le rapport entre « continuité naturelle », entre cultures des primates et cultures humaines, et l'apparition d'une divergence spécifique. Pour ce faire, nous pouvons avoir recours jusqu'à un certain point à l'analogie entre la « longue évolution » (du vivant) et la « très courte » (de la culture humaine) : des biologistes (comme Jean Claude Ameisen) ont étudié l'histoire des bactéries, afin de comprendre l'incroyable complexité des mécanismes assurant vie et mort des cellules dans les organismes multicellulaires. Ils concluent à la nécessité de reconstituer des « époques disparues », pour interpréter la situation présente, et comprendre des phénomènes comme le cancer. D'autres biologistes se sont intéressés davantage à l'histoire des espèces elles-mêmes : dans tous les cas, l'analogie avec les histoires humaines se révèle heuristique, quitte à payer le prix de l'anthropomorphisme en dotant les gènes ou les cellules de traits\nhumains intentionnels comme des « intérêts », ou des « stratégies ». En revanche, les spécialistes des sciences humaines utilisent peu le recours aux savoirs biologiques. Ils ont sans doute tort en partie, mais leurs arguments n'ont rien à voir avec une variante du Créationnisme : ils tentent seulement de mettre au point des outils d'analyse qui ne soient pas d'abord importés d'autres disciplines, alors que dans leur propre domaine (notamment pour la période de moins de 30 000 ans pour laquelle ils disposent de traces incontestables de la culture symbolique : rites funéraires, représentations, systèmes de signes), la diversité et la confluence, bref le mouvement des cultures, semble obéir en priorité à des lois spéciales.\n\n\n==== Analogies avec l'évolution biologique ====\nTout comme il y a une évolution biologique, certains éthologues, ainsi que plusieurs généticiens, estiment qu’il y a une évolution culturelle, et que cette évolution se fait par mutation, puis est transmise par des « gènes » de la culture, appelés mèmes, qui subissent une pression sociale et environnementale, aboutissant à leur disparition ou au contraire à leur expansion (propagation).\nLa spécificité durable des cultures humaines est qu’elles fonctionnent comme des « conversations politiques » entre positions différentes, des processus de propositions-objections, réorganisant constamment les collectifs sociaux. La disparition d’une culture n’est donc pas nécessairement la « mort » d’un organisme, mais le passage à une autre configuration conversationnelle ; l’abandon de certaines métaphores collectives pour d’autres. L'analogie avec l'évolution des formes vivantes demeure intéressante et fructueuse car, comme les cultures langagières humaines, les espèces biologiques sont les produits d'une histoire : elles ne « meurent » pas comme les organismes, mais se transforment. Comme l’a montré l'anthropologue britannique Mary Douglas, aucune culture humaine n’est « homogène » : elle résulte toujours d’une différenciation interne entre partisans (ou adeptes) de valeurs plus individualistes, de valeurs plus collectives, de solutions organisationnelles hiérarchiques et enfin de formes de résistance passive ou active à toutes les valeurs en vigueur. Même dans les sociétés dites — à tort — « primitives » et supposées « sans histoire », il n’existe pas de stabilité culturelle, de consensus sans résistance, d’unicité sans variations individuelles ou collectives. De la même façon, il n'existe pas d'espèces « homogènes » constituées d'individus tous identiques, toute espèce se caractérise en effet par un répertoire de gènes communs mais aussi une diversité génétique entre les individus qui la composent. Dans une espèce donnée, l'apparition et la diffusion de nouveaux allèles résultera d'une compétition au sein du pool génétique, elle aussi marquée par une « résistance » au changement quantifiable en termes de dérive génétique.\nL'analogie entre évolution biologique et évolution culturelle doit toutefois être mesurée : il ne s'agit pas des mêmes espaces de temps, l'évolution du vivant courant sur des centaines de millions d'années, alors que les cultures humaines se distinguent des cultures des autres primates par le fait qu'elles se développent probablement seulement depuis quelques centaines de milliers d'années, certains linguistes datant même l'émergence du langage symbolique à moins de 60 000 ans.\n\n\n==== Coévolution gène-culture ====\nD'autres liens plus directs ont été proposés entre l'évolution des cultures humaines et l'évolution biologique de l'espèce humaine sous le concept de coévolution gène-culture. Selon cette théorie développée par les sociobiologistes Charles J. Lumsden et Edward O. Wilson au début des années 1980, les traditions culturelles peuvent être décomposées en culturgènes, c'est-à-dire en petites « unités » de culture. La transmission culturelle est donc fortement influencée par la nature de l'esprit humain qui est le produit d'une évolution biologique. Mais réciproquement, un comportement culturel peut aussi favoriser une évolution génétique via la stabilisation de certains gènes qui donnent un avantage adaptatif dans le groupe où ce comportement culturel est observé.\n\n\n==== Histoire et devenir des cultures humaines ====\nDepuis que les primates humains ont adopté le langage symbolique pour représenter leurs relations, celui-ci les a entraînés dans un mouvement rapide qui les distingue des cultures des autres primates (telles que les décrit par exemple l’éthologue Frans de Waal, lorsqu’il parle de « politique du Chimpanzé ») : les mots fixés par les systèmes de signifiants ne sont en effet jamais assez précis et englobants pour empêcher la controverse. Ainsi l’histoire des cultures (à commencer par celle des mythes étudiés par Claude Lévi-Strauss) est-elle celle d’une sorte de « course-poursuite » entre différentes façons de « prendre la vie ».\nIl est possible que la culture mondiale en formation réduise la richesse des possibilités des milliers de cultures encore existantes, mais elle pourra difficilement absorber dans un modèle unique les différentes « passions fondamentales » dont elle est le lieu d’expression, non seulement dans l’art ou la religion, mais aussi dans l’activité pratique et dans le débat politique.\n\n\n=== Culture et transmission, la Toile ===\nConscients de l’importance des médias (journaux, radio, téléphone, télévision, etc.), dans la diffusion de la culture, les gouvernements ont souvent eu la tentation de contrôler la diffusion des informations par la prise de contrôle des médias. Cela prit parfois des formes de propagande, soit via l’art, ou la nationalisation des moyens de diffusion par l’État. Cette forme de communication fait penser à l’apparition de l’imprimerie au XVe siècle car cette nouvelle manière de diffuser l'information bouleversa la société européenne, pour finalement contribuer fortement aux développements liés à la Renaissance, à travers notamment les grandes découvertes.\nÀ l’époque du web, l’approche moderne pour appréhender la diffusion de la culture par les médias, mais aussi par la langue, est sans doute celle de la médiologie. Ce qui caractérise aujourd’hui la diffusion par les médias, spécifiquement internet, c’est que l’individu n’est plus seulement destinataire de l’information (radio, télévision) ou émetteur dans une relation un à un (téléphone). Il peut aussi émettre à un grand nombre d’individus, par le biais de forums, messageries, blogs, etc. Une communauté féministe s’intéressant au cyberespace, à internet et aux technologies numériques utilise le terme cyberféminisme. Les artistes Nancy Paterson, Olia Lialina et le collectif d'artistes australiennes VNS Matrix, illustrent cette réflexion. Sadie Plant, comme Donna Haraway, figurent parmi ses théoriciennes.\nÀ notre époque, nous vivons un passage de la culture de l’écrit à une culture de l’information numérique immatérielle. Cette transformation radicale n’est pas sans poser des problèmes de propriété intellectuelle. Par exemple, l’industrie du disque peut être gravement menacée par la multiplication des actes de copie sans redevance.\nUn autre aspect significatif de cette mutation est le fait que les bibliothèques sont maintenant amenées à s’ouvrir aux médias numériques. Les bibliothèques sont par ailleurs de plus en plus appelées médiathèques, puisque le support du média n’est plus seulement le papier, mais un support numérique. Il s’agit alors de bibliothèques numériques. La sélection sur critères des ouvrages sur des écrans informatiques permet de trouver plus facilement l’ouvrage dans les rayonnages, et l’information recherchée.\nLorsque la médiathèque renferme des jeux, il s’agit alors d’une ludothèque.\nLe nombre de sites web dans chaque pays, et notamment le nombre de sites web par habitant, est un indicateur de la diffusion contemporaine de la culture, autour de la langue.\nRégis Debray pense que la transmission de la culture comporte une forte composante de croyance et de sacré. Selon lui, après deux premières révolutions, celle du codex (la Bible), et celle de l'imprimerie, l'humanité vit aujourd’hui une nouvelle révolution qui s'appuie sur les technologies de l'information et notamment sur la Toile.\n\n\n=== Culture et zones de contact entre civilisations ===\nL’Histoire montre que les zones de contact entre civilisations peuvent être sources de conflits, ou extrêmement fructueuses sur le plan des échanges culturels.\nNous pouvons citer par exemple les échanges maritimes dans la Grèce antique entre les cités et leurs colonies (Élée, Phocée, etc.), dans la Rome antique, Venise, les zones de contact en Espagne entre musulmans et chrétiens (Califat de Cordoue), la Syrie après les conflits des Croisades, la route de la soie, le royaume de Roger II de Sicile (qui apporta une connaissance cartographique précieuse à l’Occident à partir du savoir arabo-musulman, à Palerme ; les contributions de Al Idrissi en sont emblématiques.), les voyages de missionnaires et d’explorateurs, le commerce à partir de Bruges (villes hanséatiques et relations maritimes avec le sud de l’Europe), le protectorat français au Maroc, etc.\nC’est par ce type d’échanges que de nombreux traités scientifiques et philosophiques sont parvenus en occident, depuis la Grèce antique, l’Asie, la Mésopotamie, l’Inde, ainsi que des techniques très utiles : boussole, sextant, informations cartographiques, papier, imprimerie, chiffres arabes, etc.\nLe 21 mai 2022, journée mondiale de la diversité culturelle pour le dialogue et le développement, Audrey Azoulay, directrice générale de l’UNESCO depuis 2017, proclame que l'ambition de cette journée et des conventions culturelles de l'organisation, est de « rappeler que la diversité dans l’ordre de la culture est aussi nécessaire que dans l’ordre du vivant. »\n\n\n== Culture générale d’un individu ==\nLa culture d’un individu, aussi appelée culture générale, correspond à l’ensemble des connaissances qu’il a sur le monde.\nElle est en partie construite par l’éducation et l’enseignement, mais comprend de surcroît une part de construction active de la part de l’individu. Elle comprend aussi une dimension de structuration de l’esprit, vis-à-vis de l’ensemble des connaissances : La culture est ce qui reste lorsque l’on a tout oublié (attribué en général à Édouard Herriot). Cette structuration donne au sujet cultivé la capacité de rattacher facilement un quelconque domaine d’étude à ses connaissances. C’est la culture générale.\nAinsi, la culture générale peut inclure des connaissances aussi diverses que l’histoire, la musique, l’art, la littérature, les sciences, l’astronomie, la géographie, la philosophie, le cinéma, le sport, etc.\nNous remarquons cependant que cette conception de la culture, qui peut paraître élitiste, correspond en fait à la définition de la culture individuelle. Les cultures de différents groupes sociaux (culture populaire par exemple) peuvent comporter des formes de connaissances plus variées ou plus particulières.\nPar rapport à ces formes de culture, la culture générale est le fond de culture minimal que devrait posséder un individu pour pouvoir s’intégrer dans la société.\n\n\n== Notes et références ==\n\n\n=== Notes ===\n\n\n=== Références ===\n\n\n== Bibliographie ==\nHannah Arendt (trad. Patrick Lévy), La crise de la culture : huit exercices de pensée politique, Paris, Gallimard, coll. « Folio », 2008, 380 p. (ISBN 9782070325030, lire en ligne)\nMario d'Angelo, Isabelle Goubie, Diversité culturelle et dialogue des civilisations, l'évolution des concepts de 1990 à 2001, coll. Innovations & Développement, Idée Europe, Paris, 2002  (ISBN 290994106X).\nR. Boyd, J. Silk, L’Aventure humaine : des molécules à la culture, Bruxelles, 2004, trad. de la 3e éd.  (ISBN 2-8041-4333-3).\nAnne Chalard-Fillaudeau, « Les « Cultural Studies » : une science actuelle ? », L'Homme & la Société, vol. 149,‎ 2003, p. 31-40 (DOI 10.3917/lhs.149.0031, lire en ligne, consulté le 6 avril 2023)\nPaul Claval. Géographie culturelle, Paris. Colin, 1995\nJérôme Clément, La Culture expliquée à ma fille, Paris, Plon, 2000  (ISBN 2020395886).\nD. Cuche, La Notion de culture dans les sciences sociales, Paris, 2004, 3e éd. (Repères, 205)  (ISBN 2-7071-4264-6).\nRégis Debray, Transmettre, Odile Jacob (1997), traduit en anglais en 2000 (transmitting culture).\nJacques Demorgon, Complexité des cultures et de l'interculturel. Contre les pensées uniques, Economica, 2010  (ISBN 271784841X).\nJacques Demorgon, L'histoire interculturelle des sociétés. Pour une information monde, Economica, 2002.\nJacques Demorgon, Critique de l'interculturel. L'horizon de la sociologie, Economica, 2005  (ISBN 2717850430).\nJacques Demorgon, Les sports dans le devenir des sociétés, L'Harmattan, 2005  (ISBN 2747578496).\nArnaud Dephalese, France : Dictionnaire critique de civilisation française, Coneuwe, 2024.\nErwan Dianteill, dir., La culture et les sciences de l'homme - Un dialogue avec Marshall Sahlins, Éditions Archives Karéline, 2012, 264 p.  (ISBN 978-2-35748-095-7).\nJean Dubuffet, Asphyxiante culture, Édition de Minuit, Paris, 1986  (ISBN 2-7073-1084-0)\nDamien Ehrhardt, Soraya Nour Sckell (éd.) Interculturalité et transfert, Berlin, Duncker & Humblot, 2012. Beiträge zur Politischen Wissenschaft 174  (ISBN 978-3-428-13774-9).\nArnold Groh, Theories of Culture, Londres, Routledge, 2019  (ISBN 9781138668652).\nBernard Lahire, La Culture des individus. Dissonances culturelles et distinction de soi, Paris, La Découverte, 2004.\nPaul-Edmond Lalancette, La nécessaire compréhension entre les sexes, Québec, 2008, p. 100 à 150,  (ISBN 978-2-9810478-0-9).\nDominique Lestel, Les Origines animales de la culture, Paris, 2003 (Champs, 543)  (ISBN 2-08-080069-8).\n(en) Ingrid Piller, Intercultural communication : a critical introduction, Edinburgh, Edinburgh University Press, 2017, 227 p. (ISBN 9781474412926, lire en ligne [programme d'ordinateur])\n« La culture générale », Atala, Cultures et sciences humaines, no 14, 2011, 310 p. [1]\nPeter Sloterdijk, La Domestication de l’être, Édition Mille et une nuits, 2000  (ISBN 2-84205-503-9)\nValère Staraselski, Culture pour tous, Collectif d'auteurs, éditions Bérénice, collection Cétacé  (ISBN 2-911232-41-0)\nAlbin Wagener, L'échec culturel, Peter Lang, Bruxelles, 2015  (ISBN 978-3-0352-6508-8)\n« Égalité hommes-femmes | Culture and Creativity », sur culture.ec.europa.eu (consulté le 29 mars 2023)\n\n\n=== Sur les institutions culturelles ===\nMario d'Angelo, Perspectives de gestion des institutions musicales en Europe, Série Activités et Institutions musicales, no 4, OMF (Observatoire Musical Français), Université Paris-Sorbonne, 2006.\nSylvie Octobre (dir.) et Frédérique Patureau (dir.), Normes de genre dans les institutions culturelles, Ministère de la Culture - DEPS, coll. « Questions de culture », 2018, 168 p. (ISBN 9782724623307, lire en ligne)\n\n\n=== Sur les politiques culturelles ===\nAnne-Marie Autissier, « Quelles priorités pour les politiques culturelles européennes ? », Les Mutations de la sphère culturelle « Raison présente »,‎ 2006, p. 73-81 (lire en ligne)\nMario d'Angelo, Paul Vespérini, Les politiques culturelles en Europe, éditions du Conseil de l'Europe, Strasbourg, 1998-2001 (4 vol.)\nReine Prat et Geneviève Fraisse, Exploser le plafond : précis de féminisme à l'usage du monde de la culture, Paris, Rue de l'Echiquier, coll. « Les Incisives », 2021 (ISBN 9782374253107, présentation en ligne)\nLaurent Martin et Philippe Poirrier (dir), Démocratiser la culture ! Une histoire comparée des politiques culturelles, Dijon, Territoires contemporains, 2013.\nPhilippe Poirrier, L’État et la culture en France au XXe siècle, Paris, Le Livre de Poche, 2006.\nPhilippe Poirrier, Art et pouvoir de 1848 à nos jours, Cndp, 2006.\nPhilippe Poirrier, Les Politiques culturelles en France, Paris, La Documentation française, 2002.\nPhilippe Poirrier (éd.), Politique culturelle et patrimoines, Culture & Musées, janvier-juin 2007, no 9.\nSerge Regourd, L’Exception culturelle, Paris, Puf, 2002\nJ.M. Tobelem, P. Ory, L'Arme de la culture. Les stratégies de la diplomatie culturelle non gouvernementale, Paris, L'Harmattan, 2007, 264 p.\n\n\n=== Sur l'économie de la culture ===\nBibliographie Approches théoriques (économie de l'immatériel, économie de la culture, économie de l'information) - Économie des industries culturelles et des médias (1940-)\n\n\n=== Sur les femmes et la culture ===\nLinda Timmermans, L’Accès des femmes à la culture (1598-1715), Classiques Garnier, coll. « bibliothèque de la Renaissance / 1 » (no 26), 2023 (1re éd. 1993), 940 p. (ISBN 978-2-406-14844-9, lire en ligne).\nSylvie Duval, « La littéracie des femmes à la fin du Moyen Âge. Questions sur l’histoire de la culture, de la lecture et de l’écriture à travers des travaux récents », Médiévales,‎ automne 2018, p. 227-248 (lire en ligne).\nSylvie Octobre (dir.), Questions de genre, questions de culture, Ministère de la Culture - DEPS, coll. « Questions de culture », 2014, 152 p. (DOI 10.3917/deps.octob.2014.02, lire en ligne).\nBrigitte Gonthier-Maurin, « La place des femmes dans l'art et la culture : le temps est venu de passer aux actes », rapport d'information sénatorial, sur Sénat (France), 27 juin 2013 (consulté le 12 décembre 2024)\n(en) Sadie Plant, Zeros and ones : digital women and the new technoculture, London, Fourth Estate, 1998, 305 p. (ISBN 9781857026986, lire en ligne)\n\n\n=== Revues ===\nRevue Culture & Musées (à comité de lecture), (30 numéros en ligne en 2012 avec Persée, soit 417 contributions, 1992-2009) ; Culture et Musées publie des travaux de recherche inédits sur les publics, les institutions et les médiations de la culture. Chaque numéro est un ouvrage collectif et thématique traitant d'une question sous la direction d’un scientifique spécialiste choisi par le comité de rédaction.\nTrois revues pour repenser la culture au féminin : La Déferlante, « la première revue trimestrielle post-#metoo consacrée aux féminismes et au genre », fondée par Marie Barbier, Lucie Geffroy, Emmanuelle Josse et Marion Pillas ; Gaze, « la revue des regards féminins », fondée par Clarence Edgard-Rosa, Laura Lafon, Juliette Gabolde et Stella Ammar ; Censored, « le magazine qui explore la culture féministe et artistique émergente » \n\n\n== Articles connexes ==\n\n\n=== Culture collective et civilisation ===\nCivilisation\nÉcologie intégrale\nMédiation interculturelle\nSociété interculturelle\nSubculture\nTradition, Coutume, Folklore\nInterculturel, Multiculturel, Multiculturalisme, Transculturalité\nIntégration culturelle\nAcculturation\nBiais culturel\nCulture de masse\nPhilosophie de la culture\nCulture LGBT\n\n\n=== Culture individuelle ===\nHumanités\nSecret\n\n\n=== Manifestations de la culture ===\nArt - Peinture - Musique - Architecture\nLangue - Littérature\nPhilosophie - Éthique - Esthétique\nHistoire - Géographie\nScience - Technique\nPatrimoine (culture)\n\n\n=== Aspects sociaux ===\nSociologie des cultures\nÉvolution culturelle - Sélection culturelle - Mème - Capital culturel - Créatifs Culturels\nRayonnement culturel - Diversité culturelle - Révolution culturelle - Relativisme culturel - Médiation culturelle\nCulture populaire - Culture enfantine - Culture de jeunesse - Culture libre\nÉconomie de la culture - Industrie culturelle\n\n\n=== Organisations, administrations, agences ===\n\n\n==== Organisations intergouvernementales chargées notamment de la culture ====\nOrganisation des Nations unies pour l'éducation, la science et la culture (UNESCO)\nConseil de l'Europe\nOrganisation internationale de la Francophonie (OIF)\n\n\n==== Administrations ou agences chargées de la culture ====\nMinistère de la Culture et Direction régionale des Affaires culturelles (France)\nPro Helvetia pour la Suisse\nDepartment for Culture, Media and Sport (Angleterre / Royaume-Uni)\nNational Endowment for the Arts (États-Unis, gouvernement fédéral)\n\n\n==== Technique ====\nCatégorie:Culture informatique\nCulture stratégique\n\n\n==== Autres ====\nArtisanat - Commerce - Politique - Artificiel - Religion\n\n\n==== Faune et flore ====\nÉthologie et psychologie comparée : Culture animale chez les grands singes, les dauphins\nAgriculture - Culture vivrière - culture cellulaire\nLe suffixe « -culture », sur Wiktionnaire\n\n\n== Liens externes ==\n\nRessource relative à la recherche : Stanford Encyclopedia of Philosophy \nRessource relative à la santé : Medical Subject Headings  \n\n Portail de la culture   Portail de la société   Portail de la philosophie   Portail de la sociologie"
        },
        {
            "pageid": 1472299,
            "ns": 0,
            "title": "Culture générale",
            "content": "La culture générale, appelée également culture « G », désigne les connaissances en tout genre d'un individu, sans spécialisation. S'opposant à la connaissance disciplinaire (en), elle fait partie du projet humaniste, qui trouve ses origines dans la paideia grecque. Traduite par Cicéron sous le terme d'« humanitas », elle se mêle, lors de la Renaissance artistique, aux arts libéraux. Le projet d’une culture générale est intrinsèquement lié à des réflexions à propos de l'humanité, qu'elle soit conçue comme nature humaine ou comme dépassement de la nature par la culture (ou « seconde nature »). Durant la Renaissance, elle forme l'idéal de l'« honnête homme ».\n\n\n== Concept ==\n\n\n=== Dialogue entre les connaissances ===\nLa culture générale n'est pas une discipline académique, comme peuvent l'être l'histoire-géographie ou la physique. La culture générale regroupe plusieurs disciplines à partir d'un esprit commun, qui est celui du dialogue entre les connaissances. Elle représente l'indispensable moteur permettant une réflexion autonome.\nEn tant que dialogue, elle ne peut être une simple compilation de connaissances. Elle ne vise pas une érudition ou un savoir universel. La culture générale n'est pas non plus une connaissance mondaine, qui serait innée pour les uns ou étrangère aux autres.\n\n\n=== Apprentissage à l'apprentissage ===\nLa culture générale est une propédeutique à une connaissance plus spécialisée. Le Prix Nobel de Médecine de 1965 François Jacob écrit ainsi que l'utilité de l'école est de nourrir les esprits de culture générale pour leur permettre de penser. Il déclare que « Ce qui importe, à l'école, c'est moins d'acquérir en vrac le plus de connaissances possible que d'apprendre à apprendre. La gymnastique intellectuelle qu'implique la vie moderne exige de se familiariser aussi bien avec la littérature et l'histoire qu'avec les mathématiques et la biologie ».\nLe monde étant fait d'une complexe interconnectivité et s'étant spécialisé, la culture générale permet de penser le global de manière complexe. Devant l'impossibilité de se spécialiser en tout, la culture générale permet de disposer des outils pour saisir les tenants et les aboutissants de problèmes spécialisés.\n\n\n=== Terreau citoyen et idéal républicain ===\nLa culture générale fait partie de la culture républicaine française. Elle est entretenue par son système éducatif, de l'école primaire aux Grandes écoles, et est l'objet de curiosité de la part des pays de culture anglo-saxonne.\nLa philosophie est enseignée dans l'enseignement secondaire en France depuis 1808, d'abord en latin puis en français après les réformes de Victor Cousin pendant la monarchie de Juillet, en 1840. Cousin donne explicitement à l'enseignement de la philosophie un objectif de formation de l'honnête homme. Ainsi, il affirme, dans un discours prononcé à la Chambre des pairs le 3 mai 1844 :\n\n« Nous voulons que la philosophie de nos écoles soit profondément morale et religieuse, qu'elle fasse pénétrer dans les esprits et dans les âmes les convictions qui font l'honnête homme et le bon citoyen, les croyances générales qui servent d'appui à tous les enseignements religieux des divers cultes. La philosophie sert tous les cultes sans se mettre au service d'aucun d'eux en particulier. N'est-ce pas là une noble mission et ne serait-ce pas un danger et un malheur public que d'altérer le caractère d'un pareil enseignement ? Que deviendrait alors l'unité nationale ? »\n\nFace aux critiques du président Nicolas Sarkozy envers la présence d'une épreuve de culture générale aux concours de la fonction publique française, le mathématicien Wendelin Werner, lauréat de la médaille Fields, pouvait écrire, dans une lettre ouverte au président du 19 février 2009, publiée dans Le Monde :\n\n« Lorsque l'on me demande à quoi peut servir une éducation mathématique au lycée pour quelqu'un dont le métier ne nécessitera en fait aucune connaissance scientifique, l'une de mes réponses est que la science permet de former un bon citoyen : sa pratique apprend à discerner un raisonnement juste, motivé et construit d'un semblant de raisonnement fallacieux et erroné. »\n\n\n== Histoire ==\n\n\n=== Une origine antique ===\nLa question de la culture générale émaille plusieurs textes issus de l'Antiquité. Les sophistes grecs, professeurs itinérants, délivraient un savoir général et étaient connus pour avoir des connaissances en toute chose. Cette approche du savoir est critiquée par Platon, notamment à la fin de l’Euthydème, qui promeut la spécialisation. Le philosophe reconnaît néanmoins que l'apprentissage de culture générale est ainsi un moment nécessaire du projet philosophique en ce qu'elle permet d'acquérir des connaissances sans lesquelles la raison tournerait à vide.\n\nLa notion parvient aux romains par l'étude des textes grecs. Le philosophe Cicéron définit l’humanitas comme « le traitement à appliquer aux enfants pour qu’ils deviennent hommes » dans De oratore. La culture générale est donc un outil nécessaire de l'éducation. Dans les Tusculanes, le philosophe forge l'expression de « culture de l'âme » (cultura animi) : — Cicéron, Les Tusculanes, II, 13Cicéron distingue toutefois la culture générale de l'éducation : la seconde doit être prodiguée à l'enfant, tandis que la première doit accompagner l'homme tout au long de sa vie (De oratore).\n\n\n=== Un ancrage dans la modernité ===\nL’idéal de culture s’est transmis jusqu’à aujourd’hui, s’incarnant dans des modèles scolaires distincts. Cet idéal était discuté et abondé par différents penseurs au cours des siècles.\nRené Descartes fait de la culture générale un pivot de sa pensée. Selon la première règle pour la direction de l'esprit, « Le but des études doit être de diriger l’esprit de manière à ce qu’il porte des jugements solides et vrais sur tout ce qui se présente à lui ». La culture générale ne se définit donc pas par son objet, mais par sa visée : celle d'un support pour bien juger des choses.\nEmmanuel Kant poursuit dans son sens. Il oppose la nature (règne des causes déterminées par des lois empiriques) à la culture, cette dernière étant le règne de la liberté. La culture est par conséquent la marque de l’aptitude des hommes à se donner librement des fins qui lui plaisent, en général. La culture générale ne peut donc pas être réduite à contenu déterminé, puisqu'elle vise précisément à donner à l'homme la possibilité de choisir les fins qui déterminent son existence.\nSi les humanistes se sont concentrés sur la vertu de l'homme humaniste, le philosophe allemand George Friedrich Hegel critique le projet d'une culture qui ne viserait que l’épanouissement de l'individu subjectif. Le projet de la culture générale doit tenir compte de la valeur objective de la culture, c'est-à-dire de la valeur collective qui dépasse la simple satisfaction subjective. En d’autres termes, la culture en tant que processus individuel de formation ne doit pas viser aux simples fins subjectives de l’individu, mais à celles de la collectivité. Il soutient que l'apprentissage de la philosophie et de l'autonomie de la pensée est d'autant plus aisée pour l'individu cultivé que « les matériaux de son édifice sont déjà préparés dans la culture générale ».\n\n\n== Débats ==\n\n\n=== Utilité de la culture générale ===\nLa culture générale souffre aujourd’hui d’une crise qu’on a pu appeler « crise de l’école », en ce que les idéaux classiques et humanistes d’une culture générale ont été attaqués de part et d’autre.\nLa modernité a amené avec elle une certaine conception de l’économie et de la technique, ou de la « raison instrumentale », qui a pu conduire à mésestimer la culture générale voire à la mépriser, en la tenant pour « inutile » et « stérile ». La culture générale serait ainsi considérée en fonction de son utilité dans une production capitaliste ; or, n'ayant aucune finalité d'ordre productive ou professionnelle, elle serait mise de côté. Le sociologue Thorstein Veblen fait ainsi remarquer que « du point de vue de l'efficacité économique, les humanités sont des anachronismes qui nous handicapent. Les langues classiques sont de l'information qui, en gros, ne sert à rien ».\nPour le philosophe Hegel l'inutilité, c'est-à-dire l'impossibilité d'appliquer la culture générale à un processus de production, est dans la nature même de la culture générale ; elle est désintéressée. La culture générale permet de dépouiller les choses « du caractère mesquin et rabougri que leur imposent les circonstances extérieures ». La culture générale permet de voir « la chose même dans sa vérité, hors de tout intérêt égoïste ».\n\n\n=== Sélection par la culture générale ===\nLa culture générale est utilisée en France comme critère de sélection à diverses fonctions, dont notamment au sein de la haute fonction publique française. Les instructions officielles de 1925 présentent, dans le cadre élitiste de l'enseignement secondaire — moins de 5 % d'une génération obtient alors le baccalauréat — la culture générale comme une « préparation à rien […] qui rend apte à tout ».\nLa légitimité de la culture générale comme critère de sélection est remise en cause dans les années 2000.\nDurant la campagne en vue de l'élection présidentielle française de 2007, puis en tant que président, Nicolas Sarkozy provoque une polémique en critiquant l'épreuve de culture générale dans les concours de la fonction publique, faisant une allusion dépréciative envers La Princesse de Clèves. Cela provoque des critiques importantes du monde universitaire ; le théoricien de la littérature Yves Citton, par exemple, s'engage dans la bataille pour défendre la culture générale.\nRichard Descoings, partisan de la discrimination positive, décide de supprimer l'épreuve de culture générale au concours d'entrée de Sciences Po Paris en juin 2013. L'École normale supérieure de Lyon fait de même, critiquant l'épreuve pour son « caractère rhétorique […] qui encourage souvent les étudiants à formuler des idées superficielles en les agrémentant de quelques citations ». Dominique Meurs, sociologue à l'INED affirme également que « la dissertation de culture générale fait appel à une tournure d'esprit, une assurance, un goût pour l'abstraction, une façon de présenter ses idées, de mettre la bonne citation au bon endroit, qui s'apprend dans les milieux favorisés ». En 2008 le ministre de la Fonction publique, André Santini, qualifiait l'épreuve de culture générale de « discrimination invisible ».\nInversement, dans nombre d’emplois, la culture générale peut faire la différence au moment du recrutement, et fait l'objet d'examens divers. Ces examens sont justifiés en affirmant qu'ils démontrent, de la part du candidat, une capacité d'analyse et de synthèse, ainsi que d'ouverture hors de son corps de métier, qui serait nécessaire pour les emplois qualifiés.\n\n\n=== Caractère relatif de la culture générale ===\nLa culture générale a été la cible, dans le savoir même et dans les rangs mêmes de l'université, de deux critiques alternatives : l'une, issue de l'ethnologie, en faisait l'idéal d'une civilisation déterminée visant à l'universalité (thèmes du relativisme culturel, qui fait de l'idéal classique de la culture l'idéal d'une société déterminée) ; l'autre, sociologique, en faisait un simple moyen de sélection sociale.\nAinsi, selon Pierre Bourdieu :\n\n— Pierre Bourdieu, La Distinction, Éditions de Minuit, 1979, p.94La culture générale, en tant qu'elle est un outil d'ouverture sur l'altérité, peut autant être un antidote contre la fermeture d'esprit et l'ethnocentrisme. Pour le philosophe Jean-Marie Nicolle, le propre de la culture générale est précisément de permettre une interrogation sur soi-même et sur les autres. La critiquer comme ethnocentriste n'a ainsi pas de sens, car la véritable culture générale reprend les acquis de l'humanité en transcendant les particularités des peuples.\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\n\n\n=== Bibliographie ===\nHannah Arendt, « La crise de l'éducation » in La Crise de la culture, 1961\nPierre Bourdieu, La distinction. Critique sociale du jugement, Éditions de Minuit, 1979\nJacques Cantier, Lire sous l'occupation, Paris, CNRS éditions, 2019, 381 p. (ISBN 9782271093325).\nErnst Cassirer, Essai sur l'homme (en part. chap. II, III et VI)\nJacques Derrida, L'Autre Cap, 1991\nNorbert Elias, La civilisation des mœurs (premier tome, en français, de Sur le processus de civilisation)\nHerder, Une autre philosophie de l'histoire\nDenis Kambouchner, « La culture », in Notions de philosophie, tome III, Gallimard, 1991\nClaude Lévi-Strauss, passim (en particulier Race et Histoire, Tristes Tropiques, chap. 38, et « Les trois humanismes » in Anthropologie structurale II)\nHenri-Irénée Marrou, Histoire de l'éducation\nPic de la Mirandole, Discours sur la dignité de l'homme (Éditions de l'Éclat, accès libre)\nJean-Jacques Rousseau, Discours sur l'origine et les fondements de l'inégalité parmi les hommes, 1755, et préface de Narcisse\nThorstein Veblen, Théorie de la classe de loisir (en part. dernier chapitre), 1899\n\n\n=== Articles connexes ===\nCulture\nCivilité\nCivilisation\nHumanité\nHumanisme\n\n\n=== Liens externes ===\n\n Portail de la culture"
        },
        {
            "pageid": 840014,
            "ns": 100,
            "title": "Portail:Culture",
            "content": ""
        },
        {
            "pageid": 1104089,
            "ns": 0,
            "title": "Arts et lettres",
            "content": "Les Arts et les Lettres sont un domaine d'études qui englobe plusieurs éléments de culture. Les courants artistiques, les peintres et écrivains ayant marqué l'Histoire font partie de cette culture.\nIl y a trois profils principaux d'arts et lettres :\n\nLettres, qui survole les arts en général, qui met l'accent sur le volet littéraire et sur l'écriture\nLangues, tous les domaines des arts y sont abordés mais la formation est plus axée sur les langues et la traduction\nCommunication et cinéma, qui aborde tous les domaines relatifs à l'art, en plus des médias et d'axer sa formation autour du septième art.\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nOrdre des Arts et des Lettres\n Portail de l’éducation   Portail de la culture   Portail des arts"
        },
        {
            "pageid": 396305,
            "ns": 14,
            "title": "Catégorie:Thème dans l'art et la culture",
            "content": "Cette catégorie regroupe les catégories générales concernant les différents thèmes de l'art et de la culture."
        },
        {
            "pageid": 3603445,
            "ns": 14,
            "title": "Catégorie:Culture par lieu géographique",
            "content": ""
        },
        {
            "pageid": 5351276,
            "ns": 14,
            "title": "Catégorie:Culture par période",
            "content": ""
        },
        {
            "pageid": 124525,
            "ns": 14,
            "title": "Catégorie:Culture nationale",
            "content": ""
        },
        {
            "pageid": 656773,
            "ns": 14,
            "title": "Catégorie:Liste en rapport avec la culture",
            "content": ""
        },
        {
            "pageid": 176708,
            "ns": 14,
            "title": "Catégorie:Culture alternative",
            "content": "Regroupe les articles traitant de phénomènes culturels communautaires ou individuels, organisés le plus souvent sur le mode autogestionnaire, non mercantile et mondialiste. La culture alternative en opposition à la contreculture ne cherche pas à entrer en conflit avec la culture établie mais plutôt à la détourner vers des buts moins institutionnels ou marchands."
        }
    ],
    "Médias": [
        {
            "pageid": 15472,
            "ns": 0,
            "title": "Média",
            "content": "Le terme média désigne tout moyen de distribution, de diffusion ou de communication interpersonnelle, de masse ou de groupe, d'œuvres, de documents, ou de messages écrits, visuels, sonores ou audiovisuels (comme la radio, la télévision, le cinéma, Internet, la presse, les télécommunications, etc.). Ce terme est souvent utilisé comme l'abréviation du terme anglais mass-media ou médias de masse en français.\nL'expression médias de masse désigne les médias qui ont acquis une diffusion à grande échelle pour répondre rapidement à une demande d'information d'un public vaste, complétée dans de nombreux cas par une demande de distraction. La plupart des entreprises dites de média emploient des journalistes et des animateurs de divertissement. Ils recueillent dans un premier temps des informations auprès de sources d'information, en leur assurant la protection des sources d'information, ce qui leur permet d'acquérir une audience, et valorisent, dans un second temps, leur audience par la vente d'espaces publicitaires. À côté de ce modèle dominant, les chaînes de téléachat et les périodiques ne diffusant que des petites annonces et publicités sont aussi considérés comme des médias.\nDans les pays industrialisés, où les médias se sont largement développés, ils sont majoritairement détenus par de grands groupes industriels dont les dirigeants, proches du pouvoir politique, sont régulièrement critiqués pour instrumentaliser l'information à des fins partisanes plus ou moins reconnues (thèse de la « fabrication du consentement »). Mais l'avènement d'Internet, des TIC signe celui des médias alternatifs (blogs, réseaux sociaux…). L'information n'est plus alors forcément soumise aux règles déontologiques, notamment celle de la vérification par les faits, la multiplication et l'ampleur des canulars informatiques inaugurant selon certains une nouvelle « ère » : l'ère post-vérité.\n\n\n== Étymologie ==\nEn latin, media est le pluriel de medium (milieu, intermédiaire). Le mot français est issu de l’anglais mass(-)media, mot anglais lui-même formé sur l'usage italien media, venant aussi de la langue latine.\nL'écriture du terme media prête à discussion. Faut-il rajouter un s comme marque du pluriel sur un substantif déjà au pluriel ?[réf. souhaitée]\nLe terme media est maintenant rarement employé selon son orthographe latine, « Médias » désigne plusieurs supports et « Média » un support unique. Ces termes consacrés par l'usage commencent à se trouver dans la plupart des dictionnaires francophones,,. Le Grand dictionnaire terminologique et FranceTerme recommandent eux aussi « média » pour l’anglais media.\n\n\n== Médias et communication ==\nLes médias sont des outils de communication. Le choix d'un média dépend évidemment du type de communication recherché :\n\n\n=== Communication unilatérale et multilatérale ===\nSelon ses caractéristiques techniques propres, l'usage d'un média de masse est davantage approprié à un certain type de communication : par exemple, le média de type Presse Écrite (Le Monde, Le Figaro, etc.) semble plus adapté pour communiquer de manière unilatérale, les médias de type Réseaux sociaux (Facebook, Twitter, Snapchat, Instagram, etc.) semblent plus pertinents pour communiquer de façon multilatérale.\n\n\n=== Communication mise à disposition du public ===\nSelon le critère de « mise à disposition du public » employé par les juristes français, on distingue :\n\nles médias simples : dont la consultation par le destinataire est directe (ex : un journal, une revue, un livre, une affiche…),\nles médias autonomes : dont la consultation implique la détention d'un équipement par le destinataire (ex : magnétoscope, lecteur CD, micro-ordinateur, téléphone mobile…),\nles médias de télédiffusion : dont la consultation suppose que le destinataire soit connecté à des réseaux de diffusion, dans le cadre d'une offre n'autorisant qu'un degré d'interactivité réduit. (ex : réseaux hertziens terrestres, télédistribution par câble, satellites de diffusion directe…),\nles médias de télécommunication : idem que ci-dessus, avec cependant la possibilité de consulter une offre présentant un degré d'interactivité important.\n\n\n=== Communication média et hors-média ===\ncommunication « média » : en utilisant des moyens de communication tels que l'affichage, le cinéma, la presse, la radio, la télévision et maintenant les TIC (dont en particulier le Web).\n\n\n=== Les modes de traitement journalistiques et les énonciations éditoriales sur le web ===\nL’apparition d’Internet dans les années 1990 a démultiplié les sources d’information de presse en ligne. La presse écrite a en effet souhaité s’inscrire dans cette nouvelle modernité en créant des versions en ligne de ses journaux, mais les médias en ligne ne sont pas les seules sources présentant l’actualité sur le web, on compte également les infomédiaires, les blogs et les sites natifs de l’Internet.\nCes différentes sources présentent des disparités dans les modes de traitement de l’actualité et dans leurs énonciations éditoriales qui sont marquées notamment dans les articles traitant de l’actualité politique, thème plus ou moins enclin d’être sujet à controverse. On constate une hiérarchie dans le traitement de l’actualité sur le web étant donné que tous les sites d’information n’ont pas les mêmes façons de montrer leur point de vue sur les questions traitées. Chaque site d’actualité en ligne se construit une identité propre qui découle des plus ou moins grandes restrictions qui sont faites aux auteurs et qui se traduit par différents procédés :\n\nLes médias en ligne et infomédiaires qui se doivent de respecter les normes qui régissent leur activité de journaliste font le choix d’un compte rendu factuel de l’évènement. Il est relaté de façon distanciée, ne faisant en apparence émerger aucune prise de parti. Le traitement de l’information peut cependant grandement différer d’un titre à l’autre compte tenu des contrats de communication respectifs des groupes de presse. L’opinion du journaliste peut cependant être exprimée de façon très discrète par des stratégies de discours.\nLes blogs et sites natifs de l’Internet bénéficient d’une plus grande liberté quant au fait d’exprimer leur opinion. Les auteurs de ces sites présentent en général un engagement plus marqué se manifestant de diverses manières.\nLes stratégies énonciatives et discursives mises en place par les auteurs des articles sont les suivantes (non exhaustif) :\n\nLes images illustrant les articles peuvent être connotées ou non. La photographie peut être utilisée comme une simple illustration de l’article et donc neutre ou bien elle peut être connotée dans le but de faire passer un message, émettre une critique particulière.\nLes blogs et sites natifs de l’Internet qui sont plus libres d’émettre des prises de positions ne construisent pas leurs articles de la même façon que les médias en ligne, ils utilisent des procédés afin de marquer leur opinion tels que : de longs paragraphes argumentés et agrémentés de nombreux connecteurs logiques, des adresses directes au lecteur, du vocabulaire connoté (mélioratif ou péjoratif) voire familier. Ils n’hésitent pas à montrer leur implication en utilisant la première personne.\nLes médias en ligne plus cadrés par les normes qui dirigent l’activité de journaliste font passer leurs idées de façon plus discrète : dans le cas d’un article qui pourrait susciter les réactions d’acteurs politiques, le choix des personnes interrogées est un marqueur de prise de position si un parti politique est représenté plus qu’un autre. Le lexique joue également un rôle important même s’il est plus mesuré que dans les blogs et sites natifs de l’Internet.\n\n\n== Type de supports ==\nOn peut distinguer les types de médias suivants :\n\nl'affichage où les professionnels appliquent des techniques d'écriture publicitaires ;\nla presse écrite où les professionnels appliquent des techniques d'écriture journalistiques ;\nla radio où le journalisme utilise l'écriture radio ;\nla télévision par la diffusion de montages vidéos commentés ;\nles nouvelles technologies de l'information et de la communication, avec en particulier le Web, comme système hypertexte public de contenus numériques fonctionnant sur Internet. La sophistication des TIC (Technologies de l'Information et de la Communication) (voir les fonctionnalités du WEB 2.0) autorise des fonctionnalités très puissantes :\nd'identification de la cible, d'utilisation de cookies,\nde personnalisation ou de configuration du message, et\nde collecte d'informations en ligne.\nle cinéma utilise notamment le cadrage, les jeux de lumières ou même des bandes-sons pour mieux faire passer un message ;\nla bande dessinée utilise une juxtaposition de dessins, accompagnés de textes ;\nles médias tactiques (ou média de proximité). Il s'agit de moyens de communication alternatifs : affichage indoor, supports publicitaires (sac à pain, set de table, gobelet, boite à pizza…) ;\nles jeux vidéo, sur consoles (PS4, Xbox One, Nintendo Switch, etc.), mobiles et PC.\n\n\n== « Quatrième pouvoir » et déontologie ==\nLes médias sont souvent qualifiés de quatrième pouvoir, par allusion aux trois pouvoirs constitutionnels, dans le processus de la formation de l'opinion publique et dans l'influence que la révélation de ces faits peut avoir dans les prises de position des citoyens. Les faits, analyses ou commentaires qu'ils rapportent sont porteurs de sens, par exemple dans le domaine de la politique, de l'économie ou de la culture. Le choix des faits rapportés appartient aux responsables nommés par les propriétaires de ces médias d'où la revendication par les syndicats de journalistes pour obtenir l'indépendance des rédactions. Le SNJ, le SNJ-CGT, FO, la CFTC, et l'USJ CFDT ont rédigé à l'automne 2007 la pétition nationale pour l'indépendance des rédactions, dans le sillage du combat mené par les journalistes des quotidiens économiques Les Échos et La Tribune, en 2007. Ils demandent que la ligne éditoriale respecte la Charte de Munich, adoptée par la Fédération européenne des journalistes et référence européenne concernant la déontologie du journalisme, un texte qui distingue dix devoirs et cinq droits, en reprenant les principes de la Charte des devoirs professionnels des journalistes français.\nEn France, en cas de désaccord avec la ligne éditoriale, le journaliste peut en théorie demander l'application de la clause de conscience, supervisée par la commission arbitrale, l'une des cinq grandes commissions qui cogèrent la profession, en vertu du paritarisme. En pratique, la clause de conscience est très difficile à obtenir, la loi n'étant pas assez précise.\n\n\n== Publicité et ciblage ==\n\n\n=== Publics cibles ===\n\nL'influence des médias - à condition de choisir les messages et les supports pertinents - est généralement conçue et adressée vers celui qui achète (le consommateur-acheteur). Comme l’indique le sociologue D. W. Smythe, l’audience est ce que produisent les mass-media. Cependant, d'autres cibles indirectes sont régulièrement visées :\n\nle consommateur utilisateur : celui qui va effectivement consommer, utiliser le produit final.\nle prescripteur : celui qui est réputé capable de conseiller ou d'influencer l'acheteur du produit final.\nle distributeur : les campagnes médias en faveur d'un produit final peuvent contraindre les distributeurs à référencer un produit fortement faisant l'objet d'une forte promotion (difficile de ne pas vendre le produit connu et demandé par le consommateur…).\n\n\n=== Annonceur et émetteur ===\nL'annonceur désigne l'émetteur d'information qui passe commande à un support pour réaliser ou diffuser un message en direction d'une cible.\nPour cet annonceur, la politique de communication est l'un des 4 piliers fondamentaux de son marketing mix.\nLes informations peuvent être élaborées :\n\npar l'émetteur lui-même. De ce point de vue les nouvelles technologies numériques facilitent la conception et la diffusion de l'information. Le recours à des intermédiaires spécialistes n'est plus obligatoire quand l'émetteur maîtrise lui-même les compétences basiques (rédaction de texte, graphisme, illustration par du son ou de la vidéo, utilisation du mode interactif, etc.).\nen amont,\npar des agences de presse, qui fournissent les rédactions des médias abonnés. On compte trois grandes agences de presse généralistes dans le monde : AFP (France) ; Reuters (Royaume-Uni) ; Associated Press (États-Unis) ;\npar des agences de communication, qui conçoivent l'information à transmettre et à diffuser selon le cahier des charges élaboré par l'annonceur.\n\n\n=== Importance des études médias ===\nCompte tenu de leur efficacité, mais aussi de leur coût, les données médias et leur analyse sont au cœur des choix d’investissement en communication (environnement produit, positionnement, stratégie des concurrents, choix des supports, risques pour le client, impact sur les consommateurs, etc.). Les études médias, réalisées à partir de mesures d'impact et d'audience.\n\nréalisées a priori, elles permettent aux annonceurs d'effectuer le choix du média le plus approprié à la cible de leur campagne de communication, l'achat d'espaces de communication, ainsi que le suivi des supports de diffusion.\nréalisée a posteriori, elles permettent de vérifier l'efficacité de la communication et le retour sur investissement des budgets engagés.\nL'étude des médias ne néglige pas l'analyse qualitative :\n\nelle s'efforce de comprendre comment mieux segmenter les clientèles cibles et délivrer les messages les plus efficaces.\nelle intègre de plus en plus l'analyse des thèmes comme :\nles effets de l'image de l'annonceur et de son produit\nles modes de communication en environnement urbain (la majorité des consommateurs résident aujourd'hui dans les zones urbaines).\n\n\n=== Plan média ===\n\nLe plan média d'un annonceur (media planning, en anglais) désigne le choix des supports, ainsi que le calendrier de mise en œuvre des médias.\n\n\n== Médias et culture ==\nPromoteurs et porteurs de la culture de masse, les médias, écrits d'abord puis audiovisuels (radios, télévisions), ont été et restent encore des acteurs incontournables dans la diffusion de la culture et dans la structuration des marchés culturels, en particulier des produits culturels. Pour ces derniers, aucune offre de quelque importance ne peut exister sans plan-médias et sans médiatisation réussie. Le « diktat » des médias peut être décisif dans certains cas et nécessite une intervention régulatrice des pouvoirs publics (par exemple les quotas de production de phonogrammes de langue française au Québec, ou d'œuvres audiovisuelles d'origine européenne sur les télévisions françaises).\nLa plupart des grands groupes opérant dans les industries culturelles sont également leaders dans le secteur des médias écrits, audiovisuels et d'Internet : AOL Time Warner, Lagardère, Bertelsmann, Sony, etc. (quand ces médias sont utilisés conjointement, on parle alors de médias multiplateformes).\nCes conglomérats financiers que l'on peut qualifier de « médiatico-culturels », ayant grandi par croissance externe ou fusion, sont présents sur les principaux marchés des produits culturels éditoriaux (livres, phonogrammes, vidéogrammes, presse écrite, jeux vidéo…) et des offres médiatiques. Leur stratégie dans le domaine des médias doit en revanche tenir compte de deux contraintes majeures : les réglementations publiques, qui dans de nombreux pays contrôlent leur concentration (respect de l'information…) et la barrière linguistique, frein « naturel » au développement de certains marchés comme la presse, le livre et tous les contenus linguistiques. Néanmoins, cet obstacle est de moins en moins fort, en raison de la prédominance de l'anglais. Les groupes médiatico-culturels (le plus souvent classés dans le secteur de la communication) sont ainsi les acteurs majeurs de la concentration financière et de la concentration de l'offre, allant à l'encontre de la diversité culturelle et de la diversité linguistique.\nSi les médias ont été des acteurs clés de la constitution des « majors » dans les différents secteurs qui se sont structurés autour d'oligopoles à frange (cinéma, musique enregistrée…), ils sont aujourd'hui eux-mêmes menacés par le développement d'Internet et de nouveaux acteurs liés quant à eux aux offres et aux pratiques sur le Web (Google, Yahoo, Facebook, Twitter, MyMajorCompany…). Ainsi, dans le secteur de la musique enregistrée, les deux piliers de la concentration, d'une part le couple star/major et, d'autre part, les réseaux professionnels coopératifs associant publishing, scène, médias écrits et audiovisuels, sont considérablement mis à mal par le développement d'Internet. L'évolution rapide des usages sociaux qui en a résulté (ce que Bernard Stiegler appelle la consom'action) va à l'encontre de l'exposition traditionnelle des produits et du modèle économique de la marchandisation à grande échelle sur lequel reposait jusqu'à présent le système médiatico-culturel.\nDans de nombreux pays, le secteur des médias relève également du champ culturel, en particulier pour l'intervention publique ; par exemple en France avec un ministère de la Culture et de la communication ou au Royaume-Uni avec le Department for Culture, Media and Sport (DCMS) créé par le gouvernement de Blair en 1996.\nLes nouveaux médias permettent l'hyperchoix (décrit par Alvin Toffler dans Le Choc du futur) et sont hyper-spécialisés, au détriment de ceux qui sont généralistes. Ainsi, on a eu une baisse de 15 % des ventes de journaux généralistes en 15 ans, et une augmentation de 15 % de la vente de magazines. (2)\nLes utilisateurs de média choisissent désormais ce qu'ils regardent et quand ils le regardent.\nOn peut s'inquiéter de cet effet « œillère », qui limite de plus en plus l'ouverture d'esprit du public, ou se réjouir de cette opulence de diversité.\nLa théorie de l'hyperchoix reste critiquable, notamment avec le concept de « circularité de l'information » de Pierre Bourdieu qu'il décrit dans son livre Sur la télévision. Ainsi, il indique que les médias traitent, dans un grand nombre, des mêmes sujets, aux mêmes moments, et ceux-ci influencés le plus souvent par les médias dominants (Le Monde aura plus d'impact que le Midi libre). On n'est bien sûr pas surpris de savoir que les médias ont toujours eu le pouvoir de persuader l'opinion publique depuis ces médias dominants. On voit aussi que certaines médias en particulier tendraient à distraire le public de certains problèmes, crises et changements gouvernementaux.\n\n\n== « Méta-média » à l’ère numérique ==\nLes médias de masse sont notamment influencés, voire profondément renouvelés, par l’époque de numérisation croissante présente.\nDéjà en 1984 le théoricien Alan Kay définissait ainsi l’ordinateur comme le « premier méta-medium », en ce qu’il « est un medium qui peut simuler dynamiquement les détails de toute autre medium » et, en tant que tel, « il a des degrés de liberté de représentation et d’expression jamais encore atteints » : effectivement l’ordinateur aujourd'hui, par exemple, résume dans soi-même plusieurs fonctions de médiation qu’auparavant appartenaient à différents media spécifiques : il est télévision, agenda, courrier, machine à écrire, etc.\nCette capacité des médias numériques à remédier par une programmation numérique toutes les médiations antérieures, en le reconfigurant, a été également soulignée par le philosophe Vilém Flusser, qui désigne celle du numérique comme une phase de superposition de toutes les précédentes que l’histoire de la culture humaine a vu passer.\nUne fois posée l’hypothèse d’une numérisation des médias quasiment ubiquitaire à l’époque contemporaine nous pouvons énoncer quelques-unes des principales caractéristiques de cette situation.\nEn premier lieu, et comme les Software Studies le remarquent, il y a une croissante densité et obscurité des appareils utilisés, qui se font toujours plus accessibles et simples au niveau de leurs interfaces graphiques, et pourtant toujours plus insondables dans leur fonctionnement profond, jusqu'au paradoxe des algorithmes auto-apprenants dont l’intelligence supère celle de leurs ingénieurs, ou, dans les termes de Bernard Stiegler, d’une hyper-industrialisation qui entraîne une progressive exclusion de la force de travail intellectuel humaine.\nCertains auteurs ont entraîné à cet égard la notion de gouvernementalité algorithmique,,.\nLes méta-média du numérique opèrent aussi ce que Yves Citton propose de nommer une « premédiation » : dans leur complexité vouée à l’autorégulation, les médias numériques s’évadent d’une certaine façon de l’intelligence humaine et ils tendent à pre-médier, voire à anticiper ou bien à fournir des catégories préétablies de ce qu’ils nous proposent. Cela est évident dans le cas du ciblage commercial effectué par les algorithmes sur la base des données personnelles, mais s'observe aussi dans le cas de la contestée fiabilité de l’information, qui est souvent politiquement et économiquement conditionnée ou le résultat d’un calcul algorithmique, comme dans le cas des tableaux de bord de Facebook.\nPlus généralement, la question de la pré-médiation entraine notamment des dangers au niveau socio-politique, et particulièrement pour ce qui regarde les effets de lock-in – le phénomène pour lequel plus une plateforme compte des participants, plus les données qu’elle génère sont nombreuses et ses algorithmes sont puissants, plus elle offre des services efficaces, écrasant la concurrence en une tendance monopolistique - ou bien la ci-dite vectorialisation, à savoir la monopolisation de l’information opérée par une classe dominante à travers le contrôle des vecteurs par lesquels l’information est abstraite que McKenzie Wark revendique dans son « Hacker Manifesto ». Cela pourrait nous emmener à l’horizon d’une croissante asymétrie du rapport au medium entre l’individu et le programme (et celui qui gère le programme), jusqu’aux perspectives catastrophiques d’une société numérique basée sur la surveillance et sur la manipulation perpétuelle où il ne reste pas beaucoup d’espace à l’intelligence et à l’individualité humaines.\nUne autre et essentielle caractéristique des méta-media numériques est l’interconnectivité : désormais tous les outils – et non pas seulement les Smartphones et les ordinateurs, depuis l’avènement de l’Internet des objets, sont connectés, s’envoyant constamment des données entre eux qui enregistrent les préférences, les activités, les déplacements, les compagnies, etc. dans un grand réseau virtuel mondial que certains comparent à un « cerveau collectif mondial ».\nDans le danger d’une croissante standardisation et uniformisation des pratiques et des produits médiatiques, ou encore d’une colonisation commerciale du milieu numérique où le monopole des big data appartiendrait à un nombre très restreint de colosses capitalistes,,, plusieurs auteurs soulignent l’importance de développer une conscience majeure des enjeux juridico-politiques que l’ère du numérique entraîne conjointement à un effort de réglementation, et affirment la nécessité de préserver l’hétérogénéité au sein de l’espace numérique partagé,. Dans ce contexte les actions du hacktivisme s’inscrivent dans une perspective de lutte et de contestation politique, et d’une particulière éthique de revendication des droits personnels que le numérique met en danger,,.\n\n\n== Règlementation ==\ndans l'Union européenne, les médias sont notamment réglementés par le règlement (UE) 2024/1083 du 11 avril 2024 établissant un cadre commun pour les services de médias dans le marché intérieur et par la directive-règlement européen 2010/13/UE sur la liberté des médias.\n\n\n== Par pays ==\n\n\n=== Afrique ===\nMédias en Algérie\nMédias au Bénin\nMédias au Cameroun\nMédias au Congo\nMédias en Côte d'Ivoire\nMédias en Érythrée\nMédias en Éthiopie\nMédias à Madagascar\nMédias en ligne à Madagascar\nMédias au Mali\nMédias au Maroc\nMédias en République centrafricaine\nMédias en république démocratique du Congo\nMédias au Sénégal\n\n\n=== Asie ===\nMédias en Azerbaïdjan\nMédias en Corée du Nord\nMédias au Pakistan\nMédias dans la région autonome du Tibet\nMédias en Iran\n\n\n=== Amérique ===\nMédias au Canada :\nMédias au Québec\nMédias à Cuba\nMédias aux États-Unis :\nMédias à Détroit\nMédias à Maurice\nMédias au Venezuela\n\n\n=== Océanie ===\nMédias en Australie\nMédias aux Fidji\nMédias aux Tuvalu\n\n\n=== Europe ===\nMédias en Belgique\nMédias en Écosse\nMédias en France :\nMédias en Alsace\nMédias de Caen\nMédias à Limoges\nMédias en Lorraine\nMédias à Marseille\nMédias en Nouvelle-Aquitaine\nMédias en Islande\nMédias en occitan\nMédias en Russie\nMédias russophones\nMédias en Ukraine\n\n\n== Critiques ==\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\n\n\n=== Bibliographie ===\nBibliographie sur les moyens de communication de masse (1920- )\nAndré Akoun, Sociologie de la communication, Hachette, 1998.\nAntoine Violet-Surcouf, Influence et réputation sur Internet: communautés, crises et stratégies, La Bourdonnaye, Collection AEGE, 2014 (2e ed.)\nFrancis Balle, Les nouveaux médias avec Gérard Eymery, PUF, Que sais-je ? no 2112 , 1996.\nFrancis Balle, Médias & Sociétés, Montchrestien, 2009 (14e éd.), 832 pages (ISBN 9782707616401)\nFrancis Balle, Dictionnaire des médias, dir., Larousse, 1998.\nFrancis Balle, Les médias, PUF, Que sais-je ?, 2007 (3e éd.).\nMarie Bénilde, On achète bien les cerveaux, la publicité et les médias, Raisons d'agir, 2007\nYves Citton, Médiarchie, Seuil, 2017\nMonique Dagnaud, Les Artisans de l'imaginaire, Armand Colin, 2006.\nJean-Pierre Esquenazi, Sociologie des publics, La Découverte, 2004.\nFritz Heider, Chose et médium, Paris, Vrin, 2017\nFrançois-Bernard Huyghe, Comprendre le pouvoir stratégique des médias, Paris, Eyrolles, 2005.\nJean-Noël Jeanneney, Une histoire des médias, des origines à nos jours, éd. « Points » Seuil, Paris, 1990 (réédité quatre fois)\nMarshall McLuhan, Pour comprendre les médias, coll Essais, Seuil, 1977.\nÉric Maigret, Éric Macé, Penser les Médiacultures, Paris, Armand Colin, 2005, De Boeck\nTristan Mattelart (coordination), La Mondialisation des médias contre la censure, , 2002.\nBernard Miège, Les industries du contenu face à l'ordre informationnel, Presses Universitaires de Grenoble, 2000\nEsteban Montoya, Guide des médias alternatifs et des sources d'informations différentes, Lyon, éditions Le P'tit Gavroche, septembre 2006 (en cours de réactualisation pour fin 2011).\nSimon Nora et Alain Minc, L'informatisation de la Société (Rapport au Président de la République) La Documentation Française Paris 1978\nObservatoire européen de l'audiovisuel, Annuaire statistique 2006, Conseil de l'Europe, 2006.\nDominique Pasquier, Les Scénaristes de la télévision, Approche sociologique, 1999.\nRémy Rieffel, Sociologie des médias, Paris, Ellipses, 2003 (2e éd.).\nIngrid Riocreux, La Langue des médias : Destruction du langage et fabrication du consentement, Éditions de l’Artilleur/Toucan, 2016\nAnnelise Touboul et al., « La disparité des modes de traitement journalistiques et des énonciations éditoriales sur le web. Le cas d'un sondage sur Marine Le Pen et la Présidentielle de 2012, Réseaux 2012/6 (n°176), p. 73-103.\nGrégory Rzepski et Mathias Reymond, Tous les médias sont-ils de droite ?, Syllepse, Paris, 2008.\n\n\n=== Articles connexes ===\n\n\n=== Liens externes ===\n\nRessource relative à la santé : Medical Subject Headings \nRessource relative aux beaux-arts : Grove Art Online \nRessource relative à l'audiovisuel : France 24  \n\n Portail des médias   Sciences de l’information et bibliothèques   Portail de la société   Portail de la culture"
        },
        {
            "pageid": 205264,
            "ns": 0,
            "title": "Affichage",
            "content": "L’affichage est une technique de communication en extérieur qui consiste à installer et parfois gérer dans un espace public du mobilier urbain destiné à recevoir un support de nature promotionnelle. \nDans le cadre d'une campagne publicitaire, l'affichage peut être alors considéré comme un média à part entière, sans contexte rédactionnel, et complémentaire. Réglementé, l'affichage extérieur fait partie du paysage urbain mais de nombreux collectifs (riverains ou autres) tendent à vouloir en limiter le développement.\n\n\n== Histoire ==\n\n\n=== Débuts ===\n\nEn tant que support d'information destiné au passant, l'affiche imprimée a d'abord été utilisée, et ce dès la fin du XVe siècle en Europe mais surtout dans les grandes villes, pour placarder les annonces des crieurs publics : édits royaux, fête liturgique, arrivage de marchandises, parution d'un ouvrage, spectacle de foire. Avec l'alphabétisation, elle est progressivement employée comme un moyen de mobilisation : cet élan commence au milieu du XVIIe siècle. Libelles et placards, feuillets volants souvent pamphlétaires, par contrainte étatique (censure) ou économique (peu de moyens, donc petite diffusion), trouvaient un public en étant affichés sur les places publiques. Peu illustrées, les messages s'exprimaient en caractères de petites tailles et créaient ainsi des attroupements, par conséquent des débats et des troubles, dont le contenu des affiches était le point de départ. \nL'Europe connaît au XVIIIe siècle des périodes de relatives libertés en termes d'affichage : ainsi Londres, après 1695 ; Paris, entre 1715 et 1725, puis entre 1789 et 1800. Au Japon, les rues des villes accueillent par exemple des affiches annonçant des combats de sumo.\n\n\n=== Tournant du XIXe siècle ===\nPetit à petit, les concepteurs de ce type de supports comprirent que les gros titres interpelaient les gens au simple passage. L'affiche s'est alors détachée de la « feuille politique » pour transmettre des slogans, c’est-à-dire des messages courts, que la mémoire peut retenir en quelques secondes. Suivant l'agrandissement du format et des caractères, l'affichage fut utilisé pour annoncer un évènement particulier, faire la réclame (« annonce destinée à vanter les mérites de quelque chose », 1839), ou soutenir une propagande. En France, après le Premier Empire qui réglementait l'affichage dans les villes de façon drastique, les premières campagnes d'affichage voient le jour, dynamisées par les progrès des techniques d'impression. L'affichage devient plus graphique que littéraire et ce tournant s'opère vers 1840. Pour intégrer l'image et donner à celle-ci toute sa force, il faudra attendre la deuxième révolution industrielle et l'invention de la chromolithographie. Cette époque, située entre 1865 et 1880, marque également le coup d'envoi de l'affiche comme média de publicité mais aussi comme mode d'expression artistique. C'est aux États-Unis que naissent les premières agences de publicité : au regard de l'immensité du territoire, les premiers budgets sont régionaux. En 1900, Albert Lasker est le premier à lancer un programme d'affichage national aux États-Unis avec un budget annuel, colossal pour l'époque, de 300 000 dollars payé par la marque Washer.\nLe premier panneau publicitaire commandé par ordinateur est le Panneau Westinghouse qui fut installé en Pennsylvanie.\n\n\n=== France ===\nL'affichage comme média publicitaire a débuté le long des routes de France dès le début du XXe siècle  :\n\navec les entreprises Lumi-Route, l'O.T.A. (Office Technique d'Affichage), la SFAR (Société française d'affichage routier), la SCAP qui devint par la suite Giraudy, Route et Ville, Affichage Gaillard, Lioté, Avenir, puis Dauphin OTA, dirigée par Jacques Dauphin, qui racheta les principales sociétés d'affichage routier ;\nsoit sous forme d'affiches apposées à même les murs au tout début puis sur des panneaux (palissades) ;\nsoit sous forme de publicités peintes (murs peints) directement sur les murs « pignons » avant d'être typiquement urbain (année 1960) ou rurbain (années 1980).\nAujourd'hui, l'affichage est un média essentiellement à but publicitaire. C'est un média de masse ayant une cible urbaine et majoritairement jeune et masculine. Il est aussi celui qui vise les gens qui se déplacent le plus.\nDe nombreux formats existent dans chaque pays. Il n'y a que certaines entreprises d'affichage (appelés « afficheurs ») de dimension internationales comme JC Decaux, Clear Channel Outdoor et CBS Outdoor qui utilisent les formats universels (1,20x1,60 m et 3,20x2,40 m), et ce, à travers différents réseaux « markétés » et fortement ciblés selon les espaces (par exemple : centres commerciaux, aéroports, gares, univers rural, grandes et petites agglomérations, métros, bus, parkings, etc.).\nEn ville. on trouve des panneaux d'affichage mobiles portés par des bus, des taxis ou des véhicules dédiés : camions, camionnettes ou remorques tractées par des vélos.\nLes contrats pour l'installation du mobilier urbain dans l'espace public sont conclus par les collectivités territoriales, qui, moyennant redevance, accordent pour une durée déterminée des concessions aux entreprises spécialisées en publicité extérieure, lesquelles font appel à des cabinets de designers pour créer le mobilier de service public adéquat.\nAujourd'hui intégrée dans Clear Channel Outdoor, la société Dauphin, inventeur en 1963 de l'offre « réseau », qui est un « package » de panneaux à durée déterminée au format 4x3,  a obtenu en 1947 la concession des murs aveugle et des friches de la ville de Paris, en raison de la conduite exemplaire de ses afficheurs sous l'occupation allemande au cours de la Seconde Guerre mondiale : la société était alors dirigée par Eugène Dauphin, dit le « Colonel Duc » dans la Résistance.\n\n\n== Réglementation ==\n\n\n=== Réglementation en France ===\n\nL'affichage a toujours fait l'objet d'une réglementation. La loi du 29 juillet 1881 sur la liberté de la presse, qui abrogea tous les textes antérieurs, garantit les prérogatives de l'autorité publique (création d'emplacements réservés à l'affichage administratif, réglementation de l'affichage électoral, interdiction d'affichage sur les édifices publics). Puis, en 1902, 1910 et 1935, une série de lois assura l'interdiction de l'affichage sur des monuments historiques. La loi du 12 août 1943, validé à la Libération, définit une réglementation spécifique applicable dans les lieux ne faisant pas l'objet d'une interdiction d'affichage, accordant de larges pouvoirs  aux préfets afin de prendre en compte les circonstances locales.\nLe régime actuellement applicable résulte de la loi no 79-1150 du 29 décembre 1979, relative à la publicité, aux enseignes et préenseignes, qui constitue, en droit français, la première approche globale de cette forme de publicité dans ses rapports avec l'environnement. Cette loi institue une réglementation autonome de la publicité dont elle définit l'objet et les instruments. Elle pose le principe d'une définition concertée de la réglementation par zone de publicité, sur la base d'un régime de droit commun détaillé et restrictif. Elle définit un régime mixte de sanctions administratives et pénales en cas de violation de la loi. Elle renforce enfin la protection des particuliers qui louent aux entreprises spécialisées des emplacements publicitaires. \nIl est à noter qu'en France, il arrive que les afficheurs ne respectent pas la législation.\n\n\n== Technique ==\n\n\n=== Principaux formats et univers ===\nGrands formats : 12 m2 & 8 m2 dans les zones urbaines, 4 m2 dans les zones rurales (Exterion Media, Clear Channel Communications, Dauphin OTA)\nMurs peints : Dauphin OTA\nMobiliers urbains : 2 m2 abribus et mupi, 8 m2 sur le domaine public (Sénior de JC Decaux)\nMétro : 12 m2 Quais, couloirs en 200x150 400x150 et 120x176 (Métrobus)\nGares : 12 m2, 8 m2 et 2 m2 (MédiaTransports)\nTGV : 30x40 à bord des TGV (ONBOARD Régie)\nBus : Arrières 99x83, Flancs gauches 274x68, Flancs droits (Clear Channel Communications et Métrobus)\nTaxis & VTC : 220x60 flancs Classic  (Rainbooh, Quadriplay)\nVélos publicitaires Affi'bike : 4 à 8 m2 par vélo (Agence Opé Spé)\nCamions publicitaires : 12 m2 - 2 faces de 6 m2 (Affi Mobile, Visual Affiches)\nNavettes aéroports - Cars Air France :  (ONBOARD Régie)\nCentres commerciaux : 8 m2 et 2 m2 déroulant (Clear Channel)\nVitrines : 60x80, 40x60 et 60x160 sur les vitrines des commerces en centre-ville\nSanitaires : A3 dans les toilettes hommes et femmes des lieux de sortie, d'exposition et de spectacles (Next One, LittleCorner)\nL'affiche collée tend à disparaître du paysage urbain au profit de l'affiche déroulante intégrée dans des supports électroniques qui défilent sur 1 à 5 affiches selon des créneaux de temps d'exposition du message programmés.[réf. nécessaire]\n\n\n=== Nouvelles technologies ===\nL'affiche interactive : avec une puce intégrée à l'intérieur de l'affiche et renvoyant vers un récepteur mobile, elle commence à faire son apparition dès 2005 ;\nL'e-paper ;\nL'affichage olfactif ;\nLe Bluetooth ;\nL'affichage sonore ;\nLes écrans qui comptent les passants.\n\n\n== Économie ==\n\n\n=== Acteurs du marché ===\nLes principaux acteurs du marché mondial de l'affichage et de la communication extérieure sont, en 2007 :\n\nIHeartMedia, Inc. (États-Unis) : chiffre d'affaires de 3,28 milliards de dollars en 2007\nJCDecaux (France) : 2,88 milliards\nJCDecaux (Belgique) :\nExterion Media (États-Unis) : 2,18 milliards\nLamar (États-Unis) : 1,21 milliard\nStröer (Allemagne) : 0,67 milliard\nTitan Outdoor (États-Unis) : 0,45 milliard\nNews Outdoor (NewsCorp en Russie) : 0,42 milliard\nIl faut ajouter également que l'affichage en France est détenu principalement par les sociétés JC Decaux, CBS Outdoor, Clear Channel mais qu'un nombre important de sociétés locales sont fortement implantés en region (Abri Services, Pisoni, Gb affichage, Girod media, Aloesred, Signal Régie...).\n\n\n=== Outils de mesure du média ===\nLa société Affimétrie gère en France depuis 1992 l'étude d'audience de ce media sous l'égide du CESP.\nLes résultats d'audience varient en fonction de la durée des campagnes - 7 jours, 14 jours, 1 mois, 1 an, pour les panneaux communication permanente -, du nombre d'emplacements affichés, et d'univers sélectionnés.\nAujourd'hui, seuls le grand format et le mobilier urbain des principales régies sont placés sous le contrôle d'Affimétrie. Le mode de calcul statistique de l'audience reste opaque.\nL'impact de l'affichage : mémorisation, attribution, reconnaissance, et efficacité sur les ventes, sont analysées à travers de nombreux instituts tels que Ipsos, Sodis-Conseil, TNS Secodip, etc.\nIl en est de même pour le parcours de l'œil (technique « Eyes-Tracking ») sur la surface d'une affiche ou un écran plasma[réf. nécessaire].\n\n\n== Cas de l'affichage presse ==\n\nTrois acteurs principaux se partagent la niche de l'affichage presse :\n\nPromap, créée en 1971 ;\nInsert, né du regroupement des filiales d'Emap et du groupe Hachette en 1999 qui se positionne comme l'afficheur des centres-villes ;\nMediakiosk, spécialisée sur le réseau des kiosques.\nLes supports proposés par les afficheurs sont variés et sont généralement homothétiques au format des couvertures de magazines :\n\n30x40, perpendiculaire au point de vente, il incite le passant à rentrer dans le magasin ;\n60x80 ;\nle « dos de kiosque » (120x174) en principe efficace pour l'impact visuel qu'il provoque.\n\n\n== Notes et références ==\n\n\n== Bibliographie ==\nFrédéric Graber, L'affichage administratif au XIXe siècle. Former le consentement, éditions de la Sorbonne, 2023.\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nAffiche\nAffichage libre\nAffichage mobile\nAffichage radar\nAntipub\nPublicité\nPublicité extérieure\nPublicité sur le lieu de vente\nTableau d'affichage\nTechnique d'affichage\n\n\n=== Liens externes ===\n\n Portail du commerce   Portail de la publicité   Portail des médias   Portail de l’histoire"
        },
        {
            "pageid": 928701,
            "ns": 0,
            "title": "Affichage libre",
            "content": "L’affichage libre est un mode d'expression par affiche dans un lieu public sur un support prévu à cet effet. Son nom officiel est « affichage d'opinion et publicité relative aux activités des associations sans but lucratif ».\nL’affichage libre est règlementé et doit être distingué de l'« affichage sauvage » qui, de ce fait, est illégal.\nLe plus souvent, la fixation de l'affiche sur son support se fait avec de la colle liquide. La plus utilisée est la colle à tapisser en poudre mélangée à de l'eau. La méthode la plus couramment employée est la suivante : un support (en bois, métal ou plastique) est recouvert de colle, l'affiche apposée dessus, qui est à son tour couverte de colle.\n\n\n== En France ==\n\n\n=== Règlementation ===\nL'affichage libre était réglementé par la loi n° 79-1150 du 29 décembre 1979 et par le décret d'application n° 82-220 du 25 février 1982 ; il est actuellement régi par les articles L. 581-13, R. 581-2 et R. 581-3 du code de l'environnement. Les communes sont tenues d'informer (directement ou sur demande) les citoyens des emplacements d'expression libre disponibles sur leur territoire. \nCette règlementation pose en particulier que toutes les communes françaises doivent disposer d'au moins :\n\nquatre mètres carrés d'affichage libre pour les communes de moins de 2 000 habitants ;\nquatre mètres carrés plus deux mètres carrés par tranche de 2 000 habitants au-delà de 2 000 habitants pour les communes de 2 000 à 10 000 habitants ;\ndouze mètres carrés plus cinq mètres carrés par tranche de 10 000 habitants au-delà de 10 000 habitants pour les autres communes.\nPar ailleurs, la législation précise que le ou les emplacements réservés à l'affichage d'opinion et à la publicité relative aux activités des associations sans but lucratif doivent être disposés de telle sorte que tout point situé en agglomération se trouve à moins d'un kilomètre de l'un au moins d'entre eux.\nEn principe, ces emplacements d'affichage sous différentes formes (panneau, colonne Morris, mur, etc.) doivent être réservés aux associations ou à toute personne voulant passer une annonce gratuitement sans but lucratif ou commercial. Certaines communes réservent des panneaux par type d'affichage en distinguant ces trois catégories : \n\naffichage d'expression politique ;\naffichage associatif ;\nexpression libre.\nDans la plupart des communes, l'affichage d'opinion et des associations sans but lucratif est autorisé sur les palissades de chantier (Article L581-8 du Code de l’environnement).\nLa ville de Paris, avec 2 268 265 habitants (recensement de 2010), devrait disposer de 1 142 m2 d'affichage libre, soit environ 570 panneaux. Cependant, elle ne respecte pas ses obligations réglementaires : la ville recensait seulement 16 panneaux d'expression libre au 2 janvier 2013. Des panneaux municipaux vitrés sont sous clef et réservés à l'affichage associatif, qui est de fait soumis à autorisation[source insuffisante]. Seules de rares palissades de chantiers sont disponibles pour l'affichage libre, qui est donc quasi inexistant[source insuffisante] à Paris.\n\n\n=== Détournements ===\nL’affichage libre ne doit pas être confondu avec l’affichage sauvage, illégal en France.\nUn nombre important de ces espaces (spécialement dans les grandes agglomérations) sont utilisés par des manifestations commerciales comme celles des sociétés productrices d'événements commerciaux, foires et salons d'exposition, des cirques itinérants ou des discothèques. De ce fait, il est très difficile de faire respecter des emplacements réservés à chacun.\nUne commune ne peut interdire à une association ou à un particulier de déposer un texte, sous peine de « porter une atteinte grave et manifestement illégale à la liberté d’expression » (Tribunal administratif de Versailles, 9 octobre 2011, Sébastien Durand vs Commune de Saint-Cyr-l'École).\n\n\n== En Suisse ==\nLa ville de Lausanne a mis en place une trentaine de panneaux d'affichage « à but idéal », réservé exclusivement à la promotion d'idées ou d'activités à but non lucratif (vie associative, fête de quartier, etc.).\n\n\n== À Montréal ==\nL’affichage sauvage était illégal à Montréal. En 1992, Publicité Sauvage dépose un projet de législation de l’affichage sur les chantiers de construction et obtient ce droit en 1994.\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\n\n\n=== Articles connexes ===\nAffiche\nColonne Morris\nPublicité extérieure\nPublicité\n\n\n=== Liens externes ===\nDécret 82-220 du 25 février 1982\n« Affichage Libre », blog qui recense les emplacements des panneaux d’affichage libre de toute la France.\n Portail de la liberté d’expression   Portail de la politique   Portail des associations   Portail du commerce"
        },
        {
            "pageid": 1423553,
            "ns": 0,
            "title": "Affichage mobile",
            "content": "L’affichage mobile est une technique d'affichage publicitaire sur un véhicule.\nCes véhicules circulent sur un itinéraire défini pour aller à la rencontre d'une cible privilégiée. Pratiquement tout type de véhicule peut être utilisé (des trains, des camions, des bus, des taxis, des vtc, des voitures de particuliers, des scooters, des trottinettes, des vélos, des tricycles et autres triporteurs).\nEn France la tendance va nettement vers une démarche écoresponsable de l'affichage mobile.\nL'affichage mobile peut être considéré comme un média tactique, ou plutôt un affichage géo-localisé dans la mesure où il cible géographiquement une population (CSP) donnée.\n\n\n== Réglementation en France ==\nCODE DE LA ROUTE\n(Partie Réglementaire - Décrets en Conseil d'État) \nChapitre VIII : Publicité, enseignes et préenseignes\nArticle R418-1\nToute publicité lumineuse ou par appareil réfléchissant est interdite sur les véhicules.\n(…)\nArticle R418-4\nSont interdites la publicité et les enseignes, enseignes publicitaires et préenseignes qui sont de nature, soit à réduire la visibilité ou l'efficacité des signaux réglementaires, soit à éblouir les usagers des voies publiques, soit à solliciter leur attention dans des conditions dangereuses pour la sécurité routière. \n\n\n== International ==\nSelon les réglementations d'autres affichages mobiles sont possibles, ainsi en Chine des bateaux ou des camionnettes dont un des côtés est couvert par un écran vidéo, diffusent des films publicitaires prévus pour la télévision.\nAux États-Unis, un système d'écrans vidéo sur le côté des bus, avec le film qui change suivant la géolocalisation du parcours, se met en place à New York.\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\nMédia tactique\nTechnique d'affichage\nPublicité extérieure\n Portail des médias"
        },
        {
            "pageid": 755988,
            "ns": 0,
            "title": "Annonceur",
            "content": "Dans le monde de la communication et des médias, un annonceur est une entreprise qui investit en vue de se faire connaitre en général via une campagne de marketing. L'annonceur est un professionnel qui est responsable d'une marque ou d'un produit. Il se charge d'établir une campagne publicitaire en la commanditant, en choisissant une agence de publicité et en confiant à un chef de produit le soin de suivre l'évolution de cette campagne.\nCet investissement ne concerne pas seulement les annonces publicitaires dans la presse : il peut se traduire par toute forme d'achat d'espace ou de moyen de communication (spot télévision, radio, affichage, mais aussi publipostage, publicité sur le lieu de vente ou PLV, marketing téléphonique, communication sur la Toile, etc.)\n\nAffichage : \nEn vue d’assurer la liberté d’opinion et de répondre aux besoins des associations, les communes ont l’obligation de mettre à disposition des citoyens des surfaces d’affichage, dites d’« affichage libre » (Art. L.581-16). Il est constaté que ces emplacements réservés sont parfois utilisés par les publicités commerciales, en faveur des spectacles par exemple. De tels abus peuvent être sanctionnés pour méconnaissance de l’article L.581-24 puisque l’autorisation du propriétaire de l’emplacement n’a pas été sollicitée. Dans ce cas, c’est l’annonceur qui fera l’objet de sanctions.\nDigitale :\nEn croissance constante, la publicité digitale est en train de dépasser la publicité \"traditionnelle\", notamment aux États-Unis (les annonceurs américains ont dépensé plus de 129 milliards de dollars en publicité numérique en 2019, contre plus 109 milliards pour la publicité \"traditionnelle\" la même année). Les réseaux sociaux et les moteurs de recherches tirent une part importante de leur revenu de la publicité ciblée grâce aux données qu'ils récoltent, Facebook et Google étant les entreprises les plus importantes dans ce domaine, et sont régulièrement épinglées pour leurs pratiques abusives,\n\n\n== Articles connexes ==\nRégie publicitaire\nPublicité\nMarketing\nRéseaux sociaux\nDonnées personnelles\n\n\n== Notes et références ==\n\n Portail du travail et des métiers   Portail de la publicité"
        },
        {
            "pageid": 2468764,
            "ns": 0,
            "title": "Arbre de Cracovie",
            "content": "L’arbre de Cracovie était un arbre antique dans la grande allée de marronniers du Palais-Royal plantés par le cardinal de Richelieu. Sous l’ombrage de cet arbre, le plus beau de tous et remarquable par l’étendue de son feuillage, se réunissaient les nouvellistes de l’époque pour y échanger des informations sur l’actualité. La quantité de fausses nouvelles, nommées en langage populaire, « craques », qui se débitaient sous cet arbre le fit plaisamment, dans le même style, recevoir le nom d’« arbre de Cracovie ».\n\n\n== Histoire ==\n\nLà, l’un traçait sur le sable, avec sa canne, la marche des armées russes et s’emparait en imagination de Constantinople ou les partisans respectifs de l’Angleterre et des États-Unis d’Amérique, alors en guerre pour leur indépendance, se livraient, loin du théâtre des combats sanglants, les plus pacifiques des batailles.\nLa curiosité amenait en ce lieu les lecteurs du Courier de l'Europe et de la Gazette de Leyde, à peu près les seuls journaux du temps, mais également des personnages de la plus haute classe. On raconte qu’un jour, Metra qui, ayant alors une grande renommée en ce genre, présidait ce congrès de gobe-mouches, ayant voulu expulser un domestique en livrée du groupe réuni autour de lui, ce dernier réclama, en annonçant qu’il n’était là que pour garder la place de son maître, M. le comte de…\n\nLes jardins des Tuileries et du Luxembourg, autre rendez-vous de nouvellistes, avaient aussi leur arbre de Cracovie. Sous celui de cette dernière promenade, l’orateur habituel était un certain abbé que l’on avait nommé l’abbé Trente mille hommes, parce que son éternel refrain était : « Donnez-moi seulement trente mille hommes, et je prends cette ville, ou je gagne cette bataille. » Un de ses auditeurs affiliés, enchanté de cette éloquence militaire, le fit héritier de sa petite fortune ; et, n’ayant jamais su son nom de famille, il écrivit dans son testament : « Je laisse une somme de 20 000 fr. à M. l’abbé Trente mille hommes. » Des collatéraux voulurent attaquer ce legs ; mais il fut confirmé par les tribunaux, d’après le témoignage des honnêtes gobe-mouches du faubourg Saint-Germain, qui attestèrent que l’on n’appelait point autrement l’ecclésiastique nouvelliste.\nAu mois de juin 1779, cet arbre s’était abattu aux trois quarts et avait presque écrasé dans sa chute une vingtaine de nouvellistes ; depuis, il ne formait plus qu’un tronc informe. En 1782, le duc de Chartres fit abattre cette superbe allée, ainsi que tous les arbres du jardin, pour y faire construire, en 1783, trois nouvelles rues, parallèles à celle de Richelieu, à la rue Neuve-des-Petits-Champs et à celle des Bons-Enfants.\n\n\n== Postérité ==\nEn 1742, Charles-François Panard célèbre « L'arbre de Cracovie » dans un opéra-comique éponyme donné à la foire Saint-Germain.\nL’action du roman d’Alexandre Dumas, Ingénue (1853), s’ouvre sur une scène se déroulant sous l’arbre de Cracovie.\n\n\n== Bibliographie ==\nAlexis Lévrier, « Les fausses morts du Roi-Soleil, ou l’impossible contrôle de l’information », Le Temps des médias, vol. 30, no 1,‎ 19 mars 2018, p. 32–46 (ISSN 1764-2507, DOI 10.3917/tdm.030.0032, lire en ligne, consulté le 5 janvier 2025)\n\n\n== Notes et références ==\n\n\n=== Notes ===\n\n\n=== Références ===\n\n Portail du XVIIIe siècle   Portail de Paris"
        },
        {
            "pageid": 578114,
            "ns": 0,
            "title": "Autocensure",
            "content": "L'autocensure est une forme de censure que s'applique à elle-même une personne, une institution ou une organisation. Elle est déclenchée par la crainte ou la menace de censure par une autorité politique, financière, ou religieuse.\n\n\n== Intérêts à l'origine de l'autocensure ==\n\n\n=== Intérêts des actionnaires et des annonceurs ===\nEn France au XXIe siècle, la plupart des grands médias sont détenus par des sociétés privées, principalement actives dans d'autres secteurs économiques, parfois dépendant de contrats divers avec l’État, ce qui limite la liberté d'expression en leur sein. Pour ceux qui dépendent largement de recettes publicitaires s'y ajoute la peur de froisser les annonceurs[réf. nécessaire].\nLes responsables des rédactions peuvent ainsi avoir la tentation d'écarter une information dont ils pensent qu'elle pourrait nuire aux intérêts des actionnaires ou des annonceurs, voire précéder les désirs qu'ils leur prêtent en développant un journalisme de connivence, ou des formes de publireportage.\n\n\n=== Intérêts politiques ===\nAu Japon, le tabou du chrysanthème a contribué à instaurer un climat d'autocensure dans le monde de la presse nippone, effaçant la critique à l'égard de l'empereur du Japon.\nDans d'autres pays, le rôle du pouvoir politique dans la nomination des dirigeants de médias, publics surtout mais aussi parfois détenus par des amis de ce pouvoir, via des mécanismes décrits dans \"L'Élysée (et les oligarques) contre l'info\", un livre consacré aux relations entre magnats des médias et exécutif,, créé un risque d'être placés sur une liste noire pour les journalistes, et des pressions sur la hiérarchie du média concerné.\nL'autocensure permet au pouvoir politique d'éviter la forme contraignante d'une censure effective, qui serait mal perçue par l'opinion : il s'affranchit ainsi des coûts de surveillance et de la contre-publicité engendrée par la perception de la censure dans l'opinion publique[réf. nécessaire].\n\n\n=== Intérêts religieux ===\n\nSi le blasphème a été aboli en France par la Révolution française en 1792, il provoque encore des réactions violentes, comme les menaces qui ont suivi les caricatures de Mahomet dans le journal Jyllands-Posten au Danemark puis celles accompagnées d'abord d'un incendie et ensuite d'un attentat dans les locaux de Charlie Hebdo, le 7 janvier 2015. Malgré le risque d'éventuelles représailles de groupes religieux, les médias français ont refusé symboliquement de pratiquer l'\"autocensure\" et soutenu Charlie Hebdo en republiant ses caricatures.\n\n\n== Sondage sur la fréquence de l'autocensure ==\nSelon un sondage réalisé auprès des membres de la Guilde des auteurs réalisateurs de documentaires, au sujet de leurs investissements personnels dans des projets sur la période 2016-2021, 60% ont reconnu \"s’autocensurer\" avant même de proposer un documentaire, afin d'éviter qu’il ne soit refusé, et 63% ont estimé \"ne plus être à l'initiative de leurs films mais réaliser une commande initiée par la chaîne de télévisions ou le producteur\".\n\n\n== Formes de l'autocensure ==\n\n\n=== Choix des sujets abordés ===\nL'autocensure se pratique essentiellement dans le choix rédactionnel des sujets abordés, la manière de traiter une interview ou de rendre compte d'un sujet[réf. nécessaire]. Elle vise à diminuer la crédibilité à accorder à des sujets qui ne circulent que sur Internet, qualifiés de « rumeurs » et de ce fait ignorés des médias de masse[réf. nécessaire]. De leur côté, les agences de presse valident l'essentiel des informations et économisent d'autant le besoin d'autocensure[réf. nécessaire].\n\n\n=== Rôle de la hiérarchie ===\nPour des raisons hiérarchiques, un journaliste peut hésiter à écrire un article dont il sait qu'il sera mal perçu par sa hiérarchie, par peur d'un possible licenciement lorsque le journaliste est salarié, et a fortiori lorsque son statut est plus précaire (pigiste).\n\n\n=== Rôle de l'environnement économique ===\nSelon Elizabeth Drévillon, présidente de la Guilde des auteurs réalisateurs de documentaires, qui a alerté sur les risques d'autocensure croissante après un sondage parmi ses membres, la liberté de création dans le domaine documentaire requiert l’indépendance financière des auteurs, qui pourrait être assurée par un salaire minimum. Rémunérés à 40 % en droit d’auteurs, les auteurs de documentaires ne bénéficient pas du statut de journaliste, mais de celui de l’intermittence, alors qu’ils font le même métier, et de ce fait travaillent souvent en situation de précarité avec peu de marge de manœuvre.\n\n\n== Conséquences ==\n\n\n=== Démocratie et débat démocratique ===\nLa réalisatrice Elizabeth Drévillon, présidente de la Guilde des auteurs réalisateurs de documentaires, et le journaliste d'investigation Jean-Baptiste Rivoire, ex-rédacteur en chef adjoint du magazine \"Spécial investigation\" de Canal+, fondateur du média \"Off-investigation\" auditionnés en février 2022 par la commission d'enquête sur la concentration dans les médias créée au Sénat, ont estimé que la pression des chaînes sur les auteurs-réalisateurs de documentaires aboutissait à une « autocensure », avec pour conséquence un « préjudice au débat démocratique ».\n\n\n=== Recours aux \"fake news\" ===\nLe développement de l'autocensure donne l'impression que les journaux développent les mêmes sujets, avec les mêmes idées, ou encore se prêtent au politiquement correct, d'où un déficit de pluralisme dans les médias, et une perte de crédibilité[réf. nécessaire].\nConcernant les informations du domaine politique et économique, compte tenu du manque de contenus d’investigation télévisé, les citoyens \"vont chercher ailleurs l’information, ce qui ouvre la porte aux fake news et au complotisme\", selon Elizabeth Drévillon, présidente de la Guilde des auteurs réalisateurs de documentaires et Jean-Baptiste Rivoire.\n\n\n== L'auto-censure en dehors des médias ==\nSi les médias et le journalisme concentre la plupart des situations où sont évoquées des tactiques d'auto-censure, d'autres secteurs sont parfois cités, même si les liens avec le terme parent de censure y sont beaucoup plus distendus:\n\nla réponse à des enquêtes d'opinion non anonyme engendrant un biais d'autocensure\nles discussions entre amis où se dessine l'autocensure de certains sujets sensibles, politiques ou privés;\nle cadre professionnel avec l'autocensure face à la hiérarchie;\nen psychologie comportementale, avec les raisons diverses, comme le manque de confiance en soi, pour laquelle nous ne disons pas tout ce que l'on pense;\nles questions autour de la communication anonyme ou dissimulées derrière un pseudo.\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\n\n\n=== Bibliographie ===\nPascal Durand (éd.), Médias et censure. Figures de l'orthodoxie, Éditions de l'Université de Liège, coll. \"Sociopolis\", 2004\n\n\n=== Articles connexes ===\nCode Hays, autocensure au cinéma\nORTF\nCensure\nLiberté de la presse\nPolitiquement correct\nLes nouveaux chiens de garde, livre de Serge Halimi, journaliste et sociologue\nCaricatures de Mahomet du journal Jyllands-Posten\nDésinformation\nRadio edit (clean version)\nChilling effect (en)\n\n\n=== Liens externes ===\n\nConcentration des médias et perte du pluralisme\nLa recherche \"auto-censure\" sur le site de l'Observatoire des médias\n Portail des médias    Portail de la politique   Portail du droit   Portail des arts   Portail de la liberté d’expression"
        },
        {
            "pageid": 10599,
            "ns": 0,
            "title": "Autocollant",
            "content": "Un autocollant est un support de texte ou d'images disposant sur sa face inférieure d'une fine couche d'adhésif permettant de le fixer sur une surface, à titre généralement définitif.\nLe support est constitué de papier ou d'un matériau synthétique. Le message véhiculé est en général imprimé ou sérigraphié mais peut être également dessiné à la main pour les productions à petite échelle.\nLa partie adhésive est normalement couverte par une protection siliconée (le transfert) tant que l'autocollant n'a pas été apposé. Certains autocollants n'utilisent pas de colle, mais sont conçus pour tenir sur des surfaces très lisses (verre, céramique) par effet électrostatique. D'autres sont magnétiques (aimantins). Dans ce cas ils peuvent être retirés ou repositionnés sans altérer la surface qui les porte.\n\n\n== Terminologie ==\nLe terme autocollant désigne à la fois l'objet et la technologie adhésive qui consiste en une vignette prête à être collée sans ajout de colle supplémentaire. \n\nIl est utilisé comme adjectif — vignette autocollante, timbre autocollant, étiquette autocollante, postit autocollant, macaron autocollant — lorsque la technologie est utilisée dans un but pratique, par exemple pour l'étiquetage des produits vendus dans un commerce.\nIl est utilisé en tant que nom pour un objet indépendant qui n'a d'autre objectif que de véhiculer le message promotionnel que constitue cet objet.\n\n\n== Typologie ==\n\nDifférents types d’autocollants existent :\n\nLes autocollants politiques ou militants qui revendiquent une position ou un message. Ils représentent une contre-culture et ont tendance à être collés sur les « publicités » même qu'ils dénoncent. Ils se présentent ainsi comme une façon de se rapproprier l'espace en général et l'espace publicitaire en particulier. En France, ce mode d'expression est avant tout utilisé par les extrêmes : identitaires, anarchistes et syndicats radicaux.\n\nLes autocollants artistiques : il s'agit souvent, à partir d'un autocollant vierge, de faire une petite œuvre d'art, allant du tag au dessin. On voit de plus en plus se développer des autocollants artistiques en vinyle, sur lesquels un travail graphique est effectué avant de les imprimer en série.\n\nLes autocollants publicitaires : ils servent à faire apparaître des marques, des logos, des slogans publicitaires. La plupart sont produits en masse par les annonceurs et les partis politiques, mais certains sont produits en petites séries comme les autocollants de groupes de musique pour véhiculer leur nom.\nLes autocollants muraux font également leur apparition dans le monde de la décoration d'intérieur.\nLes autocollants politiques, artistiques et certains publicitaires sont destinés à être collés dans la rue[réf. nécessaire] alors que les autocollants muraux, de plus grand format, sont des accessoires d'intérieur.\n\n\n== Histoire ==\n\nLe principe de l'autocollant apparaît peu de temps après la mise au point d'adhésifs efficaces et bon marché, vers la fin des années 1960. Il prend rapidement la place des décalcomanies, et devient très utilisé dans les années 1970 à 1990 en tant qu'objet publicitaire. Le nombre et la variété des autocollants publicitaires produits à ce titre en ont fait un objet de collection, la stickophilie, facile d'accès car peu cher et très répandu.\nLes autocollants militants et artistiques (les objets et la pratique) deviennent à partir des années 1980 un moyen de communication alternatif, et un phénomène en expansion depuis les années 1990 : le prix de production relativement bas et la facilité à coller les autocollants n'importe où sont deux explications de cette pratique souvent qualifiée de « sauvage ».\nDepuis le début des années 2000, l’autocollant est apparu dans nos foyers en tant qu’outil de décoration. Aujourd’hui il rencontre un véritable succès sur ce marché de la décoration d'intérieur et est devenu un vrai phénomène de mode.\n\n\n== Dispositions légales ==\nEn 1979, la loi du 29 décembre interdit de fixer des publicités et donc des autocollants hors des espaces autorisés ; en parallèle, le second choc pétrolier et la conjoncture économique qui s'ensuit font préférer d'autres supports de communication aux annonceurs. L'autocollant reste toutefois un support pour des campagnes publicitaires ponctuelles. Pour dégager leur responsabilité en cas d'affichage sauvage, les annonceurs ajoutent la plupart du temps sur les autocollants édités la mention « à coller uniquement sur les endroits autorisés, conformément à la loi du 29 décembre 1979 ».\n\n\n== Voir aussi ==\n\nContre-culture\nPublicité\nMédias\nStickophilie\nSticker art\nAutocollant mural\nGommette\nAdhésif sensible à la pression\nChimie des adhésifs sensibles à la pression\n\n\n== Références ==\n\n Portail des technologies"
        },
        {
            "pageid": 732322,
            "ns": 0,
            "title": "Biais médiatique",
            "content": "Un biais médiatique est une tendance des médias à présenter involontairement les informations, idées ou évènements d'une façon altérée par un apriori ou une conviction.\nLe phénomène de biais médiatique est connu des attachés de presse, des états-majors du monde politique, et bien entendu des médias eux-mêmes. Il s'ensuit une course au positionnement.\n\n\n== Typologie ==\nUne étude de 2014 analyse l’envergure et les différentes formes de biais médiatiques. Cette étude distingue quatre formes de biais en lien avec:\n\nles caractéristiques propres à un événement ;\ndes facteurs contextuels ;\nla structure organisationnelle des médias et son marché ;\ndes facteurs liés à la méthode de recherche scientifique lors de la récolte et du traitement de données.\nEn 2017, une étude américaine dévoile un biais médiatique en lien avec la couverture d'acte terroriste. L'étude dénombre cinq fois plus de couverture médiatique si les auteurs sont musulmans.\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\nCritique des médias\nDésinformation\nÉthique\nFaux équilibre médiatique\nPluralisme des médias (en)\nModèle de propagande\nPropagande\nSources d'informations\nL'Homme unidimensionnel\n\n Portail des médias"
        },
        {
            "pageid": 1571150,
            "ns": 0,
            "title": "Biais spatial et biais temporel",
            "content": "Le théoricien Harold Innis (1894–1952) a élaboré en [Quand ?] le concept de biais temporel ou spatial pour décrire la façon dont fonctionnent les médias en société. Il distingue deux tendances au sein des médias : \n\nles médias à biais temporel, qui favorisent la conservation du savoir sur de longues périodes,\nles médias à biais spatial, qui diffusent le savoir sur de grandes distances.\nLes partis pris de la communication influent directement sur la façon dont les médias exercent le contrôle[Lequel ?], et partant, la façon dont la société est organisée.\n\n\n== Notes et références ==\n\n\n== Voir aussi ==\nBiais cognitif\nMédias\nArchives\n\n\n=== Bibliographie ===\nTremblay, G. (2008). De Marshall McLuhan à Harold Innis ou du village global à l\"empire mondial. tic&société, 1(1).\n Portail des médias"
        }
    ]
}